{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55953785-3d57-4f25-8ad1-e347113c44b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyPDF2\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b98d96-37db-410f-9d69-472c517bc270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(path):\n",
    "    \"\"\" Convert pdf at the path to a text string\"\"\"\n",
    "    reader = PdfReader(path)\n",
    "    print(f'Converting {len(reader.pages)} pages of the pdf to text')\n",
    "    text = '\\n'.join(page.extract_text() for page in reader.pages)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f5cff0-1f7f-4492-81e7-3dc623f5904c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 12 pages of the pdf to text\n"
     ]
    }
   ],
   "source": [
    "text_faq = pdf_to_text('test-pdfs/RH-1-productfaq.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a10fac6-65f9-4237-b454-b332e0717bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 81 pages of the pdf to text\n"
     ]
    }
   ],
   "source": [
    "text_cadmin = pdf_to_text('test-pdfs/RH-7-cluster_administration.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20efdee3-b074-4fc9-bb20-bf9846f0cac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home Products Red Hat OpenShift Re…   \n",
      "Red Hat OpenShift\n",
      "Service on AWS\n",
      "frequently asked\n",
      "questions\n",
      "Get answers to common questions about Red\n",
      "Hat® OpenShift® Service on AWS (ROSA).  Learn\n",
      "how to quickly build, deploy, and manage\n",
      "Kubernetes applications on the industry’s most\n",
      "comprehensive application platform in AWS cloud.\n",
      "General questions\n",
      "What does Red Hat OpenShift Service on AWS include?\n",
      "Each Red Hat OpenShift Service on AWS cluster comes with a fully-managed\n",
      "control plane (master nodes) and application nodes. Installation, management,\n",
      "maintenance and upgrades are  monitored by Red Hat site reliability engineers\n",
      "(SRE) with joint Red Hat and Amazon support.  Cluster services (such as logging,\n",
      "metrics, monitoring) are available as well.\n",
      "How is Red Hat OpenShift Service on AWS different from Red Hat\n",
      "OpenShift Container Platform?\n",
      "Red Hat OpenShift Service on AWS delivers a turnkey application platform,\n",
      "optimized for performance, scalability, and security. Red Hat OpenShift Service\n",
      "on AWS is hosted on Amazon Web Services public cloud and jointly managed by\n",
      "Red Hat and AWS. Some options and administrative functions may be restricted\n",
      "or unavailable. A Red Hat OpenShift Container Platform subscription entitles you\n",
      "to host and manage the software on your infrastructure of choice.\n",
      "How Is Red Hat OpenShift Service on AWS different from Red Hat\n",
      "OpenShift Dedicated?\n",
      "Red Hat OpenShift Service on AWS is a fully managed implementation of\n",
      "OpenShift Container Platform deployed and operated on AWS, jointly managed\n",
      "and supported by both Red Hat and AWS.\n",
      "Red Hat OpenShift Dedicated is a service hosted and fully-managed by Red Hat\n",
      "that offers clusters in a virtual private cloud on AWS or Google Cloud Platform.\n",
      "What are the differences between Red Hat OpenShift Service on\n",
      "AWS and Kubernetes?\n",
      "Red Hat OpenShift\n",
      "Everything you need to deploy and manage containers is bundled with ROSA,\n",
      "including container management, automation (Operators), networking, load\n",
      "balancing, service mesh, CI/CD, firewall, monitoring, registry, authentication, and\n",
      "authorization capabilities. These components are tested together for unified\n",
      "operations as a complete platform. Automated cluster operations, including over-\n",
      "the-air platform upgrades, further enhance your Kubernetes experience.\n",
      "Will Red Hat OpenShift Service on AWS integrate with other AWS\n",
      "services?\n",
      "Yes. Red Hat OpenShift Service on AWS will integrate with a range of AWS\n",
      "compute, database, analytics, machine learning, networking, mobile, and various\n",
      "application services which will enable customers to benefit from the robust\n",
      "portfolio of AWS services which scale on-demand across the globe. These AWS\n",
      "native services will be directly accessible to quickly deploy and scale services\n",
      "through the same management interface.\n",
      "How does ROSA work?\n",
      "Red Hat OpenShift Service on AWS (ROSA) has infrastructure components\n",
      "(virtual machines, storage disks, etc.) and a software component (OpenShift).\n",
      "When you provision ROSA clusters you will incur the infrastructure and OpenShift\n",
      "charges at the pay-as-you-go hourly rate. Refer to the Red Hat OpenShift\n",
      "Service on AWS pricing page for more information. You can also do 1 or 3 year\n",
      "commits for even deeper discounts.\n",
      "How do I receive support for ROSA?\n",
      "ROSA is supported by AWS and Red Hat, and you have the option to contact\n",
      "support from either company to begin troubleshooting. Any escalations that are\n",
      "necessary will be facilitated as necessary by AWS and Red Hat to engage the best\n",
      "team to address the issues.\n",
      "Where can I see a roadmap or make feature requests for the\n",
      "service?\n",
      "You can visit our ROSA roadmap to stay up to date with the status of features\n",
      "currently in development. Please feel free to open a new issue if you have any\n",
      "suggestions for the product team.\n",
      "Purchasing\n",
      "Is Red Hat OpenShift Service on AWS available for purchase in all\n",
      "countries?\n",
      "Red Hat OpenShift Service on AWS is available for purchase in all countries where\n",
      "AWS is commercially available.Red Hat OpenShift\n",
      "How can I purchase Red Hat OpenShift Service on AWS?\n",
      "Customers can acquire the service directly from the AWS console on their own. As\n",
      "with other AWS services, such as EC2, customers will spin up OpenShift clusters\n",
      "and will be charged based on their consumption. Customers can also contact their\n",
      "Red Hat or AWS representative for more detailed pricing information.\n",
      "Will I receive an invoice from Red Hat or AWS?\n",
      "You will receive a single invoice from AWS.\n",
      "Does Red Hat OpenShift Service on AWS qualify for the AWS EDP\n",
      "Program?\n",
      "Yes, Red Hat OpenShift Service on AWS fully qualifies for 100% of the spend on\n",
      "the AWS Enterprise Discount Program.\n",
      "How is the pricing of Red Hat OpenShift Service on AWS\n",
      "calculated?\n",
      "See here for pricing information: ROSA Pricing\n",
      "When pricing out my EC2 instances, do I need to use RHEL for the\n",
      "operating system?\n",
      "No, being that ROSA includes Red Hat Enterprise Linux CoreOS (RHCOS) as the\n",
      "operating system, you only need to choose Linux. For clarity, ROSA cluster\n",
      "creation handles the setup of all the RHCOS nodes entirely.\n",
      "Can I use spot/preemptible VMs?\n",
      "Yes, additional MachinePools can be configured with Spot instances. Using\n",
      "Amazon EC2 Spot instances allows you to take advantage of unused capacity at a\n",
      "significant cost savings. Please see the Creating a machine pool section in the\n",
      "documentation for more information.\n",
      "Is there an upfront commitment?\n",
      "There is no required upfront commitment. ROSA clusters can be provisioned on-\n",
      "demand, with pay-as-you-go billing, for both AWS & OpenShift expenses. In this\n",
      "case, there is no upfront commitment. One year & three year RI pricing is also\n",
      "available to take advantage of pricing discounts.\n",
      "How can I start using Red Hat OpenShift Service on AWS?\n",
      "You can acquire the service directly from the AWS console. As with other AWS\n",
      "services, such as EC2, just spin up your OpenShift clusters and you will be charged\n",
      "based on your consumption. You can also contact your Red Hat or AWS\n",
      "representative. Here is a short video that demonstrates the process to deploy a\n",
      "ROSA cluster.\n",
      "Do I need to sign/have a contract with Red Hat?\n",
      "Red Hat OpenShift\n",
      "No. You do not need to have a contract with Red Hat to use ROSA. You will need a\n",
      "Red Hat account for use on console.redhat.com which includes accepting our\n",
      "Enterprise Agreement and Online services terms.\n",
      "Can I bring my own license to the service (e.g. Red Hat Cloud\n",
      "Access)?\n",
      "No. Billing occurs directly through AWS, preventing OpenShift Container Platform\n",
      "or OpenShift Dedicated subscriptions from being used with Red Hat OpenShift\n",
      "Service on AWS.\n",
      "Can I migrate my existing OpenShift Subscriptions to AWS?\n",
      "OpenShift (OCP, OSD, OKE) subscriptions cannot be used with ROSA.\n",
      "It is not possible to transfer the unused part of your Red Hat OpenShift\n",
      "subscription to ROSA.\n",
      "Subscriptions included with a purchase of an IBM CloudPak cannot be used\n",
      "with ROSA.\n",
      "ROSA subscriptions can only be purchased directly from AWS & AWS\n",
      "resellers.\n",
      "Can I purchase middleware subscriptions on-demand for my ROSA\n",
      "clusters?\n",
      "Middleware subscriptions (e.g, Integration or Runtimes) are purchased from Red\n",
      "Hat yearly via the standard process. Currently, there is no on-demand purchasing\n",
      "ability for your ROSA clusters.\n",
      "What are the regions where SREs have residency to operate?\n",
      "Please see the Red Hat Subprocessor List.\n",
      "Customization\n",
      "Which Amazon regions are supported?\n",
      "See supported resources for a list of global regions where Red Hat OpenShift\n",
      "Service on AWS is supported.\n",
      "What virtual machine sizes can I use?\n",
      "See Red Hat OpenShift Service on AWS virtual machine sizes for a list of virtual\n",
      "machine sizes you can use with a Red Hat OpenShift Service on AWS cluster.\n",
      "Which Red Hat OpenShift Container Platform rights do we have?\n",
      "Cluster-admin? Project-admin?\n",
      "You are granted Cluster Admin rights on the clusters you create.Red Hat OpenShift\n",
      "Can I add RHEL workers to my cluster?\n",
      "No. In order to maintain our ability to provide seamless updates to your clusters,\n",
      "only Red Hat Enterprise Linux CoreOS (RHCOS) workers are supported by Red\n",
      "Hat OpenShift Service on AWS.\n",
      "Operations\n",
      "Which services are performed by Red Hat and AWS Operations?\n",
      "Red Hat SRE is responsible for provisioning, managing, and upgrading the Red\n",
      "Hat OpenShift platform as well as monitoring the core cluster infrastructure for\n",
      "availability. They are not responsible for managing the application lifecycle of\n",
      "applications that run on the platform.\n",
      "How do I make configuration changes to my cluster?\n",
      "An administrative user has the ability to add/remove users and projects, manage\n",
      "project quotas, view cluster usage statistics, and change the default project\n",
      "template. Admins can also scale a cluster up or down, and even delete an existing\n",
      "cluster.\n",
      "Are ROSA clusters deployed in the customer account?\n",
      "Yes. ROSA clusters are deployed in your account with support for existing VPCs.\n",
      "We suggest you follow security best practices for application isolation and least\n",
      "privileges when considering placement.\n",
      "Is my ROSA cluster infrastructure shared with any other customer?\n",
      "How are upgrades managed?\n",
      "Customers can define the upgrade policy and schedule for their clusters in\n",
      "OpenShift Cluster Manager. Clusters can be configured to be automatically\n",
      "upgraded during a customer defined maintenance window to the latest release,\n",
      "for example \"Saturday at 02:00 UTC\" or clusters can be upgraded to a specific\n",
      "release at a date and time specified by the customer. Following best practices\n",
      "helps ensure minimal to no downtime.\n",
      "All upgrades are monitored and managed by Red Hat’s SREs service.\n",
      "What about emergency vs. planned maintenance windows?\n",
      "We do not distinguish between the two types of maintenance. Our teams are\n",
      "available 24/7/365 and do not use traditional scheduled \"out-of-hours\"\n",
      "maintenance windows.\n",
      "How will the host operating systems and OpenShift software be\n",
      "updated?\n",
      "The host operating systems and OpenShift software are updated through the\n",
      "general upgrade process.Red Hat OpenShift\n",
      "Can logs of underlying VMs be streamed out to a customer log\n",
      "analysis system?\n",
      "Customers are able to select Application, Infrastructure, and Audit log streams to\n",
      "be forwarded.\n",
      "Which UNIX rights (in IaaS) are available for Masters/Worker Nodes?\n",
      "Not applicable to this offering. Node access is not enabled. Worker nodes are fully\n",
      "managed by the SRE team.\n",
      "Service\n",
      "Which compliance certifications does ROSA have so far?\n",
      "Red Hat OpenShift Service on AWS is currently compliant with SOC-1, SOC-2\n",
      "type 1 & type 2, ISO-27001, & PCI-DSS. We are also currently working towards\n",
      "FedRAMP High, HIPAA, ISO 27017 and ISO 27018.\n",
      "Can a cluster have worker nodes across multiple AWS regions?\n",
      "No, all nodes in a Red Hat OpenShift Service on AWS cluster must be located in\n",
      "the same AWS region; this follows the same model as that of OCP. For clusters\n",
      "configured for multiple availability zones control plane nodes and worker nodes\n",
      "will be distributed across the availability zones.\n",
      "What is the minimum number of worker nodes that a ROSA cluster\n",
      "can have?\n",
      "For a ROSA cluster the minimum is 2 worker nodes for single AZ and 3 for multiple\n",
      "AZ.\n",
      "Where can I find the product documentation for ROSA?\n",
      "ROSA documentation can be found here.\n",
      "Can an admin manage users and quotas?\n",
      "Yes, a Red Hat OpenShift Service on AWS customer administrator can manage\n",
      "users and quotas in addition to accessing all user created projects. Please see for\n",
      "example resource quotas per project.\n",
      "When will features of the latest version of Kubernetes be supported\n",
      "in ROSA via OpenShift 4?\n",
      "Customers are able to upgrade to the newest version of OpenShift in order to\n",
      "inherit the features from that version of OpenShift (see life cycle dates). Note,\n",
      "that since ROSA is an opinionated installation of OpenShift Container Platform,\n",
      "not all features may be available on ROSA. Please review the Service Definition.\n",
      "How can customers get support for the service?\n",
      "Red Hat OpenShift\n",
      "ROSA is supported by AWS and Red Hat, and you have the option to contact\n",
      "support from either company to begin troubleshooting. Any escalations that are\n",
      "necessary will be facilitated as necessary by AWS and Red Hat to engage the best\n",
      "team to address the issues.\n",
      "Red Hat Support\n",
      "AWS Support (Customer must have valid AWS support contract)\n",
      "You can also visit the Red Hat Customer Portal to search or browse through the\n",
      "Red Hat Knowledgebase of articles and solutions relating to Red Hat products or\n",
      "to submit a support case to Red Hat Support. Or you can open up a ticket directly\n",
      "from OpenShift Cluster Manager (OCM). See the ROSA documentation for more\n",
      "details about obtaining support.\n",
      "What happens if I do not upgrade my cluster before the \"end of life\"\n",
      "date?\n",
      "Nothing will happen to an existing ROSA cluster. Your ROSA cluster will continue\n",
      "to operate though it will be in a \"limited support\" status. In short, this means that\n",
      "the SLA for that cluster will no longer be applicable, but you can still get support\n",
      "for that cluster. Please see Limited support status for more details.\n",
      "What is the SLA?\n",
      "Please refer to the Red Hat OpenShift Service on AWS SLA page for details.\n",
      "How will customers be notified when new features/updates are\n",
      "available?\n",
      "Updates will go through the regular communication channels, including AWS\n",
      "updates and email.\n",
      "What version of OpenShift is running?\n",
      "Red Hat OpenShift Service on AWS is a managed service which is based on\n",
      "OpenShift Container Platform. You can view the current version and life cycle\n",
      "dates in the ROSA documentation.\n",
      "Is Open Service Broker for AWS (OBSA) supported?\n",
      "Yes, you can use OSBA with Red Hat OpenShift Service on AWS. See Open\n",
      "Service Broker for AWS for more information. Though a more recent development\n",
      "is the AWS Controller for Kubernetes. This is the preferred method.\n",
      "What is the underlying node OS used?\n",
      "As with all OpenShift v4.x offerings, the control plane, infra and worker nodes run\n",
      "Red Hat Enterprise Linux CoreOS (RHCOS).\n",
      "What is the process for customers wishing to ‘offboard’ their\n",
      "deployment in ROSA - is there a process?\n",
      "Red Hat OpenShift\n",
      "Customers can stop using the service anytime and move their applications to on-\n",
      "prem, private cloud or other cloud providers. Standard reserved instances (RI)\n",
      "policy applies for unused RI.\n",
      "What authentication mechanisms are supported with ROSA?\n",
      "OpenID Connect (a profile of OAuth2), Google OAuth, GitHub OAuth, GitLab,\n",
      "and LDAP.\n",
      "How will events such as product updates and scheduled\n",
      "maintenance be communicated?\n",
      "Red Hat will provide updates via email and Red Hat console service log.\n",
      "Does ROSA have a hibernation or shut-down feature for any nodes\n",
      "in the cluster to save costs on infrastructure or to retain a\n",
      "configured cluster for long-term?\n",
      "No, not at this time. The shutdown/hibernation feature is an OpenShift platform\n",
      "feature not yet mature enough for widespread cloud services use.\n",
      "Technical questions\n",
      "Is SRE access to clusters secured by MFA?\n",
      "Yes, all SRE access is secured by MFA. See SRE access in the documentation for\n",
      "more details.\n",
      "What encryption keys, if any, are used in a new ROSA cluster?\n",
      "We encrypt EBS volumes that we use for ROSA, using a key stored in KMS.\n",
      "Customers have the option to provide their own KMS keys at cluster creation time\n",
      "as well.\n",
      "If I specify a KMS key to use, what exactly gets encrypted with that\n",
      "key?\n",
      "Control plane, infrastructure and worker node root volumes, along with your\n",
      "persistent volumes.\n",
      "Is data on my cluster encrypted?\n",
      "By default, there is encryption at rest. The AWS Storage platform automatically\n",
      "encrypts your data before persisting it, and decrypts the data before retrieval. See\n",
      "AWS EBS Encryption details. There is also the ability to encrypt etcd in the cluster,\n",
      "and that would combine with AWS storage encryption, resulting in double the\n",
      "encryption (redundant), which adds up to 20% performance hit. For further details\n",
      "see etcd encryption.\n",
      "When can etcd encryption be done with a ROSA cluster?\n",
      "Red Hat OpenShift\n",
      "Only at cluster creation time, can etcd encryption be enabled. Note that this\n",
      "incurs additional overhead with negligible security risk mitigation. See the prior\n",
      "question about EBS encryption.\n",
      "How is etcd encryption configured in a ROSA cluster?\n",
      "The same as in OCP. The aescbc cypher is used and the setting is patched during\n",
      "cluster deployment. Relevant Kubernetes documentation. For further details see\n",
      "etcd encryption.\n",
      "What infrastructure is provisioned as part of a new ROSA cluster?\n",
      "ROSA makes use of a number of different cloud services such as virtual machines,\n",
      "storage, load balancers, etc. You can see a defined list in the AWS prerequisites.\n",
      "I see there are two \"kinds\" of ROSA clusters. One uses an IAM user\n",
      "with admin permissions and the other AWS STS. Which should I\n",
      "choose?\n",
      "AWS STS. These aren't \"kinds\" but rather credential methods. Basically, \"how do\n",
      "you grant Red Hat the permissions needed in order to perform the required\n",
      "actions in your AWS account?\". The roadmap forward is focused on STS, and the\n",
      "IAM user method will eventually be deprecated. This better aligns with principles\n",
      "of least privilege and is much better aligned to secure practices in cloud service\n",
      "resource management. Please see the section \"ROSA with STS Explained\" for a\n",
      "detailed explanation.\n",
      "I’m seeing a number of permission or failure errors related to\n",
      "prerequisite tasks or cluster creation, what might be the problem?\n",
      "Please check for a newer version of the ROSA CLI. Every release of the ROSA CLI\n",
      "lands in two places: Github and the Red Hat signed binary releases.\n",
      "What are the available storage options?\n",
      "Please refer to the storage section of the service definition.\n",
      "What options are available to use shared storage in containers?\n",
      "AWS EFS (Using AWS EFS CSI Driver, OpenShift includes the CSI driver out of\n",
      "the box in 4.10.). See Setting up AWS EFS for Red Hat OpenShift Service on AWS.\n",
      "Can I deploy into an already existing VPC and choose the specific\n",
      "subnets?\n",
      "Yes. At install time you are able to select whether you’d like to deploy to an\n",
      "existing VPC and then choose that VPC. You will then be able to select the\n",
      "desired subnets and also provide a valid CIDR range (encompassing the subnets)\n",
      "the installer will handle using those subnets. Please see the VPC section in the\n",
      "documentation for further details.\n",
      "Which network plugin is used in Red Hat OpenShift Service on\n",
      "AWS?\n",
      "Red Hat OpenShift\n",
      "Red Hat OpenShift Service on AWS uses the default OpenShift SDN network\n",
      "provider configured to NetworkPolicy mode. OVN-Kubernetes is on our roadmap.\n",
      "Is cross-namespace networking supported?\n",
      "Cluster admins in ROSA can customize cross-namespace networking (including\n",
      "denying it) on a per project basis using NetworkPolicy objects. Refer to\n",
      "Configuring multitenant isolation with network policy on how to configure.\n",
      "Can more than one ROSA cluster be set up in one VPC?\n",
      "Yes, ROSA allows multiple clusters to share the same VPC. Essentially, the\n",
      "number of clusters would be limited by what AWS resource quota remains, as well\n",
      "as any chosen CIDR ranges that must not overlap. See CIDR Range Definitions for\n",
      "more information.\n",
      "Can I use Prometheus/Grafana to monitor containers and manage\n",
      "capacity?\n",
      "Yes, using OpenShift User Workload Monitoring. This is a check-box option in\n",
      "OpenShift Cluster Manager (console.redhat.com/openshift)\n",
      "Can I see audit logs output from the cluster control-plane?\n",
      "If the Cluster Logging Operator Add-on has been added to the cluster then audit\n",
      "logs are available through CloudWatch. If it has not, then a support request would\n",
      "allow you to request some audit logs. Small targeted and time-boxed logs can be\n",
      "requested for export and sent to a customer. The selection of audit logs available\n",
      "are at the discretion of SRE in the category of platform security and compliance.\n",
      "Requests for exports of a cluster’s entirety of logs will be rejected.\n",
      "Can I use an AWS Permissions Boundary around the policies for my\n",
      "cluster?\n",
      "Yes, using AWS Permissions Boundary is supported.\n",
      "Do ROSA worker nodes share the same AMI as other OpenShift\n",
      "products?\n",
      "ROSA worker nodes use a different AMI from OSD and OCP. Control Plane and\n",
      "Infra node AMIs are common across products in the same version.\n",
      "Are backups taken for clusters?\n",
      "Only non-STS clusters have SRE managed backups at this time, which means that\n",
      "ROSA STS clusters don’t have backups. You can also see our backup policy. It is\n",
      "imperative for users to have their own backup policies for applications and data.\n",
      "Is ROSA GDPR Compliant?\n",
      "Yes:  Learn more.Red Hat OpenShift\n",
      "Does the ROSA CLI accept Multi-region KMS keys for EBS\n",
      "encryption?\n",
      "Not at this time. This feature is in our backlog. Though we do accept single region\n",
      "KMS keys for EBS Encryption as long as it is defined at cluster creation time.\n",
      "Can I define a custom domain and certificate for my applications?\n",
      "Yes. See Configuring custom domains for applications for more information.\n",
      "How are the ROSA domain certificates managed?\n",
      "Red Hat infrastructure (Hive) manages certificate rotation for default application\n",
      "ingress (apps.*.openshiftapps.com)\n",
      "What features are upcoming for ROSA?\n",
      "The current ROSA roadmap can be seen at: https://red.ht/rosa-roadmap\n",
      "What kind of instances are supported for worker nodes?\n",
      "See AWS compute types in the service definition for the up to date list of\n",
      "supported instance types. Additionally, spot instances are supported.\n",
      "Does ROSA support an air-gapped, disconnected environment\n",
      "where the ROSA cluster does not have internet access?\n",
      "Is node autoscaling available?\n",
      "Yes. Autoscaling allows you to automatically adjust the size of the cluster based\n",
      "on the current workload. See About autoscaling nodes on a cluster in the\n",
      "documentation for more details.\n",
      "What is the maximum number of worker nodes that a cluster can\n",
      "support?\n",
      "The maximum number of worker nodes is 180 per ROSA cluster. See here for\n",
      "limits and scalability considerations and more details on node counts.\n",
      "Products\n",
      "Red Hat OpenShift\n",
      "About Red Hat Jobs\n",
      "Events Locations\n",
      "Contact Red Hat Red Hat Blog\n",
      "Diversity , equity , and inclusion Cool Stuf f Store\n",
      "Red Hat Summit\n",
      "© 2023 Red Hat, Inc.\n",
      "Privacy statement\n",
      "Terms of use\n",
      "All policies and guidelines\n",
      "Digital accessibility\n",
      "Cookie PreferencesTools\n",
      "Try, buy, & sell\n",
      "Communicate\n",
      "About Red Hat\n",
      "We’re the world’s leading provider of enterprise open source solutions—including Linux, cloud, container,\n",
      "and Kubernetes. We deliver hardened solutions that make it easier for enterprises to work across\n",
      "platforms and environments, from the core datacenter to the network edge.\n",
      "Subscribe to our newsletter, Red Hat Shares\n",
      "Sign up\n",
      "now \n",
      "Select a language\n",
      "English\n",
      "Red Hat OpenShift\n"
     ]
    }
   ],
   "source": [
    "print(text_faq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1e666fc-5339-4322-ba36-05b739d180e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Hat OpenShift Service on AWS\n",
      " \n",
      "4\n",
      "Cluster administration\n",
      "Configuring Red Hat OpenShift Service on AWS clusters\n",
      "Last Updated: 2023-01-26\n",
      "\n",
      "Red Hat OpenShift Service on AWS\n",
      " \n",
      "4\n",
      " \n",
      "Cluster administration\n",
      "Configuring Red Hat OpenShift Service on AWS clusters\n",
      "Legal Notice\n",
      "Copyright \n",
      "©\n",
      " 2023 Red Hat, Inc.\n",
      "The text of and illustrations in this document are licensed by Red Hat under a Creative Commons\n",
      "Attribution–Share Alike 3.0 Unported license (\"CC-BY-SA\"). An explanation of CC-BY-SA is\n",
      "available at\n",
      "http://creativecommons.org/licenses/by-sa/3.0/\n",
      ". In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must\n",
      "provide the URL for the original version.\n",
      "Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert,\n",
      "Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.\n",
      "Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift,\n",
      "Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States\n",
      "and other countries.\n",
      "Linux ®\n",
      " is the registered trademark of Linus Torvalds in the United States and other countries.\n",
      "Java ®\n",
      " is a registered trademark of Oracle and/or its affiliates.\n",
      "XFS ®\n",
      " is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States\n",
      "and/or other countries.\n",
      "MySQL ®\n",
      " is a registered trademark of MySQL AB in the United States, the European Union and\n",
      "other countries.\n",
      "Node.js ®\n",
      " is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the\n",
      "official Joyent Node.js open source or commercial project.\n",
      "The \n",
      "OpenStack ®\n",
      " Word Mark and OpenStack logo are either registered trademarks/service marks\n",
      "or trademarks/service marks of the OpenStack Foundation, in the United States and other\n",
      "countries and are used with the OpenStack Foundation's permission. We are not affiliated with,\n",
      "endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.\n",
      "All other trademarks are the property of their respective owners.\n",
      "Abstract\n",
      "This document provides information about configuring Red Hat OpenShift Service on AWS (ROSA)\n",
      "clusters.\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Table of Contents\n",
      "CHAPTER 1. CONFIGURING PRIVATE CONNECTIONS\n",
      "1.1. CONFIGURING PRIVATE CONNECTIONS\n",
      "1.2. CONFIGURING AWS VPC PEERING\n",
      "1.2.1. VPC peering terms\n",
      "1.2.2. Initiating the VPC peer request\n",
      "1.2.3. Accepting the VPC peer request\n",
      "1.2.4. Configuring the routing tables\n",
      "1.2.5. Verifying and troubleshooting VPC peering\n",
      "1.3. CONFIGURING AWS VPN\n",
      "1.3.1. Creating a VPN connection\n",
      "1.3.1.1. Configuring the VPN connection\n",
      "1.3.1.2. Establishing the VPN Connection\n",
      "1.3.1.3. Enabling VPN route propagation\n",
      "1.3.2. Verifying the VPN connection\n",
      "1.3.3. Troubleshooting the VPN connection\n",
      "Tunnel does not connect\n",
      "Tunnel does not stay connected\n",
      "Secondary tunnel in Down state\n",
      "1.4. CONFIGURING AWS DIRECT CONNECT\n",
      "1.4.1. AWS Direct Connect methods\n",
      "1.4.2. Creating the hosted Virtual Interface\n",
      "1.4.2.1. Determining the type of Direct Connect connection\n",
      "1.4.2.2. Creating a Private Direct Connect\n",
      "1.4.2.3. Creating a Public Direct Connect\n",
      "1.4.2.4. Verifying the Virtual Interfaces\n",
      "1.4.3. Connecting to an existing Direct Connect Gateway\n",
      "1.4.4. Troubleshooting Direct Connect\n",
      "CHAPTER 2. NODES\n",
      "2.1. ABOUT MACHINE POOLS\n",
      "2.1.1. Machines\n",
      "2.1.2. Machine sets\n",
      "2.1.3. Machine pools\n",
      "2.1.4. Machine pools in multiple zone clusters\n",
      "2.1.5. Additional resources\n",
      "2.2. MANAGING COMPUTE NODES\n",
      "2.2.1. Creating a machine pool\n",
      "2.2.1.1. Creating a machine pool using OpenShift Cluster Manager\n",
      "2.2.1.2. Creating a machine pool using the ROSA CLI\n",
      "2.2.2. Scaling compute nodes manually\n",
      "2.2.3. Node labels\n",
      "2.2.3.1. Adding node labels to a machine pool\n",
      "2.2.4. Adding taints to a machine pool\n",
      "2.2.5. Additional resources\n",
      "2.3. ABOUT AUTOSCALING NODES ON A CLUSTER\n",
      "2.3.1. Enabling autoscaling nodes on a cluster\n",
      "Enabling autoscaling nodes in an existing cluster using Red Hat OpenShift Cluster Manager\n",
      "Enabling autoscaling nodes in an existing cluster using the rosa CLI\n",
      "2.3.2. Disabling autoscaling nodes on a cluster\n",
      "Disabling autoscaling nodes in an existing cluster using Red Hat OpenShift Cluster Manager\n",
      "Disabling autoscaling nodes in an existing cluster using the rosa CLI\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "7\n",
      "7\n",
      "8\n",
      "9\n",
      "9\n",
      "10\n",
      "10\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "14\n",
      "14\n",
      "14\n",
      "15\n",
      "16\n",
      "16\n",
      "17\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "21\n",
      "24\n",
      "25\n",
      "25\n",
      "27\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "31\n",
      "31\n",
      "32\n",
      "32\n",
      "Table of Contents\n",
      "1\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "2.3.3. Additional resources\n",
      "CHAPTER 3. LOGGING\n",
      "3.1. ACCESSING THE SERVICE LOGS FOR ROSA CLUSTERS\n",
      "3.1.1. Viewing the service logs by using OpenShift Cluster Manager\n",
      "3.1.2. Adding cluster notification contacts\n",
      "3.2. INSTALLING LOGGING ADD-ON SERVICES\n",
      "3.2.1. Install the logging add-on service\n",
      "3.2.2. Additional resources\n",
      "3.3. VIEWING CLUSTER LOGS IN THE AWS CONSOLE\n",
      "3.3.1. Viewing forwarded logs\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "4.1. UNDERSTANDING THE MONITORING STACK\n",
      "4.1.1. Understanding the monitoring stack\n",
      "4.1.1.1. Components for monitoring user-defined projects\n",
      "4.1.1.2. Monitoring targets for user-defined projects\n",
      "4.1.2. Additional resources\n",
      "4.1.3. Next steps\n",
      "4.2. ACCESSING MONITORING FOR USER-DEFINED PROJECTS\n",
      "4.2.1. Next steps\n",
      "4.3. CONFIGURING THE MONITORING STACK\n",
      "4.3.1. Maintenance and support for monitoring\n",
      "4.3.1.1. Support considerations for monitoring user-defined projects\n",
      "4.3.2. Configuring the monitoring stack\n",
      "4.3.3. Configurable monitoring components\n",
      "4.3.4. Moving monitoring components to different nodes\n",
      "4.3.5. Assigning tolerations to components that monitor user-defined projects\n",
      "4.3.6. Configuring persistent storage\n",
      "4.3.6.1. Persistent storage prerequisites\n",
      "4.3.6.2. Configuring a local persistent volume claim\n",
      "4.3.6.3. Modifying the retention time for Prometheus metrics data\n",
      "4.3.7. Controlling the impact of unbound metrics attributes in user-defined projects\n",
      "4.3.7.1. Setting a scrape sample limit for user-defined projects\n",
      "4.3.8. Setting log levels for monitoring components\n",
      "4.3.9. Next steps\n",
      "4.4. ENABLING ALERT ROUTING FOR USER-DEFINED PROJECTS\n",
      "4.4.1. Understanding alert routing for user-defined projects\n",
      "4.4.2. Enabling a separate Alertmanager instance for user-defined alert routing\n",
      "4.4.3. Granting users permission to configure alert routing for user-defined projects\n",
      "4.5. MANAGING METRICS\n",
      "4.5.1. Understanding metrics\n",
      "4.5.2. Setting up metrics collection for user-defined projects\n",
      "4.5.2.1. Deploying a sample service\n",
      "4.5.2.2. Specifying how a service is monitored\n",
      "4.5.3. Querying metrics\n",
      "4.5.3.1. Querying metrics for all projects as an administrator\n",
      "4.5.3.2. Querying metrics for user-defined projects as a developer\n",
      "4.5.3.3. Exploring the visualized metrics\n",
      "4.5.4. Next steps\n",
      "4.6. ALERTS\n",
      "4.6.1. Accessing the Alerting UI in the Administrator and Developer perspectives\n",
      "4.6.2. Searching and filtering alerts, silences, and alerting rules\n",
      "32\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "34\n",
      "34\n",
      "36\n",
      "36\n",
      "36\n",
      "37\n",
      "37\n",
      "37\n",
      "37\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "39\n",
      "39\n",
      "39\n",
      "39\n",
      "39\n",
      "41\n",
      "41\n",
      "43\n",
      "44\n",
      "44\n",
      "45\n",
      "46\n",
      "48\n",
      "48\n",
      "49\n",
      "51\n",
      "51\n",
      "51\n",
      "52\n",
      "53\n",
      "53\n",
      "53\n",
      "54\n",
      "54\n",
      "56\n",
      "57\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "60\n",
      "60\n",
      "61\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "2\n",
      "Understanding alert filters\n",
      "Understanding silence filters\n",
      "Understanding alerting rule filters\n",
      "Searching and filtering alerts, silences, and alerting rules in the Developer perspective\n",
      "4.6.3. Getting information about alerts, silences, and alerting rules\n",
      "4.6.4. Managing silences\n",
      "4.6.4.1. Silencing alerts\n",
      "4.6.4.2. Editing silences\n",
      "4.6.4.3. Expiring silences\n",
      "4.6.5. Managing alerting rules for user-defined projects\n",
      "4.6.5.1. Optimizing alerting for user-defined projects\n",
      "4.6.5.2. Creating alerting rules for user-defined projects\n",
      "4.6.5.3. Reducing latency for alerting rules that do not query platform metrics\n",
      "4.6.5.4. Accessing alerting rules for user-defined projects\n",
      "4.6.5.5. Listing alerting rules for all projects in a single view\n",
      "4.6.5.6. Removing alerting rules for user-defined projects\n",
      "4.6.6. Applying a custom configuration to Alertmanager for user-defined alert routing\n",
      "4.6.7. Next steps\n",
      "4.7. REVIEWING MONITORING DASHBOARDS\n",
      "4.7.1. Reviewing monitoring dashboards as a developer\n",
      "4.7.2. Next steps\n",
      "4.8. TROUBLESHOOTING MONITORING ISSUES\n",
      "4.8.1. Determining why user-defined project metrics are unavailable\n",
      "61\n",
      "62\n",
      "62\n",
      "63\n",
      "63\n",
      "65\n",
      "65\n",
      "66\n",
      "67\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "72\n",
      "73\n",
      "74\n",
      "74\n",
      "74\n",
      "75\n",
      "75\n",
      "75\n",
      "Table of Contents\n",
      "3\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "4\n",
      "CHAPTER 1. CONFIGURING PRIVATE CONNECTIONS\n",
      "1.1. CONFIGURING PRIVATE CONNECTIONS\n",
      "Private cluster access can be implemented to suit the needs of your Red Hat OpenShift Service on AWS\n",
      "(ROSA) environment.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Access your ROSA AWS account and use one or more of the following methods to establish a\n",
      "private connection to your cluster:\n",
      "Configuring AWS VPC peering\n",
      ": Enable VPC peering to route network traffic between two\n",
      "private IP addresses.\n",
      "Configuring AWS VPN\n",
      ": Establish a Virtual Private Network to securely connect your private\n",
      "network to your Amazon Virtual Private Cloud.\n",
      "Configuring AWS Direct Connect\n",
      ": Configure AWS Direct Connect to establish a dedicated\n",
      "network connection between your private network and an AWS Direct Connect location.\n",
      "2\n",
      ". \n",
      "Configure a private cluster on ROSA\n",
      ".\n",
      "1.2. CONFIGURING AWS VPC PEERING\n",
      "This sample process configures an Amazon Web Services (AWS) VPC containing an Red Hat OpenShift\n",
      "Service on AWS cluster to peer with another AWS VPC network. For more information about creating an\n",
      "AWS VPC Peering connection or for other possible configurations, see the \n",
      "AWS VPC Peering\n",
      " guide.\n",
      "1.2.1. VPC peering terms\n",
      "When setting up a VPC peering connection between two VPCs on two separate AWS accounts, the\n",
      "following terms are used:\n",
      "Red Hat\n",
      "OpenShift\n",
      "Service on\n",
      "AWS AWS\n",
      "Account\n",
      "The AWS account that contains the Red Hat OpenShift Service on AWS cluster.\n",
      "Red Hat\n",
      "OpenShift\n",
      "Service on\n",
      "AWS Cluster\n",
      "VPC\n",
      "The VPC that contains the Red Hat OpenShift Service on AWS cluster.\n",
      "Customer\n",
      "AWS\n",
      "Account\n",
      "Your non-Red Hat OpenShift Service on AWS AWS Account that you would like to peer with.\n",
      "Customer\n",
      "VPC\n",
      "The VPC in your AWS Account that you would like to peer with.\n",
      "CHAPTER 1. CONFIGURING PRIVATE CONNECTIONS\n",
      "5\n",
      "Customer\n",
      "VPC Region\n",
      "The region where the customer’s VPC resides.\n",
      "NOTE\n",
      "As of July 2018, AWS supports inter-region VPC peering between all commercial regions\n",
      "excluding China\n",
      ".\n",
      "1.2.2. Initiating the VPC peer request\n",
      "You can send a VPC peering connection request from the Red Hat OpenShift Service on AWS AWS\n",
      "Account to the Customer AWS Account.\n",
      "Prerequisites\n",
      "Gather the following information about the Customer VPC required to initiate the peering\n",
      "request:\n",
      "Customer AWS account number\n",
      "Customer VPC ID\n",
      "Customer VPC Region\n",
      "Customer VPC CIDR\n",
      "Check the CIDR block used by the Red Hat OpenShift Service on AWS Cluster VPC. If it\n",
      "overlaps or matches the CIDR block for the Customer VPC, then peering between these two\n",
      "VPCs is not possible; see the Amazon VPC \n",
      "Unsupported VPC Peering Configurations\n",
      "documentation for details. If the CIDR blocks do not overlap, you can continue with the\n",
      "procedure.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Log in to the Web Console for the Red Hat OpenShift Service on AWS AWS Account and\n",
      "navigate to the \n",
      "VPC Dashboard\n",
      " in the region where the cluster is being hosted.\n",
      "2\n",
      ". \n",
      "Go to the \n",
      "Peering Connections\n",
      " page and click the \n",
      "Create Peering Connection\n",
      " button.\n",
      "3\n",
      ". \n",
      "Verify the details of the account you are logged in to and the details of the account and VPC\n",
      "you are connecting to:\n",
      "a\n",
      ". \n",
      "Peering connection name tag\n",
      ": Set a descriptive name for the VPC Peering Connection.\n",
      "b\n",
      ". \n",
      "VPC (Requester)\n",
      ": Select the Red Hat OpenShift Service on AWS Cluster VPC ID from the\n",
      "dropdown *list.\n",
      "c\n",
      ". \n",
      "Account\n",
      ": Select \n",
      "Another account\n",
      " and provide the Customer AWS Account number *\n",
      "(without dashes).\n",
      "d\n",
      ". \n",
      "Region\n",
      ": If the Customer VPC Region differs from the current region, select \n",
      "Another Region\n",
      "and select the customer VPC Region from the dropdown list.\n",
      "e\n",
      ". \n",
      "VPC (Accepter)\n",
      ": Set the Customer VPC ID.\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "6\n",
      "4\n",
      ". \n",
      "Click \n",
      "Create Peering Connection\n",
      ".\n",
      "5\n",
      ". \n",
      "Confirm that the request enters a \n",
      "Pending\n",
      " state. If it enters a \n",
      "Failed\n",
      " state, confirm the details\n",
      "and repeat the process.\n",
      "1.2.3. Accepting the VPC peer request\n",
      "After you create the VPC peering connection, you must accept the request in the Customer AWS\n",
      "Account.\n",
      "Prerequisites\n",
      "Initiate the VPC peer request.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Log in to the AWS Web Console.\n",
      "2\n",
      ". \n",
      "Navigate to \n",
      "VPC Service\n",
      ".\n",
      "3\n",
      ". \n",
      "Go to \n",
      "Peering Connections\n",
      ".\n",
      "4\n",
      ". \n",
      "Click on \n",
      "Pending peering connection\n",
      ".\n",
      "5\n",
      ". \n",
      "Confirm the AWS Account and VPC ID that the request originated from. This should be from\n",
      "the Red Hat OpenShift Service on AWS AWS Account and Red Hat OpenShift Service on AWS\n",
      "Cluster VPC.\n",
      "6\n",
      ". \n",
      "Click \n",
      "Accept Request\n",
      ".\n",
      "1.2.4. Configuring the routing tables\n",
      "After you accept the VPC peering request, both VPCs must configure their routes to communicate\n",
      "across the peering connection.\n",
      "Prerequisites\n",
      "Initiate and accept the VPC peer request.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Log in to the AWS Web Console for the Red Hat OpenShift Service on AWS AWS Account.\n",
      "2\n",
      ". \n",
      "Navigate to the \n",
      "VPC Service\n",
      ", then \n",
      "Route Tables\n",
      ".\n",
      "3\n",
      ". \n",
      "Select the Route Table for the Red Hat OpenShift Service on AWS Cluster VPC.\n",
      "NOTE\n",
      "On some clusters, there may be more than one route table for a particular VPC.\n",
      "Select the private one that has a number of explicitly associated subnets.\n",
      "4\n",
      ". \n",
      "Select the \n",
      "Routes\n",
      " tab, then \n",
      "Edit\n",
      ".\n",
      "CHAPTER 1. CONFIGURING PRIVATE CONNECTIONS\n",
      "7\n",
      "5\n",
      ". \n",
      "Enter the Customer VPC CIDR block in the \n",
      "Destination\n",
      " text box.\n",
      "6\n",
      ". \n",
      "Enter the Peering Connection ID in the \n",
      "Target\n",
      " text box.\n",
      "7\n",
      ". \n",
      "Click \n",
      "Save\n",
      ".\n",
      "8\n",
      ". \n",
      "You must complete the same process with the other VPC’s CIDR block:\n",
      "a\n",
      ". \n",
      "Log into the Customer AWS Web Console \n",
      "→\n",
      " \n",
      "VPC Service\n",
      " \n",
      "→\n",
      " \n",
      "Route Tables\n",
      ".\n",
      "b\n",
      ". \n",
      "Select the Route Table for your VPC.\n",
      "c\n",
      ". \n",
      "Select the \n",
      "Routes\n",
      " tab, then \n",
      "Edit\n",
      ".\n",
      "d\n",
      ". \n",
      "Enter the Red Hat OpenShift Service on AWS Cluster VPC CIDR block in the \n",
      "Destination\n",
      "text box.\n",
      "e\n",
      ". \n",
      "Enter the Peering Connection ID in the \n",
      "Target\n",
      " text box.\n",
      "f\n",
      ". \n",
      "Click \n",
      "Save\n",
      ".\n",
      "The VPC peering connection is now complete. Follow the verification procedure to ensure connectivity\n",
      "across the peering connection is working.\n",
      "1.2.5. Verifying and troubleshooting VPC peering\n",
      "After you set up a VPC peering connection, it is best to confirm it has been configured and is working\n",
      "correctly.\n",
      "Prerequisites\n",
      "Initiate and accept the VPC peer request.\n",
      "Configure the routing tables.\n",
      "Procedure\n",
      "In the AWS console, look at the route table for the cluster VPC that is peered. Ensure that the\n",
      "steps for configuring the routing tables were followed and that there is a route table entry\n",
      "pointing the VPC CIDR range destination to the peering connection target.\n",
      "If the routes look correct on both the Red Hat OpenShift Service on AWS Cluster VPC route\n",
      "table and Customer VPC route table, then the connection should be tested using the \n",
      "netcat\n",
      "method below. If the test calls are successful, then VPC peering is working correctly.\n",
      "To test network connectivity to an endpoint device, \n",
      "nc\n",
      " (or \n",
      "netcat\n",
      ") is a helpful troubleshooting\n",
      "tool. It is included in the default image and provides quick and clear output if a connection can\n",
      "be established:\n",
      "a\n",
      ". \n",
      "Create a temporary pod using the \n",
      "busybox\n",
      " image, which cleans up after itself:\n",
      "$ oc run netcat-test \\\n",
      "    --image=busybox -i -t \\\n",
      "    --restart=Never --rm \\\n",
      "    -- /bin/sh\n",
      "b\n",
      ". \n",
      "Check the connection using \n",
      "nc\n",
      ".\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "8\n",
      "Example successful connection results:\n",
      "/ nc -zvv 192.168.1.1 8080\n",
      "10.181.3.180 (10.181.3.180:8080) open\n",
      "sent 0, rcvd 0\n",
      "Example failed connection results:\n",
      "/ nc -zvv 192.168.1.2 8080\n",
      "nc: 10.181.3.180 (10.181.3.180:8081): Connection refused\n",
      "sent 0, rcvd 0\n",
      "c\n",
      ". \n",
      "Exit the container, which automatically deletes the Pod:\n",
      "/ exit\n",
      "1.3. CONFIGURING AWS VPN\n",
      "This sample process configures an Amazon Web Services (AWS) Red Hat OpenShift Service on AWS\n",
      "cluster to use a customer’s on-site hardware VPN device.\n",
      "NOTE\n",
      "AWS VPN does not currently provide a managed option to apply NAT to VPN traffic. See\n",
      "the \n",
      "AWS Knowledge Center\n",
      " for more details.\n",
      "NOTE\n",
      "Routing all traffic, for example \n",
      "0.0.0.0/0\n",
      ", through a private connection is not supported.\n",
      "This requires deleting the internet gateway, which disables SRE management traffic.\n",
      "For more information about connecting an AWS VPC to remote networks using a hardware VPN device,\n",
      "see the Amazon VPC \n",
      "VPN Connections\n",
      " documentation.\n",
      "1.3.1. Creating a VPN connection\n",
      "You can configure an Amazon Web Services (AWS) Red Hat OpenShift Service on AWS cluster to use a\n",
      "customer’s on-site hardware VPN device using the following procedures.\n",
      "Prerequisites\n",
      "Hardware VPN gateway device model and software version, for example Cisco ASA running\n",
      "version 8.3. See the Amazon VPC \n",
      "Network Administrator Guide\n",
      " to confirm whether your\n",
      "gateway device is supported by AWS.\n",
      "Public, static IP address for the VPN gateway device.\n",
      "BGP or static routing: if BGP, the ASN is required. If static routing, you must configure at least\n",
      "one static route.\n",
      "Optional: IP and Port/Protocol of a reachable service to test the VPN connection.\n",
      "CHAPTER 1. CONFIGURING PRIVATE CONNECTIONS\n",
      "9\n",
      "1.3.1.1. Configuring the VPN connection\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Log in to the Red Hat OpenShift Service on AWS AWS Account Dashboard, and navigate to the\n",
      "VPC Dashboard.\n",
      "2\n",
      ". \n",
      "Click on \n",
      "Your VPCs\n",
      " and identify the name and VPC ID for the VPC containing the Red Hat\n",
      "OpenShift Service on AWS cluster.\n",
      "3\n",
      ". \n",
      "From the VPC Dashboard, click \n",
      "Customer Gateway\n",
      ".\n",
      "4\n",
      ". \n",
      "Click \n",
      "Create Customer Gateway\n",
      " and give it a meaningful name.\n",
      "5\n",
      ". \n",
      "Select the routing method: \n",
      "Dynamic\n",
      " or \n",
      "Static\n",
      ".\n",
      "6\n",
      ". \n",
      "If Dynamic, enter the BGP ASN in the field that appears.\n",
      "7\n",
      ". \n",
      "Paste in the VPN gateway endpoint IP address.\n",
      "8\n",
      ". \n",
      "Click \n",
      "Create\n",
      ".\n",
      "9\n",
      ". \n",
      "If you do not already have a Virtual Private Gateway attached to the intended VPC:\n",
      "a\n",
      ". \n",
      "From the VPC Dashboard, click on \n",
      "Virtual Private Gateway\n",
      ".\n",
      "b\n",
      ". \n",
      "Click \n",
      "Create Virtual Private Gateway\n",
      ", give it a meaningful name, and click \n",
      "Create\n",
      ".\n",
      "c\n",
      ". \n",
      "Leave the default Amazon default ASN.\n",
      "d\n",
      ". \n",
      "Select the newly created gateway, click \n",
      "Attach to VPC\n",
      ", and attach it to the cluster VPC you\n",
      "identified earlier.\n",
      "1.3.1.2. Establishing the VPN Connection\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "From the VPC dashboard, click on \n",
      "Site-to-Site VPN Connections\n",
      ".\n",
      "2\n",
      ". \n",
      "Click \n",
      "Create VPN Connection\n",
      ".\n",
      "a\n",
      ". \n",
      "Give it a meaningful name tag.\n",
      "b\n",
      ". \n",
      "Select the virtual private gateway created previously.\n",
      "c\n",
      ". \n",
      "For Customer Gateway, select \n",
      "Existing\n",
      ".\n",
      "d\n",
      ". \n",
      "Select the customer gateway device by name.\n",
      "e\n",
      ". \n",
      "If the VPN will use BGP, select \n",
      "Dynamic\n",
      ", otherwise select \n",
      "Static\n",
      ". Enter Static IP CIDRs. If\n",
      "there are multiple CIDRs, add each CIDR as \n",
      "Another Rule\n",
      ".\n",
      "f\n",
      ". \n",
      "Click \n",
      "Create\n",
      ".\n",
      "g\n",
      ". \n",
      "Wait for VPN status to change to \n",
      "Available\n",
      ", approximately 5 to 10 minutes.\n",
      "3\n",
      ". \n",
      "Select the VPN you just created and click \n",
      "Download Configuration\n",
      ".\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "10\n",
      "a\n",
      ". \n",
      "From the dropdown list, select the vendor, platform, and version of the customer gateway\n",
      "device, then click \n",
      "Download\n",
      ".\n",
      "b\n",
      ". \n",
      "The \n",
      "Generic\n",
      " vendor configuration is also available for retrieving information in a plain text\n",
      "format.\n",
      "NOTE\n",
      "After the VPN connection has been established, be sure to set up Route Propagation or\n",
      "the VPN may not function as expected.\n",
      "NOTE\n",
      "Note the VPC subnet information, which you must add to your configuration as the\n",
      "remote network.\n",
      "1.3.1.3. Enabling VPN route propagation\n",
      "After you have set up the VPN connection, you must ensure that route propagation is enabled so that\n",
      "the necessary routes are added to the VPC’s route table.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "From the VPC Dashboard, click on \n",
      "Route Tables\n",
      ".\n",
      "2\n",
      ". \n",
      "Select the private Route table associated with the VPC that contains your Red Hat OpenShift\n",
      "Service on AWS cluster.\n",
      "NOTE\n",
      "On some clusters, there may be more than one route table for a particular VPC.\n",
      "Select the private one that has a number of explicitly associated subnets.\n",
      "3\n",
      ". \n",
      "Click on the \n",
      "Route Propagation\n",
      " tab.\n",
      "4\n",
      ". \n",
      "In the table that appears, you should see the virtual private gateway you created previously.\n",
      "Check the value in the \n",
      "Propagate column\n",
      ".\n",
      "a\n",
      ". \n",
      "If Propagate is set to \n",
      "No\n",
      ", click \n",
      "Edit route propagation\n",
      ", check the Propagate checkbox next\n",
      "to the virtual private gateway’s name and click \n",
      "Save\n",
      ".\n",
      "After you configure your VPN tunnel and AWS detects it as \n",
      "Up\n",
      ", your static or BGP routes are\n",
      "automatically added to the route table.\n",
      "1.3.2. Verifying the VPN connection\n",
      "After you have set up your side of the VPN tunnel, you can verify that the tunnel is up in the AWS\n",
      "console and that connectivity across the tunnel is working.\n",
      "Prerequisites\n",
      "Created a VPN connection.\n",
      "Procedure\n",
      "CHAPTER 1. CONFIGURING PRIVATE CONNECTIONS\n",
      "11\n",
      "1\n",
      ". \n",
      "Verify the tunnel is up in AWS.\n",
      "a\n",
      ". \n",
      "From the VPC Dashboard, click on \n",
      "VPN Connections\n",
      ".\n",
      "b\n",
      ". \n",
      "Select the VPN connection you created previously and click the \n",
      "Tunnel Details\n",
      " tab.\n",
      "c\n",
      ". \n",
      "You should be able to see that at least one of the VPN tunnels is \n",
      "Up\n",
      ".\n",
      "2\n",
      ". \n",
      "Verify the connection.\n",
      "To test network connectivity to an endpoint device, \n",
      "nc\n",
      " (or \n",
      "netcat\n",
      ") is a helpful troubleshooting\n",
      "tool. It is included in the default image and provides quick and clear output if a connection can\n",
      "be established:\n",
      "a\n",
      ". \n",
      "Create a temporary pod using the \n",
      "busybox\n",
      " image, which cleans up after itself:\n",
      "$ oc run netcat-test \\\n",
      "    --image=busybox -i -t \\\n",
      "    --restart=Never --rm \\\n",
      "    -- /bin/sh\n",
      "b\n",
      ". \n",
      "Check the connection using \n",
      "nc\n",
      ".\n",
      "Example successful connection results:\n",
      "/ nc -zvv 192.168.1.1 8080\n",
      "10.181.3.180 (10.181.3.180:8080) open\n",
      "sent 0, rcvd 0\n",
      "Example failed connection results:\n",
      "/ nc -zvv 192.168.1.2 8080\n",
      "nc: 10.181.3.180 (10.181.3.180:8081): Connection refused\n",
      "sent 0, rcvd 0\n",
      "c\n",
      ". \n",
      "Exit the container, which automatically deletes the Pod:\n",
      "/ exit\n",
      "1.3.3. Troubleshooting the VPN connection\n",
      "Tunnel does not connect\n",
      "If the tunnel connection is still \n",
      "Down\n",
      ", there are several things you can verify:\n",
      "The AWS tunnel will not initiate a VPN connection. The connection attempt must be initiated\n",
      "from the Customer Gateway.\n",
      "Ensure that your source traffic is coming from the same IP as the configured customer gateway.\n",
      "AWS will silently drop all traffic to the gateway whose source IP address does not match.\n",
      "Ensure that your configuration matches values \n",
      "supported by AWS\n",
      ". This includes IKE versions,\n",
      "DH groups, IKE lifetime, and more.\n",
      "Recheck the route table for the VPC. Ensure that propagation is enabled and that there are\n",
      "entries in the route table that have the virtual private gateway you created earlier as a target.\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "12\n",
      "Confirm that you do not have any firewall rules that could be causing an interruption.\n",
      "Check if you are using a policy-based VPN as this can cause complications depending on how it\n",
      "is configured.\n",
      "Further troubleshooting steps can be found at the \n",
      "AWS Knowledge Center\n",
      ".\n",
      "Tunnel does not stay connected\n",
      "If the tunnel connection has trouble staying \n",
      "Up\n",
      " consistently, know that all AWS tunnel connections must\n",
      "be initiated from your gateway. AWS tunnels \n",
      "do not initiate tunneling\n",
      ".\n",
      "Red Hat recommends setting up an SLA Monitor (Cisco ASA) or some device on your side of the tunnel\n",
      "that constantly sends \"interesting\" traffic, for example \n",
      "ping\n",
      ", \n",
      "nc\n",
      ", or \n",
      "telnet\n",
      ", at any IP address configured\n",
      "within the VPC CIDR range. It does not matter whether the connection is successful, just that the traffic\n",
      "is being directed at the tunnel.\n",
      "Secondary tunnel in Down state\n",
      "When a VPN tunnel is created, AWS creates an additional failover tunnel. Depending upon the gateway\n",
      "device, sometimes the secondary tunnel will be seen as in the \n",
      "Down\n",
      " state.\n",
      "The AWS Notification is as follows:\n",
      "You have new non-redundant VPN connections\n",
      "One or more of your vpn connections are not using both tunnels. This mode of\n",
      "operation is not highly available and we strongly recommend you configure your\n",
      "second tunnel. View your non-redundant VPN connections.\n",
      "1.4. CONFIGURING AWS DIRECT CONNECT\n",
      "This process describes accepting an AWS Direct Connect virtual interface with Red Hat OpenShift\n",
      "Service on AWS. For more information about AWS Direct Connect types and configuration, see the\n",
      "AWS Direct Connect components\n",
      " documentation.\n",
      "1.4.1. AWS Direct Connect methods\n",
      "A Direct Connect connection requires a hosted Virtual Interface (VIF) connected to a Direct Connect\n",
      "Gateway (DXGateway), which is in turn associated to a Virtual Gateway (VGW) or a Transit Gateway in\n",
      "order to access a remote VPC in the same or another account.\n",
      "If you do not have an existing DXGateway, the typical process involves creating the hosted VIF, with the\n",
      "DXGateway and VGW being created in the Red Hat OpenShift Service on AWS AWS Account.\n",
      "If you have an existing DXGateway connected to one or more existing VGWs, the process involves the\n",
      "Red Hat OpenShift Service on AWS AWS Account sending an Association Proposal to the DXGateway\n",
      "owner. The DXGateway owner must ensure that the proposed CIDR will not conflict with any other\n",
      "VGWs they have associated.\n",
      "See the following AWS documentation for more details:\n",
      "Virtual Interfaces\n",
      "Direct Connect Gateways\n",
      "Associating a VGW across accounts\n",
      "IMPORTANT\n",
      "CHAPTER 1. CONFIGURING PRIVATE CONNECTIONS\n",
      "13\n",
      "IMPORTANT\n",
      "When connecting to an existing DXGateway, you are responsible for the \n",
      "costs\n",
      ".\n",
      "There are two configuration options available:\n",
      "Method 1\n",
      "Create the hosted VIF and then the DXGateway and VGW.\n",
      "Method 2\n",
      "Request a connection via an existing Direct Connect Gateway that you own.\n",
      "1.4.2. Creating the hosted Virtual Interface\n",
      "Prerequisites\n",
      "Gather Red Hat OpenShift Service on AWS AWS Account ID.\n",
      "1.4.2.1. Determining the type of Direct Connect connection\n",
      "View the Direct Connect Virtual Interface details to determine the type of connection.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Log in to the Red Hat OpenShift Service on AWS AWS Account Dashboard and select the\n",
      "correct region.\n",
      "2\n",
      ". \n",
      "Select \n",
      "Direct Connect\n",
      " from the \n",
      "Services\n",
      " menu.\n",
      "3\n",
      ". \n",
      "There will be one or more Virtual Interfaces waiting to be accepted, select one of them to view\n",
      "the \n",
      "Summary\n",
      ".\n",
      "4\n",
      ". \n",
      "View the Virtual Interface type: private or public.\n",
      "5\n",
      ". \n",
      "Record the \n",
      "Amazon side ASN\n",
      " value.\n",
      "If the Direct Connect Virtual Interface type is Private, a Virtual Private Gateway is created. If the Direct\n",
      "Connect Virtual Interface is Public, a Direct Connect Gateway is created.\n",
      "1.4.2.2. Creating a Private Direct Connect\n",
      "A Private Direct Connect is created if the Direct Connect Virtual Interface type is Private.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Log in to the Red Hat OpenShift Service on AWS AWS Account Dashboard and select the\n",
      "correct region.\n",
      "2\n",
      ". \n",
      "From the AWS region, select \n",
      "VPC\n",
      " from the \n",
      "Services\n",
      " menu.\n",
      "3\n",
      ". \n",
      "Select \n",
      "Virtual Private Gateways\n",
      " from \n",
      "VPN Connections\n",
      ".\n",
      "4\n",
      ". \n",
      "Click \n",
      "Create Virtual Private Gateway\n",
      ".\n",
      "5\n",
      ". \n",
      "Give the Virtual Private Gateway a suitable name.\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "14\n",
      "6\n",
      ". \n",
      "Select \n",
      "Custom ASN\n",
      " and enter the \n",
      "Amazon side ASN\n",
      " value gathered previously.\n",
      "7\n",
      ". \n",
      "Create the Virtual Private Gateway.\n",
      "8\n",
      ". \n",
      "Click the newly created Virtual Private Gateway and choose \n",
      "Attach to VPC\n",
      " from the \n",
      "Actions\n",
      "tab.\n",
      "9\n",
      ". \n",
      "Select the \n",
      "Red Hat OpenShift Service on AWS Cluster VPC\n",
      " from the list, and attach the\n",
      "Virtual Private Gateway to the VPC.\n",
      "10\n",
      ". \n",
      "From the \n",
      "Services\n",
      " menu, click \n",
      "Direct Connect\n",
      ". Choose one of the Direct Connect Virtual\n",
      "Interfaces from the list.\n",
      "11\n",
      ". \n",
      "Acknowledge the \n",
      "I understand that Direct Connect port charges apply once I click Accept\n",
      "Connection\n",
      " message, then choose \n",
      "Accept Connection\n",
      ".\n",
      "12\n",
      ". \n",
      "Choose to \n",
      "Accept\n",
      " the Virtual Private Gateway Connection and select the Virtual Private\n",
      "Gateway that was created in the previous steps.\n",
      "13\n",
      ". \n",
      "Select \n",
      "Accept\n",
      " to accept the connection.\n",
      "14\n",
      ". \n",
      "Repeat the previous steps if there is more than one Virtual Interface.\n",
      "1.4.2.3. Creating a Public Direct Connect\n",
      "A Public Direct Connect is created if the Direct Connect Virtual Interface type is Public.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Log in to the Red Hat OpenShift Service on AWS AWS Account Dashboard and select the\n",
      "correct region.\n",
      "2\n",
      ". \n",
      "From the Red Hat OpenShift Service on AWS AWS Account region, select \n",
      "Direct Connect\n",
      " from\n",
      "the \n",
      "Services\n",
      " menu.\n",
      "3\n",
      ". \n",
      "Select \n",
      "Direct Connect Gateways\n",
      " and \n",
      "Create Direct Connect Gateway\n",
      ".\n",
      "4\n",
      ". \n",
      "Give the Direct Connect Gateway a suitable name.\n",
      "5\n",
      ". \n",
      "In the \n",
      "Amazon side ASN\n",
      ", enter the Amazon side ASN value gathered previously.\n",
      "6\n",
      ". \n",
      "Create the Direct Connect Gateway.\n",
      "7\n",
      ". \n",
      "Select \n",
      "Direct Connect\n",
      " from the \n",
      "Services\n",
      " menu.\n",
      "8\n",
      ". \n",
      "Select one of the Direct Connect Virtual Interfaces from the list.\n",
      "9\n",
      ". \n",
      "Acknowledge the \n",
      "I understand that Direct Connect port charges apply once I click Accept\n",
      "Connection\n",
      " message, then choose \n",
      "Accept Connection\n",
      ".\n",
      "10\n",
      ". \n",
      "Choose to \n",
      "Accept\n",
      " the Direct Connect Gateway Connection and select the Direct Connect\n",
      "Gateway that was created in the previous steps.\n",
      "11\n",
      ". \n",
      "Click \n",
      "Accept\n",
      " to accept the connection.\n",
      "12\n",
      ". \n",
      "Repeat the previous steps if there is more than one Virtual Interface.\n",
      "CHAPTER 1. CONFIGURING PRIVATE CONNECTIONS\n",
      "15\n",
      "1.4.2.4. Verifying the Virtual Interfaces\n",
      "After the Direct Connect Virtual Interfaces have been accepted, wait a short period and view the status\n",
      "of the Interfaces.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Log in to the Red Hat OpenShift Service on AWS AWS Account Dashboard and select the\n",
      "correct region.\n",
      "2\n",
      ". \n",
      "From the Red Hat OpenShift Service on AWS AWS Account region, select \n",
      "Direct Connect\n",
      " from\n",
      "the \n",
      "Services\n",
      " menu.\n",
      "3\n",
      ". \n",
      "Select one of the Direct Connect Virtual Interfaces from the list.\n",
      "4\n",
      ". \n",
      "Check the Interface State has become \n",
      "Available\n",
      "5\n",
      ". \n",
      "Check the Interface BGP Status has become \n",
      "Up\n",
      ".\n",
      "6\n",
      ". \n",
      "Repeat this verification for any remaining Direct Connect Interfaces.\n",
      "After the Direct Connect Virtual Interfaces are available, you can log in to the Red Hat OpenShift\n",
      "Service on AWS AWS Account Dashboard and download the Direct Connect configuration file for\n",
      "configuration on your side.\n",
      "1.4.3. Connecting to an existing Direct Connect Gateway\n",
      "Prerequisites\n",
      "Confirm the CIDR range of the Red Hat OpenShift Service on AWS VPC will not conflict with\n",
      "any other VGWs you have associated.\n",
      "Gather the following information:\n",
      "The Direct Connect Gateway ID.\n",
      "The AWS Account ID associated with the virtual interface.\n",
      "The BGP ASN assigned for the DXGateway. Optional: the Amazon default ASN may also be\n",
      "used.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Log in to the Red Hat OpenShift Service on AWS AWS Account Dashboard and select the\n",
      "correct region.\n",
      "2\n",
      ". \n",
      "From the Red Hat OpenShift Service on AWS AWS Account region, select \n",
      "VPC\n",
      " from the\n",
      "Services\n",
      " menu.\n",
      "3\n",
      ". \n",
      "From \n",
      "VPN Connections\n",
      ", select \n",
      "Virtual Private Gateways\n",
      ".\n",
      "4\n",
      ". \n",
      "Select \n",
      "Create Virtual Private Gateway\n",
      ".\n",
      "5\n",
      ". \n",
      "Give the Virtual Private Gateway a suitable name.\n",
      "6\n",
      ". \n",
      "Click \n",
      "Custom ASN\n",
      " and enter the \n",
      "Amazon side ASN\n",
      " value gathered previously or use the\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "16\n",
      "6\n",
      ". \n",
      "Click \n",
      "Custom ASN\n",
      " and enter the \n",
      "Amazon side ASN\n",
      " value gathered previously or use the\n",
      "Amazon Provided ASN.\n",
      "7\n",
      ". \n",
      "Create the Virtual Private Gateway.\n",
      "8\n",
      ". \n",
      "In the \n",
      "Navigation\n",
      " pane of the Red Hat OpenShift Service on AWS AWS Account Dashboard,\n",
      "choose \n",
      "Virtual private gateways\n",
      " and select the virtual private gateway. Choose \n",
      "View details\n",
      ".\n",
      "9\n",
      ". \n",
      "Choose \n",
      "Direct Connect gateway associations\n",
      " and click \n",
      "Associate Direct Connect gateway\n",
      ".\n",
      "10\n",
      ". \n",
      "Under \n",
      "Association account type\n",
      ", for Account owner, choose \n",
      "Another account\n",
      ".\n",
      "11\n",
      ". \n",
      "For \n",
      "Direct Connect gateway owner\n",
      ", enter the ID of the AWS account that owns the Direct\n",
      "Connect gateway.\n",
      "12\n",
      ". \n",
      "Under \n",
      "Association settings\n",
      ", for Direct Connect gateway ID, enter the ID of the Direct Connect\n",
      "gateway.\n",
      "13\n",
      ". \n",
      "Under \n",
      "Association settings\n",
      ", for Virtual interface owner, enter the ID of the AWS account that\n",
      "owns the virtual interface for the association.\n",
      "14\n",
      ". \n",
      "Optional: Add prefixes to Allowed prefixes, separating them using commas.\n",
      "15\n",
      ". \n",
      "Choose \n",
      "Associate Direct Connect gateway\n",
      ".\n",
      "16\n",
      ". \n",
      "After the Association Proposal has been sent, it will be waiting for your acceptance. The final\n",
      "steps you must perform are available in the \n",
      "AWS Documentation\n",
      ".\n",
      "1.4.4. Troubleshooting Direct Connect\n",
      "Further troubleshooting can be found in the \n",
      "Troubleshooting AWS Direct Connect\n",
      " documentation.\n",
      "CHAPTER 1. CONFIGURING PRIVATE CONNECTIONS\n",
      "17\n",
      "CHAPTER 2. NODES\n",
      "2.1. ABOUT MACHINE POOLS\n",
      "Red Hat OpenShift Service on AWS uses machine pools as an elastic, dynamic provisioning method on\n",
      "top of your cloud infrastructure.\n",
      "The primary resources are machines, compute machine sets, and machine pools.\n",
      "IMPORTANT\n",
      "As of the Red Hat OpenShift Service on AWS versions 4.8.35, 4.9.26, 4.10.6, the Red Hat\n",
      "OpenShift Service on AWS default per-pod pid limit is \n",
      "4096\n",
      ". If you want to enable this\n",
      "PID limit, you must upgrade your Red Hat OpenShift Service on AWS clusters to these\n",
      "versions or later. Red Hat OpenShift Service on AWS clusters with prior versions use a\n",
      "default PID limit of \n",
      "1024\n",
      ".\n",
      "You cannot configure the per-pod PID limit on any Red Hat OpenShift Service on AWS\n",
      "cluster.\n",
      "2.1.1. Machines\n",
      "A machine is a fundamental unit that describes the host for a worker node.\n",
      "2.1.2. Machine sets\n",
      "MachineSet\n",
      " resources are groups of compute machines. If you need more machines or must scale them\n",
      "down, change the number of replicas in the machine pool to which the compute machine sets belong.\n",
      "Machine sets are not directly modifiable in ROSA.\n",
      "2.1.3. Machine pools\n",
      "Machine pools are a higher level construct to compute machine sets.\n",
      "A machine pool creates compute machine sets that are all clones of the same configuration across\n",
      "availability zones. Machine pools perform all of the host node provisioning management actions on a\n",
      "worker node. If you need more machines or must scale them down, change the number of replicas in the\n",
      "machine pool to meet your compute needs. You can manually configure scaling or set autoscaling.\n",
      "By default, a cluster is created with one machine pool. You can add additional machine pools to an\n",
      "existing cluster, modify the default machine pool, and delete machine pools.\n",
      "Multiple machine pools can exist on a single cluster, and they can each have different types or different\n",
      "size nodes.\n",
      "2.1.4. Machine pools in multiple zone clusters\n",
      "When you create a machine pool in a multiple availability zone (Multi-AZ) cluster, that one machine pool\n",
      "has 3 zones. The machine pool, in turn, creates a total of 3 compute machine sets - one for each zone in\n",
      "the cluster. Each of those compute machine sets manages one or more machines in its respective\n",
      "availability zone.\n",
      "If you create a new Multi-AZ cluster, the machine pools are replicated to those zones automatically. If\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "18\n",
      "you add a machine pool to an existing Multi-AZ, the new pool is automatically created in those zones.\n",
      "Similarly, deleting a machine pool will delete it from all zones. Due to this multiplicative effect, using\n",
      "machine pools in Multi-AZ cluster can consume more of your project’s quota for a specific region when\n",
      "creating machine pools.\n",
      "2.1.5. Additional resources\n",
      "Managing worker nodes\n",
      "About autoscaling\n",
      "2.2. MANAGING COMPUTE NODES\n",
      "This document describes how to manage compute (also known as worker) nodes with Red Hat\n",
      "OpenShift Service on AWS (ROSA).\n",
      "The majority of changes for compute nodes are configured on machine pools. A machine pool is a group\n",
      "of compute nodes in a cluster that have the same configuration, providing ease of management.\n",
      "You can edit machine pool configuration options such as scaling, adding node labels, and adding taints.\n",
      "2.2.1. Creating a machine pool\n",
      "A default machine pool is created when you install a Red Hat OpenShift Service on AWS (ROSA) cluster.\n",
      "After installation, you can create additional machine pools for your cluster by using OpenShift Cluster\n",
      "Manager or the ROSA CLI (\n",
      "rosa\n",
      ").\n",
      "2.2.1.1. Creating a machine pool using OpenShift Cluster Manager\n",
      "You can create additional machine pools for your Red Hat OpenShift Service on AWS (ROSA) cluster by\n",
      "using OpenShift Cluster Manager.\n",
      "Prerequisites\n",
      "You created a ROSA cluster.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Navigate to \n",
      "OpenShift Cluster Manager Hybrid Cloud Console\n",
      " and select your cluster.\n",
      "2\n",
      ". \n",
      "Under the \n",
      "Machine pools\n",
      " tab, click \n",
      "Add machine pool\n",
      ".\n",
      "3\n",
      ". \n",
      "Add a \n",
      "Machine pool name\n",
      ".\n",
      "4\n",
      ". \n",
      "Select a \n",
      "Worker node instance type\n",
      " from the drop-down menu. The instance type defines the\n",
      "vCPU and memory allocation for each compute node in the machine pool.\n",
      "NOTE\n",
      "You cannot change the instance type for a machine pool after the pool is\n",
      "created.\n",
      "5\n",
      ". \n",
      "Optional: Configure autoscaling for the machine pool:\n",
      "a\n",
      ". \n",
      "Select \n",
      "Enable autoscaling\n",
      " to automatically scale the number of machines in your machine\n",
      "CHAPTER 2. NODES\n",
      "19\n",
      "a\n",
      ". \n",
      "Select \n",
      "Enable autoscaling\n",
      " to automatically scale the number of machines in your machine\n",
      "pool to meet the deployment needs.\n",
      "b\n",
      ". \n",
      "Set the minimum and maximum node count limits for autoscaling. The cluster autoscaler\n",
      "does not reduce or increase the machine pool node count beyond the limits that you\n",
      "specify.\n",
      "If you deployed your cluster using a single availability zone, set the \n",
      "Minimum and\n",
      "maximum node count\n",
      ". This defines the minimum and maximum compute node limits in\n",
      "the availability zone.\n",
      "If you deployed your cluster using multiple availability zones, set the \n",
      "Minimum nodes\n",
      "per zone\n",
      " and \n",
      "Maximum nodes per zone\n",
      ". This defines the minimum and maximum\n",
      "compute node limits per zone.\n",
      "NOTE\n",
      "Alternatively, you can set your autoscaling preferences for the machine\n",
      "pool after the machine pool is created.\n",
      "6\n",
      ". \n",
      "If you did not enable autoscaling, select a compute node count:\n",
      "If you deployed your cluster using a single availability zone, select a \n",
      "Worker node count\n",
      "from the drop-down menu. This defines the number of compute nodes to provision to the\n",
      "machine pool for the zone.\n",
      "If you deployed your cluster using multiple availability zones, select a \n",
      "Worker node count\n",
      "(per zone)\n",
      " from the drop-down menu. This defines the number of compute nodes to\n",
      "provision to the machine pool per zone.\n",
      "7\n",
      ". \n",
      "Optional: Add node labels and taints for your machine pool:\n",
      "a\n",
      ". \n",
      "Expand the \n",
      "Edit node labels and taints\n",
      " menu.\n",
      "b\n",
      ". \n",
      "Under \n",
      "Node labels\n",
      ", add \n",
      "Key\n",
      " and \n",
      "Value\n",
      " entries for your node labels.\n",
      "c\n",
      ". \n",
      "Under \n",
      "Taints\n",
      ", add \n",
      "Key\n",
      " and \n",
      "Value\n",
      " entries for your taints.\n",
      "d\n",
      ". \n",
      "For each taint, select an \n",
      "Effect\n",
      " from the drop-down menu. Available options include \n",
      "NoSchedule\n",
      ", \n",
      "PreferNoSchedule\n",
      ", and \n",
      "NoExecute\n",
      ".\n",
      "NOTE\n",
      "Alternatively, you can add the node labels and taints after you create the\n",
      "machine pool.\n",
      "8\n",
      ". \n",
      "Optional: Use Amazon EC2 Spot Instances if you want to configure your machine pool to deploy\n",
      "machines as non-guaranteed AWS Spot Instances:\n",
      "a\n",
      ". \n",
      "Select \n",
      "Use Amazon EC2 Spot Instances\n",
      ".\n",
      "b\n",
      ". \n",
      "Leave \n",
      "Use On-Demand instance price\n",
      " selected to use the on-demand instance price.\n",
      "Alternatively, select \n",
      "Set maximum price\n",
      " to define a maximum hourly price for a Spot\n",
      "Instance.\n",
      "For more information about Amazon EC2 Spot Instances, see the \n",
      "AWS documentation\n",
      ".\n",
      "IMPORTANT\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "20\n",
      "1\n",
      "2\n",
      "IMPORTANT\n",
      "Your Amazon EC2 Spot Instances might be interrupted at any time. Use\n",
      "Amazon EC2 Spot Instances only for workloads that can tolerate\n",
      "interruptions.\n",
      "NOTE\n",
      "If you select \n",
      "Use Amazon EC2 Spot Instances\n",
      " for a machine pool, you\n",
      "cannot disable the option after the machine pool is created.\n",
      "9\n",
      ". \n",
      "Click \n",
      "Add machine pool\n",
      " to create the machine pool.\n",
      "Verification\n",
      "Verify that the machine pool is visible on the \n",
      "Machine pools\n",
      " page and the configuration is as\n",
      "expected.\n",
      "2.2.1.2. Creating a machine pool using the ROSA CLI\n",
      "You can create additional machine pools for your Red Hat OpenShift Service on AWS (ROSA) cluster by\n",
      "using the ROSA CLI (\n",
      "rosa\n",
      ").\n",
      "Prerequisites\n",
      "You installed and configured the latest AWS (\n",
      "aws\n",
      "), ROSA (\n",
      "rosa\n",
      "), and OpenShift (\n",
      "oc\n",
      ") CLIs on\n",
      "your workstation.\n",
      "You logged in to your Red Hat account by using the \n",
      "rosa\n",
      " CLI.\n",
      "You created a ROSA cluster.\n",
      "Procedure\n",
      "To add a machine pool that does not use autoscaling, create the machine pool and define the\n",
      "instance type, compute (also known as worker) node count, and node labels:\n",
      "Specifies the name of the machine pool. Replace \n",
      "<machine_pool_id>\n",
      " with the name of\n",
      "your machine pool.\n",
      "Specifies the number of compute nodes to provision. If you deployed ROSA using a single\n",
      "availability zone, this defines the number of compute nodes to provision to the machine\n",
      "pool for the zone. If you deployed your cluster using multiple availability zones, this defines\n",
      "the number of compute nodes to provision in total across all zones and the count must be a\n",
      "$ rosa create machinepool --cluster=<cluster-name> \\\n",
      "                          --name=<machine_pool_id> \\ \n",
      "1\n",
      "                          --replicas=<replica_count> \\ \n",
      "2\n",
      "                          --instance-type=<instance_type> \\ \n",
      "3\n",
      "                          --labels=<key>=<value>,<key>=<value> \\ \n",
      "4\n",
      "                          --taints=<key>=<value>:<effect>,<key>=<value>:<effect> \\ \n",
      "5\n",
      "                          --use-spot-instances \\ \n",
      "6\n",
      "                          --spot-max-price=0.5 \n",
      "7\n",
      "CHAPTER 2. NODES\n",
      "21\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "multiple of 3. The \n",
      "--replicas\n",
      " argument is required when autoscaling is not configured.\n",
      "Optional: Sets the instance type for the compute nodes in your machine pool. The\n",
      "instance type defines the vCPU and memory allocation for each compute node in the pool.\n",
      "Replace \n",
      "<instance_type>\n",
      " with an instance type. The default is \n",
      "m5.xlarge\n",
      ". You cannot\n",
      "change the instance type for a machine pool after the pool is created.\n",
      "Optional: Defines the labels for the machine pool. Replace \n",
      "<key>=<value>,<key>=\n",
      "<value>\n",
      " with a comma-delimited list of key-value pairs, for example \n",
      "--\n",
      "labels=key1=value1,key2=value2\n",
      ".\n",
      "Optional: Defines the taints for the machine pool. Replace \n",
      "<key>=<value>:<effect>,\n",
      "<key>=<value>:<effect>\n",
      " with a key, value, and effect for each taint, for example \n",
      "--\n",
      "taints=key1=value1:NoSchedule,key2=value2:NoExecute\n",
      ". Available effects include \n",
      "NoSchedule\n",
      ", \n",
      "PreferNoSchedule\n",
      ", and \n",
      "NoExecute\n",
      ".\n",
      "Optional: Configures your machine pool to deploy machines as non-guaranteed AWS Spot\n",
      "Instances. For information, see \n",
      "Amazon EC2 Spot Instances\n",
      " in the AWS documentation. If\n",
      "you select \n",
      "Use Amazon EC2 Spot Instances\n",
      " for a machine pool, you cannot disable the\n",
      "option after the machine pool is created.\n",
      "Optional: If you have opted to use Spot Instances, you can specify this argument to define\n",
      "a maximum hourly price for a Spot Instance. If this argument is not specified, the on-\n",
      "demand price is used.\n",
      "IMPORTANT\n",
      "Your Amazon EC2 Spot Instances might be interrupted at any time. Use Amazon\n",
      "EC2 Spot Instances only for workloads that can tolerate interruptions.\n",
      "The following example creates a machine pool called \n",
      "mymachinepool\n",
      " that uses the \n",
      "m5.xlarge\n",
      "instance type and has 2 compute node replicas. The example also adds 2 workload-specific\n",
      "labels:\n",
      "Example output\n",
      "To add a machine pool that uses autoscaling, create the machine pool and define the\n",
      "autoscaling configuration, instance type and node labels:\n",
      "$ rosa create machinepool --cluster=mycluster --name=mymachinepool --replicas=2 --\n",
      "instance-type=m5.xlarge --labels=app=db,tier=backend\n",
      "I: Machine pool 'mymachinepool' created successfully on cluster 'mycluster'\n",
      "I: To view all machine pools, run 'rosa list machinepools -c mycluster'\n",
      "$ rosa create machinepool --cluster=<cluster-name> \\\n",
      "                          --name=<machine_pool_id> \\ \n",
      "1\n",
      "                          --enable-autoscaling \\ \n",
      "2\n",
      "                          --min-replicas=<minimum_replica_count> \\ \n",
      "3\n",
      "                          --max-replicas=<maximum_replica_count> \\ \n",
      "4\n",
      "                          --instance-type=<instance_type> \\ \n",
      "5\n",
      "                          --labels=<key>=<value>,<key>=<value> \\ \n",
      "6\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "22\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Specifies the name of the machine pool. Replace \n",
      "<machine_pool_id>\n",
      " with the name of\n",
      "your machine pool.\n",
      "Enables autoscaling in the machine pool to meet the deployment needs.\n",
      "Defines the minimum and maximum compute node limits. The cluster autoscaler does not\n",
      "reduce or increase the machine pool node count beyond the limits that you specify. If you\n",
      "deployed ROSA using a single availability zone, the \n",
      "--min-replicas\n",
      " and \n",
      "--max-replicas\n",
      "arguments define the autoscaling limits in the machine pool for the zone. If you deployed\n",
      "your cluster using multiple availability zones, the arguments define the autoscaling limits in\n",
      "total across all zones and the counts must be multiples of 3.\n",
      "Optional: Sets the instance type for the compute nodes in your machine pool. The\n",
      "instance type defines the vCPU and memory allocation for each compute node in the pool.\n",
      "Replace \n",
      "<instance_type>\n",
      " with an instance type. The default is \n",
      "m5.xlarge\n",
      ". You cannot\n",
      "change the instance type for a machine pool after the pool is created.\n",
      "Optional: Defines the labels for the machine pool. Replace \n",
      "<key>=<value>,<key>=\n",
      "<value>\n",
      " with a comma-delimited list of key-value pairs, for example \n",
      "--\n",
      "labels=key1=value1,key2=value2\n",
      ".\n",
      "Optional: Defines the taints for the machine pool. Replace \n",
      "<key>=<value>:<effect>,\n",
      "<key>=<value>:<effect>\n",
      " with a key, value, and effect for each taint, for example \n",
      "--\n",
      "taints=key1=value1:NoSchedule,key2=value2:NoExecute\n",
      ". Available effects include \n",
      "NoSchedule\n",
      ", \n",
      "PreferNoSchedule\n",
      ", and \n",
      "NoExecute\n",
      ".\n",
      "Optional: Configures your machine pool to deploy machines as non-guaranteed AWS Spot\n",
      "Instances. For information, see \n",
      "Amazon EC2 Spot Instances\n",
      " in the AWS documentation. If\n",
      "you select \n",
      "Use Amazon EC2 Spot Instances\n",
      " for a machine pool, you cannot disable the\n",
      "option after the machine pool is created.\n",
      "Optional: If you have opted to use Spot Instances, you can specify this argument to define\n",
      "a maximum hourly price for a Spot Instance. If this argument is not specified, the on-\n",
      "demand price is used.\n",
      "IMPORTANT\n",
      "Your Amazon EC2 Spot Instances might be interrupted at any time. Use Amazon\n",
      "EC2 Spot Instances only for workloads that can tolerate interruptions.\n",
      "The following example creates a machine pool called \n",
      "mymachinepool\n",
      " that uses the \n",
      "m5.xlarge\n",
      "instance type and has autoscaling enabled. The minimum compute node limit is 3 and the\n",
      "maximum is 6 overall. The example also adds 2 workload-specific labels:\n",
      "Example output\n",
      "                          --taints=<key>=<value>:<effect>,<key>=<value>:<effect> \\ \n",
      "7\n",
      "                          --use-spot-instances \\ \n",
      "8\n",
      "                          --spot-max-price=0.5 \n",
      "9\n",
      "$ rosa create machinepool --cluster=mycluster --name=mymachinepool --enable-autoscaling\n",
      " \n",
      "--min-replicas=3 --max-replicas=6 --instance-type=m5.xlarge --labels=app=db,tier=backend\n",
      "CHAPTER 2. NODES\n",
      "23\n",
      "Verification\n",
      "1\n",
      ". \n",
      "List the available machine pools in your cluster:\n",
      "Example output\n",
      "2\n",
      ". \n",
      "Verify that the machine pool is included in the output and the configuration is as expected.\n",
      "Additional resources\n",
      "For a detailed list of the arguments that are available for the \n",
      "rosa create machinepool\n",
      "subcommand, see \n",
      "Managing objects with the rosa CLI\n",
      ".\n",
      "2.2.2. Scaling compute nodes manually\n",
      "If you have not enabled autoscaling for your machine pool, you can manually scale the number of\n",
      "compute (also known as worker) nodes in the pool to meet your deployment needs.\n",
      "You must scale each machine pool separately.\n",
      "Prerequisites\n",
      "You installed and configured the latest AWS (\n",
      "aws\n",
      "), ROSA (\n",
      "rosa\n",
      "), and OpenShift (\n",
      "oc\n",
      ") CLIs on\n",
      "your workstation.\n",
      "You logged in to your Red Hat account by using the \n",
      "rosa\n",
      " CLI.\n",
      "You created a Red Hat OpenShift Service on AWS (ROSA) cluster.\n",
      "You have an existing machine pool.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "List the machine pools in the cluster:\n",
      "Example output\n",
      "I: Machine pool 'mymachinepool' created successfully on cluster 'mycluster'\n",
      "I: To view all machine pools, run 'rosa list machinepools -c mycluster'\n",
      "$ rosa list machinepools --cluster=<cluster_name>\n",
      "ID             AUTOSCALING  REPLICAS  INSTANCE TYPE  LABELS                  TAINTS\n",
      "    \n",
      "AVAILABILITY ZONES                    SPOT INSTANCES\n",
      "Default        No           3         m5.xlarge                                        us-east-1a, us-east-1b, us-\n",
      "east-1c    N/A\n",
      "mymachinepool  Yes          3-6       m5.xlarge      app=db, tier=backend              us-east-1a,\n",
      " \n",
      "us-east-1b, us-east-1c    No\n",
      "$ rosa list machinepools --cluster=<cluster_name>\n",
      "ID        AUTOSCALING   REPLICAS    INSTANCE TYPE  LABELS    TAINTS\n",
      "   \n",
      "AVAILABILITY ZONES\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "24\n",
      "1\n",
      "2\n",
      "2\n",
      ". \n",
      "Increase or decrease the number of compute node replicas in a machine pool:\n",
      "If you deployed Red Hat OpenShift Service on AWS (ROSA) using a single availability zone,\n",
      "the replica count defines the number of compute nodes to provision to the machine pool\n",
      "for the zone. If you deployed your cluster using multiple availability zones, the count\n",
      "defines the total number of compute nodes in the machine pool across all zones and must\n",
      "be a multiple of 3.\n",
      "Replace \n",
      "<machine_pool_id>\n",
      " with the ID of your machine pool, as listed in the output of\n",
      "the preceding command.\n",
      "Verification\n",
      "1\n",
      ". \n",
      "List the available machine pools in your cluster:\n",
      "Example output\n",
      "2\n",
      ". \n",
      "In the output of the preceding command, verify that the compute node replica count is as\n",
      "expected for your machine pool. In the example output, the compute node replica count for the \n",
      "mp1\n",
      " machine pool is scaled to 3.\n",
      "2.2.3. Node labels\n",
      "A label is a key-value pair applied to a \n",
      "Node\n",
      " object. You can use labels to organize sets of objects and\n",
      "control the scheduling of pods.\n",
      "You can add labels during cluster creation or after. Labels can be modified or updated at any time.\n",
      "Additional resources\n",
      "For more information about labels, see \n",
      "Kubernetes Labels and Selectors overview\n",
      ".\n",
      "2.2.3.1. Adding node labels to a machine pool\n",
      "Add or edit labels for compute (also known as worker) nodes at any time to manage the nodes in a\n",
      "manner that is relevant to you. For example, you can assign types of workloads to specific nodes.\n",
      "Labels are assigned as key-value pairs. Each key must be unique to the object it is assigned to.\n",
      "Prerequisites\n",
      "default   No            2           m5.xlarge                        us-east-1a\n",
      "mp1       No            2           m5.xlarge                        us-east-1a\n",
      "$ rosa edit machinepool --cluster=<cluster_name> \\\n",
      "                        --replicas=<replica_count> \\ \n",
      "1\n",
      "                        <machine_pool_id> \n",
      "2\n",
      "$ rosa list machinepools --cluster=<cluster_name>\n",
      "ID        AUTOSCALING   REPLICAS    INSTANCE TYPE  LABELS    TAINTS\n",
      "   \n",
      "AVAILABILITY ZONES\n",
      "default   No            2           m5.xlarge                        us-east-1a\n",
      "mp1       No            3           m5.xlarge                        us-east-1a\n",
      "CHAPTER 2. NODES\n",
      "25\n",
      "1\n",
      "2\n",
      "Prerequisites\n",
      "You installed and configured the latest AWS (\n",
      "aws\n",
      "), ROSA (\n",
      "rosa\n",
      "), and OpenShift (\n",
      "oc\n",
      ") CLIs on\n",
      "your workstation.\n",
      "You logged in to your Red Hat account by using the \n",
      "rosa\n",
      " CLI.\n",
      "You created a Red Hat OpenShift Service on AWS (ROSA) cluster.\n",
      "You have an existing machine pool.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "List the machine pools in the cluster:\n",
      "Example output\n",
      "2\n",
      ". \n",
      "Add or update the node labels for a machine pool:\n",
      "To add or update node labels for a machine pool that does not use autoscaling, run the\n",
      "following command:\n",
      "For machine pools that do not use autoscaling, you must provide a replica count when\n",
      "adding node labels. If you do not specify the \n",
      "--replicas\n",
      " argument, you are prompted\n",
      "for a replica count before the command completes. If you deployed Red Hat OpenShift\n",
      "Service on AWS (ROSA) using a single availability zone, the replica count defines the\n",
      "number of compute nodes to provision to the machine pool for the zone. If you\n",
      "deployed your cluster using multiple availability zones, the count defines the total\n",
      "number of compute nodes in the machine pool across all zones and must be a multiple\n",
      "of 3.\n",
      "Replace \n",
      "<key>=<value>,<key>=<value>\n",
      " with a comma-delimited list of key-value\n",
      "pairs, for example \n",
      "--labels=key1=value1,key2=value2\n",
      ". This list overwrites any\n",
      "modifications made to node labels on an ongoing basis.\n",
      "The following example adds labels to the \n",
      "db-nodes-mp\n",
      " machine pool:\n",
      "Example output\n",
      "$ rosa list machinepools --cluster=<cluster_name>\n",
      "ID           AUTOSCALING  REPLICAS  INSTANCE TYPE  LABELS    TAINTS\n",
      "    \n",
      "AVAILABILITY ZONES    SPOT INSTANCES\n",
      "Default      No           2         m5.xlarge                          us-east-1a            N/A\n",
      "db-nodes-mp  No           2         m5.xlarge                          us-east-1a            No\n",
      "$ rosa edit machinepool --cluster=<cluster_name> \\\n",
      "                        --replicas=<replica_count> \\ \n",
      "1\n",
      "                        --labels=<key>=<value>,<key>=<value> \\ \n",
      "2\n",
      "                        <machine_pool_id>\n",
      "$ rosa edit machinepool --cluster=mycluster --replicas=2 --labels=app=db,tier=backend\n",
      " \n",
      "db-nodes-mp\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "26\n",
      "1\n",
      "2\n",
      "3\n",
      "To add or update node labels for a machine pool that uses autoscaling, run the following\n",
      "command:\n",
      "For machine pools that use autoscaling, you must provide minimum and maximum\n",
      "compute node replica limits. If you do not specify the arguments, you are prompted\n",
      "for the values before the command completes. The cluster autoscaler does not reduce\n",
      "or increase the machine pool node count beyond the limits that you specify. If you\n",
      "deployed ROSA using a single availability zone, the \n",
      "--min-replicas\n",
      " and \n",
      "--max-replicas\n",
      "arguments define the autoscaling limits in the machine pool for the zone. If you\n",
      "deployed your cluster using multiple availability zones, the arguments define the\n",
      "autoscaling limits in total across all zones and the counts must be multiples of 3.\n",
      "Replace \n",
      "<key>=<value>,<key>=<value>\n",
      " with a comma-delimited list of key-value\n",
      "pairs, for example \n",
      "--labels=key1=value1,key2=value2\n",
      ". This list overwrites any\n",
      "modifications made to node labels on an ongoing basis.\n",
      "The following example adds labels to the \n",
      "db-nodes-mp\n",
      " machine pool:\n",
      "Example output\n",
      "Verification\n",
      "1\n",
      ". \n",
      "List the available machine pools in your cluster:\n",
      "Example output\n",
      "2\n",
      ". \n",
      "Verify that the labels are included for your machine pool in the output.\n",
      "2.2.4. Adding taints to a machine pool\n",
      "You can add taints for compute (also known as worker) nodes in a machine pool to control which pods\n",
      "I: Updated machine pool 'db-nodes-mp' on cluster 'mycluster'\n",
      "$ rosa edit machinepool --cluster=<cluster_name> \\\n",
      "                        --min-replicas=<minimum_replica_count> \\ \n",
      "1\n",
      "                        --max-replicas=<maximum_replica_count> \\ \n",
      "2\n",
      "                        --labels=<key>=<value>,<key>=<value> \\ \n",
      "3\n",
      "                        <machine_pool_id>\n",
      "$ rosa edit machinepool --cluster=mycluster --min-replicas=2 --max-replicas=3 --\n",
      "labels=app=db,tier=backend db-nodes-mp\n",
      "I: Updated machine pool 'db-nodes-mp' on cluster 'mycluster'\n",
      "$ rosa list machinepools --cluster=<cluster_name>\n",
      "ID           AUTOSCALING  REPLICAS  INSTANCE TYPE  LABELS                  TAINTS\n",
      "    \n",
      "AVAILABILITY ZONES    SPOT INSTANCES\n",
      "Default      No           2         m5.xlarge                                        us-east-1a            N/A\n",
      "db-nodes-mp  No           2         m5.xlarge      app=db, tier=backend              us-east-1a\n",
      "            \n",
      "No\n",
      "CHAPTER 2. NODES\n",
      "27\n",
      "1\n",
      "2\n",
      "You can add taints for compute (also known as worker) nodes in a machine pool to control which pods\n",
      "are scheduled to them. When you apply a taint to a machine pool, the scheduler cannot place a pod on\n",
      "the nodes in the pool unless the pod specification includes a toleration for the taint.\n",
      "Prerequisites\n",
      "You installed and configured the latest AWS (\n",
      "aws\n",
      "), ROSA (\n",
      "rosa\n",
      "), and OpenShift (\n",
      "oc\n",
      ") CLIs on\n",
      "your workstation.\n",
      "You logged in to your Red Hat account by using the \n",
      "rosa\n",
      " CLI.\n",
      "You created a Red Hat OpenShift Service on AWS (ROSA) cluster.\n",
      "You have an existing machine pool.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "List the machine pools in the cluster:\n",
      "Example output\n",
      "2\n",
      ". \n",
      "Add or update the taints for a machine pool:\n",
      "To add or update taints for a machine pool that does not use autoscaling, run the following\n",
      "command:\n",
      "For machine pools that do not use autoscaling, you must provide a replica count when\n",
      "adding taints. If you do not specify the \n",
      "--replicas\n",
      " argument, you are prompted for a\n",
      "replica count before the command completes. If you deployed Red Hat OpenShift\n",
      "Service on AWS (ROSA) using a single availability zone, the replica count defines the\n",
      "number of compute nodes to provision to the machine pool for the zone. If you\n",
      "deployed your cluster using multiple availability zones, the count defines the total\n",
      "number of compute nodes in the machine pool across all zones and must be a multiple\n",
      "of 3.\n",
      "Replace \n",
      "<key>=<value>:<effect>,<key>=<value>:<effect>\n",
      " with a key, value, and\n",
      "effect for each taint, for example \n",
      "--\n",
      "taints=key1=value1:NoSchedule,key2=value2:NoExecute\n",
      ". Available effects include\n",
      "NoSchedule\n",
      ", \n",
      "PreferNoSchedule\n",
      ", and \n",
      "NoExecute\n",
      ".This list overwrites any\n",
      "modifications made to node taints on an ongoing basis.\n",
      "The following example adds taints to the \n",
      "db-nodes-mp\n",
      " machine pool:\n",
      "$ rosa list machinepools --cluster=<cluster_name>\n",
      "ID           AUTOSCALING  REPLICAS  INSTANCE TYPE  LABELS    TAINTS\n",
      "    \n",
      "AVAILABILITY ZONES    SPOT INSTANCES\n",
      "Default      No           2         m5.xlarge                          us-east-1a            N/A\n",
      "db-nodes-mp  No           2         m5.xlarge                          us-east-1a            No\n",
      "$ rosa edit machinepool --cluster=<cluster_name> \\\n",
      "                        --replicas=<replica_count> \\ \n",
      "1\n",
      "                        --taints=<key>=<value>:<effect>,<key>=<value>:<effect> \\ \n",
      "2\n",
      "                        <machine_pool_id>\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "28\n",
      "1\n",
      "2\n",
      "3\n",
      "Example output\n",
      "To add or update taints for a machine pool that uses autoscaling, run the following\n",
      "command:\n",
      "For machine pools that use autoscaling, you must provide minimum and maximum\n",
      "compute node replica limits. If you do not specify the arguments, you are prompted\n",
      "for the values before the command completes. The cluster autoscaler does not reduce\n",
      "or increase the machine pool node count beyond the limits that you specify. If you\n",
      "deployed ROSA using a single availability zone, the \n",
      "--min-replicas\n",
      " and \n",
      "--max-replicas\n",
      "arguments define the autoscaling limits in the machine pool for the zone. If you\n",
      "deployed your cluster using multiple availability zones, the arguments define the\n",
      "autoscaling limits in total across all zones and the counts must be multiples of 3.\n",
      "Replace \n",
      "<key>=<value>:<effect>,<key>=<value>:<effect>\n",
      " with a key, value, and\n",
      "effect for each taint, for example \n",
      "--\n",
      "taints=key1=value1:NoSchedule,key2=value2:NoExecute\n",
      ". Available effects include\n",
      "NoSchedule\n",
      ", \n",
      "PreferNoSchedule\n",
      ", and \n",
      "NoExecute\n",
      ".This list overwrites any\n",
      "modifications made to node taints on an ongoing basis.\n",
      "The following example adds taints to the \n",
      "db-nodes-mp\n",
      " machine pool:\n",
      "Example output\n",
      "Verification\n",
      "1\n",
      ". \n",
      "List the available machine pools in your cluster:\n",
      "Example output\n",
      "$ rosa edit machinepool --cluster=mycluster --replicas 2 --\n",
      "taints=key1=value1:NoSchedule,key2=value2:NoExecute db-nodes-mp\n",
      "I: Updated machine pool 'db-nodes-mp' on cluster 'mycluster'\n",
      "$ rosa edit machinepool --cluster=<cluster_name> \\\n",
      "                        --min-replicas=<minimum_replica_count> \\ \n",
      "1\n",
      "                        --max-replicas=<maximum_replica_count> \\ \n",
      "2\n",
      "                        --taints=<key>=<value>:<effect>,<key>=<value>:<effect> \\ \n",
      "3\n",
      "                        <machine_pool_id>\n",
      "$ rosa edit machinepool --cluster=mycluster --min-replicas=2 --max-replicas=3 --\n",
      "taints=key1=value1:NoSchedule,key2=value2:NoExecute db-nodes-mp\n",
      "I: Updated machine pool 'db-nodes-mp' on cluster 'mycluster'\n",
      "$ rosa list machinepools --cluster=<cluster_name>\n",
      "ID           AUTOSCALING  REPLICAS  INSTANCE TYPE  LABELS    TAINTS\n",
      "                                           \n",
      "AVAILABILITY ZONES    SPOT INSTANCES\n",
      "Default      No           2         m5.xlarge                                                                 us-east-1a\n",
      "            \n",
      "CHAPTER 2. NODES\n",
      "29\n",
      "2\n",
      ". \n",
      "Verify that the taints are included for your machine pool in the output.\n",
      "2.2.5. Additional resources\n",
      "About machine pools\n",
      "About autoscaling\n",
      "Enabling autoscaling\n",
      "Disabling autoscaling\n",
      "ROSA Service Definition\n",
      "2.3. ABOUT AUTOSCALING NODES ON A CLUSTER\n",
      "The autoscaler option can be configured to automatically scale the number of machines in a cluster.\n",
      "The cluster autoscaler increases the size of the cluster when there are pods that failed to schedule on\n",
      "any of the current nodes due to insufficient resources or when another node is necessary to meet\n",
      "deployment needs. The cluster autoscaler does not increase the cluster resources beyond the limits\n",
      "that you specify.\n",
      "Additionally, the cluster autoscaler decreases the size of the cluster when some nodes are consistently\n",
      "not needed for a significant period, such as when it has low resource use and all of its important pods\n",
      "can fit on other nodes.\n",
      "When you enable autoscaling, you must also set a minimum and maximum number of worker nodes.\n",
      "NOTE\n",
      "Only cluster owners and organization admins can scale or delete a cluster.\n",
      "2.3.1. Enabling autoscaling nodes on a cluster\n",
      "You can enable autoscaling on worker nodes to increase or decrease the number of nodes available by\n",
      "editing the machine pool definition for an existing cluster.\n",
      "Enabling autoscaling nodes in an existing cluster using Red Hat OpenShift Cluster Manager\n",
      "Enable autoscaling for worker nodes in the machine pool definition from OpenShift Cluster Manager\n",
      "console.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "From \n",
      "OpenShift Cluster Manager Hybrid Cloud Console\n",
      ", navigate to the \n",
      "Clusters\n",
      " page and\n",
      "select the cluster that you want to enable autoscaling for.\n",
      "2\n",
      ". \n",
      "On the selected cluster, select the \n",
      "Machine pools\n",
      " tab.\n",
      "N/A\n",
      "db-nodes-mp  No           2         m5.xlarge                key1=value1:NoSchedule,\n",
      " \n",
      "key2=value2:NoExecute    us-east-1a            No\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "30\n",
      "3\n",
      ". \n",
      "Click the Options menu \n",
      " \n",
      "at the end of the machine pool that you want to enable\n",
      "autoscaling for and select \n",
      "Scale\n",
      ".\n",
      "4\n",
      ". \n",
      "On the \n",
      "Edit node count\n",
      " dialog, select the \n",
      "Enable autoscaling\n",
      " checkbox.\n",
      "5\n",
      ". \n",
      "Select \n",
      "Apply\n",
      " to save these changes and enable autoscaling for the cluster.\n",
      "NOTE\n",
      "Additionally, you can configure autoscaling on the default machine pool when you \n",
      "create\n",
      "the cluster using interactive mode\n",
      ".\n",
      "Enabling autoscaling nodes in an existing cluster using the rosa CLI\n",
      "Configure autoscaling to dynamically scale the number of worker nodes up or down based on load.\n",
      "Successful autoscaling is dependent on having the correct AWS resource quotas in your AWS account.\n",
      "Verify resource quotas and request quota increases from the \n",
      "AWS console\n",
      ".\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "To identify the machine pool IDs in a cluster, enter the following command:\n",
      "Example output\n",
      "2\n",
      ". \n",
      "Get the ID of the machine pools that you want to configure.\n",
      "3\n",
      ". \n",
      "To enable autoscaling on a machine pool, enter the following command:\n",
      "Example\n",
      "Enable autoscaling on a machine pool with the ID \n",
      "mp1\n",
      " on a cluster named \n",
      "mycluster\n",
      ", with the\n",
      "number of replicas set to scale between 2 and 5 worker nodes:\n",
      "2.3.2. Disabling autoscaling nodes on a cluster\n",
      "You can disable autoscaling on worker nodes to increase or decrease the number of nodes available by\n",
      "editing the machine pool definition for an existing cluster.\n",
      "You can disable autoscaling on a cluster using OpenShift Cluster Manager console or the Red Hat\n",
      "$ rosa list machinepools --cluster=<cluster_name>\n",
      "ID        AUTOSCALING   REPLICAS    INSTANCE TYPE  LABELS    TINTS   AVAILABILITY\n",
      " \n",
      "ZONES\n",
      "default   No            2           m5.xlarge                        us-east-1a\n",
      "mp1       No            2           m5.xlarge                        us-east-1a\n",
      "$ rosa edit machinepool --cluster=<cluster_name> <machinepool_ID> --enable-autoscaling -\n",
      "-min-replicas=<number> --max-replicas=<number>\n",
      "$ rosa edit machinepool --cluster=mycluster mp1 --enable-autoscaling --min-replicas=2 --\n",
      "max-replicas=5\n",
      "CHAPTER 2. NODES\n",
      "31\n",
      "You can disable autoscaling on a cluster using OpenShift Cluster Manager console or the Red Hat\n",
      "OpenShift Service on AWS CLI.\n",
      "NOTE\n",
      "Additionally, you can configure autoscaling on the default machine pool when you \n",
      "create\n",
      "the cluster using interactive mode\n",
      ".\n",
      "Disabling autoscaling nodes in an existing cluster using Red Hat OpenShift Cluster Manager\n",
      "Disable autoscaling for worker nodes in the machine pool definition from OpenShift Cluster Manager\n",
      "console.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "From \n",
      "OpenShift Cluster Manager Hybrid Cloud Console\n",
      ", navigate to the \n",
      "Clusters\n",
      " page and\n",
      "select the cluster with autoscaling that must be disabled.\n",
      "2\n",
      ". \n",
      "On the selected cluster, select the \n",
      "Machine pools\n",
      " tab.\n",
      "3\n",
      ". \n",
      "Click the Options menu \n",
      " \n",
      "at the end of the machine pool with autoscaling and select \n",
      "Scale\n",
      ".\n",
      "4\n",
      ". \n",
      "On the \"Edit node count\" dialog, deselect the \n",
      "Enable autoscaling\n",
      " checkbox.\n",
      "5\n",
      ". \n",
      "Select \n",
      "Apply\n",
      " to save these changes and disable autoscaling from the cluster.\n",
      "Disabling autoscaling nodes in an existing cluster using the rosa CLI\n",
      "Disable autoscaling for worker nodes in the machine pool definition.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Enter the following command:\n",
      "Example\n",
      "Disable autoscaling on the \n",
      "default\n",
      " machine pool on a cluster named \n",
      "mycluster\n",
      ":\n",
      "2.3.3. Additional resources\n",
      "About machinepools\n",
      "Managing worker nodes\n",
      "Managing objects with the rosa CLI\n",
      "$ rosa edit machinepool --cluster=<cluster_name> <machinepool_ID> --enable-\n",
      "autoscaling=false --replicas=<number>\n",
      "$ rosa edit machinepool --cluster=mycluster default --enable-autoscaling=false --replicas=3\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "32\n",
      "CHAPTER 3. LOGGING\n",
      "3.1. ACCESSING THE SERVICE LOGS FOR ROSA CLUSTERS\n",
      "You can view the service logs for your Red Hat OpenShift Service on AWS (ROSA) clusters by using Red\n",
      "Hat OpenShift Cluster Manager. The service logs detail cluster events such as load balancer quota\n",
      "updates and scheduled maintenance upgrades. The logs also show cluster resource changes such as the\n",
      "addition or deletion of users, groups, and identity providers.\n",
      "Additionally, you can add notification contacts for a ROSA cluster. Subscribed users receive emails\n",
      "about cluster events that require customer action, known cluster incidents, upgrade maintenance, and\n",
      "other topics.\n",
      "3.1.1. Viewing the service logs by using OpenShift Cluster Manager\n",
      "You can view the service logs for a Red Hat OpenShift Service on AWS (ROSA) cluster by using Red Hat\n",
      "OpenShift Cluster Manager.\n",
      "Prerequisites\n",
      "You have installed a ROSA cluster.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Navigate to \n",
      "OpenShift Cluster Manager Hybrid Cloud Console\n",
      " and select your cluster.\n",
      "2\n",
      ". \n",
      "In the \n",
      "Overview\n",
      " page for your cluster, view the service logs in the \n",
      "Cluster history\n",
      " section.\n",
      "3\n",
      ". \n",
      "Optional: Filter the cluster service logs by \n",
      "Description\n",
      " or \n",
      "Severity\n",
      " from the drop-down menu.\n",
      "You can filter further by entering a specific item in the search bar.\n",
      "4\n",
      ". \n",
      "Optional: Click \n",
      "Download history\n",
      " to download the service logs for your cluster in JSON or CSV\n",
      "format.\n",
      "3.1.2. Adding cluster notification contacts\n",
      "You can add notification contacts for your Red Hat OpenShift Service on AWS (ROSA) cluster. When an\n",
      "event occurs that triggers a cluster notification email, subscribed users are notified.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Navigate to \n",
      "OpenShift Cluster Manager Hybrid Cloud Console\n",
      " and select your cluster.\n",
      "2\n",
      ". \n",
      "On the \n",
      "Support\n",
      " tab, under the \n",
      "Notification contacts\n",
      " heading, click \n",
      "Add notification contact\n",
      ".\n",
      "3\n",
      ". \n",
      "Enter the Red Hat username or email of the contact you want to add.\n",
      "NOTE\n",
      "The username or email address must relate to a user account in the Red Hat\n",
      "organization where the cluster is deployed.\n",
      "4\n",
      ". \n",
      "Click \n",
      "Add contact\n",
      ".\n",
      "CHAPTER 3. LOGGING\n",
      "33\n",
      "Verification\n",
      "You see a confirmation message when you have successfully added the contact. The user\n",
      "appears under the \n",
      "Notification contacts\n",
      " heading on the \n",
      "Support\n",
      " tab.\n",
      "3.2. INSTALLING LOGGING ADD-ON SERVICES\n",
      "This section describes how to install the logging add-on and Amazon Web Services (AWS) CloudWatch\n",
      "log forwarding add-on services on Red Hat OpenShift Service on AWS (ROSA).\n",
      "The AWS CloudWatch log forwarding service on ROSA has the following approximate log throughput\n",
      "rates. Message rates greater than these can result in dropped log messages.\n",
      "Table 3.1. Approximate log throughput rates\n",
      "Message size (bytes)\n",
      "Maximum expected rate (messages/second/node)\n",
      "512\n",
      "1,000\n",
      "1,024\n",
      "650\n",
      "2,048\n",
      "450\n",
      "3.2.1. Install the logging add-on service\n",
      "Red Hat OpenShift Service on AWS (ROSA) provides logging through the \n",
      "cluster-logging-operator\n",
      "add-on. This add-on service offers an optional application log forwarding solution based on AWS\n",
      "CloudWatch. This logging solution can be installed after the ROSA cluster is provisioned.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Enter the following command:\n",
      "For \n",
      "<cluster_name>\n",
      ", enter the name of your cluster.\n",
      "2\n",
      ". \n",
      "When prompted, accept the default \n",
      "yes\n",
      " to install the \n",
      "cluster-logging-operator\n",
      ".\n",
      "3\n",
      ". \n",
      "When prompted, accept the default \n",
      "yes\n",
      " to install the optional Amazon CloudWatch log\n",
      "forwarding add-on or enter \n",
      "no\n",
      " to decline the installation of this add-on.\n",
      "NOTE\n",
      "It is not necessary to install the AWS CloudWatch service when you install the \n",
      "cluster-logging-operator\n",
      ". You can install the AWS CloudWatch service at any\n",
      "time through OpenShift Cluster Manager console from the cluster’s \n",
      "Add-ons\n",
      "tab.\n",
      "4\n",
      ". \n",
      "For the collection of applications, infrastructure, and audit logs, accept the default values or\n",
      "change them as needed:\n",
      "Applications logs\n",
      ": Lets the Operator collect application logs, which includes everything that\n",
      "$ rosa install addon cluster-logging-operator --cluster=<cluster_name> --interactive\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "34\n",
      "Applications logs\n",
      ": Lets the Operator collect application logs, which includes everything that\n",
      "is \n",
      "not\n",
      " deployed in the openshift-\n",
      ", kube-\n",
      ", and default namespaces. Default: \n",
      "yes\n",
      "Infrastructure logs\n",
      ": Lets the Operator collect logs from OpenShift Container Platform,\n",
      "Kubernetes, and some nodes. Default: \n",
      "yes\n",
      "Audit logs\n",
      ": Type \n",
      "yes\n",
      " to let the Operator collect node logs related to security audits. By\n",
      "default, Red Hat stores audit logs outside the cluster through a separate mechanism that\n",
      "does not rely on the Cluster Logging Operator. For more information about default audit\n",
      "logging, see the ROSA Service Definition. Default: \n",
      "no\n",
      "5\n",
      ". \n",
      "For the Amazon CloudWatch region, use the default cluster region, leave the \n",
      "Cloudwatch\n",
      " \n",
      "region\n",
      " value empty.\n",
      "Example output\n",
      "NOTE\n",
      "The installation can take approximately 10 minutes to complete.\n",
      "Verification steps\n",
      "1\n",
      ". \n",
      "To verify the logging installation status, enter the following command:\n",
      "2\n",
      ". \n",
      "To verify which pods are deployed by \n",
      "cluster-logging-operator\n",
      " and their state of readiness:\n",
      "a\n",
      ". \n",
      "Log in to the \n",
      "oc\n",
      " CLI using \n",
      "cluster-admin\n",
      " credentials:\n",
      "b\n",
      ". \n",
      "Enter the following command to get information about the pods for the default project.\n",
      "Alternatively, you can specify a different project.\n",
      "Example output\n",
      "? Are you sure you want to install add-on 'cluster-logging-operator' on cluster\n",
      " \n",
      "'<cluster_name>'? Yes\n",
      "? Use AWS CloudWatch (optional): Yes\n",
      "? Collect Applications logs (optional): Yes\n",
      "? Collect Infrastructure logs (optional): Yes\n",
      "? Collect Audit logs (optional): No\n",
      "? CloudWatch region (optional):\n",
      "I: Add-on 'cluster-logging-operator' is now installing. To check the status run 'rosa list addons\n",
      " \n",
      "--cluster=<cluster_name>'\n",
      "$ rosa list addons --cluster=<cluster_name>\n",
      "$ oc login https://api.mycluster.abwp.s1.example.org:6443 \\\n",
      "   --username cluster-admin\n",
      "   --password <password>\n",
      "$ oc get pods -n openshift-logging\n",
      "NAME                                        READY         STATUS      RESTARTS   AGE\n",
      "cluster-logging-operator-<pod_ID  >         2/2           Running     0          7m1s\n",
      "fluentd-4mnwp                               1/1           Running     0          6m3s\n",
      "CHAPTER 3. LOGGING\n",
      "35\n",
      "3\n",
      ". \n",
      "Optional: To get information about the \n",
      "clusterlogging\n",
      " instance, enter the following command:\n",
      "4\n",
      ". \n",
      "Optional: To get information about \n",
      "clusterlogforwarders\n",
      " instances, enter the following\n",
      "command:\n",
      "3.2.2. Additional resources\n",
      "Adding services to your cluster\n",
      "3.3. VIEWING CLUSTER LOGS IN THE AWS CONSOLE\n",
      "View forwarded cluster logs in the AWS console.\n",
      "3.3.1. Viewing forwarded logs\n",
      "Logs that are being forwarded from Red Hat OpenShift Service on AWS are viewed in the Amazon Web\n",
      "Services (AWS) console.\n",
      "Prerequisites\n",
      "The \n",
      "cluster-logging-operator\n",
      " add-on service is installed and \n",
      "Cloudwatch\n",
      " is enabled.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Log in to the AWS console.\n",
      "2\n",
      ". \n",
      "Select the region the cluster is deployed in.\n",
      "3\n",
      ". \n",
      "Select the \n",
      "CloudWatch\n",
      " service.\n",
      "4\n",
      ". \n",
      "Select \n",
      "Logs\n",
      " from the left column, and select \n",
      "Log Groups\n",
      ".\n",
      "5\n",
      ". \n",
      "Select a log group to explore. You can view application, infrastructure, or audit logs, depending\n",
      "on which types were enabled during the add-on service installation. See the \n",
      "Amazon\n",
      "CloudWatch User Guide\n",
      " for more information.\n",
      "fluentd-6xt25                               1/1           Running     0          6m3s\n",
      "fluentd-fqjhv                               1/1           Running     0          6m3s\n",
      "fluentd-gcvrg                               1/1           Running     0          6m3s\n",
      "fluentd-vpwrt                               1/1           Running     0          6m3s\n",
      "$ oc get clusterlogging -n openshift-logging\n",
      "$ oc get clusterlogforwarders -n openshift-logging\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "36\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "4.1. UNDERSTANDING THE MONITORING STACK\n",
      "In Red Hat OpenShift Service on AWS, you can monitor your own projects in isolation from Red Hat Site\n",
      "Reliability Engineer (SRE) platform metrics. You can monitor your own projects without the need for an\n",
      "additional monitoring solution.\n",
      "NOTE\n",
      "Follow the instructions in this document carefully to configure a supported Prometheus\n",
      "instance for monitoring user-defined projects. Custom Prometheus instances are not\n",
      "supported by Red Hat OpenShift Service on AWS.\n",
      "4.1.1. Understanding the monitoring stack\n",
      "The Red Hat OpenShift Service on AWS (ROSA) monitoring stack is based on the \n",
      "Prometheus\n",
      " open\n",
      "source project and its wider ecosystem. The monitoring stack includes the following:\n",
      "Default platform monitoring components\n",
      ". A set of platform monitoring components are\n",
      "installed in the \n",
      "openshift-monitoring\n",
      " project and enabled by default during a ROSA\n",
      "installation. This provides monitoring for core cluster components. The default monitoring stack\n",
      "also enables remote health monitoring for clusters. Critical metrics, such as CPU and memory,\n",
      "are collected from all of the workloads in every namespace and are made available for your use.\n",
      "These components are illustrated in the \n",
      "Installed by default\n",
      " section in the following diagram.\n",
      "Components for monitoring user-defined projects\n",
      ". This feature is enabled by default and\n",
      "provides monitoring for user-defined projects. These components are illustrated in the \n",
      "User\n",
      "section in the following diagram.\n",
      "Insta l l e d b y d e f a u l t\n",
      "Depl o yOpe n S h i f t \n",
      "P\n",
      "r\n",
      "o\n",
      "j\n",
      "e\n",
      "c\n",
      "t\n",
      "s\n",
      "4.1.1.1. Components for monitoring user-defined projects\n",
      "Red Hat OpenShift Service on AWS includes an optional enhancement to the monitoring stack that\n",
      "enables you to monitor services and pods in user-defined projects. This feature includes the following\n",
      "components:\n",
      "Table 4.1. Components for monitoring user-defined projects\n",
      "Component\n",
      "Description\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "37\n",
      "Prometheus Operator\n",
      "The Prometheus Operator in the \n",
      "openshift-user-workload-\n",
      "monitoring\n",
      " project creates, configures, and manages Prometheus and\n",
      "Thanos Ruler instances in the same project.\n",
      "Prometheus\n",
      "Prometheus is the monitoring system through which monitoring is\n",
      "provided for user-defined projects. Prometheus sends alerts to\n",
      "Alertmanager for processing. However, alert routing is not currently\n",
      "supported.\n",
      "Thanos Ruler\n",
      "The Thanos Ruler is a rule evaluation engine for Prometheus that is\n",
      "deployed as a separate process. In Red Hat OpenShift Service on AWS\n",
      "4, Thanos Ruler provides rule and alerting evaluation for the monitoring\n",
      "of user-defined projects.\n",
      "Component\n",
      "Description\n",
      "All of these components are monitored by the stack and are automatically updated when Red Hat\n",
      "OpenShift Service on AWS is updated.\n",
      "4.1.1.2. Monitoring targets for user-defined projects\n",
      "Monitoring is enabled by default for Red Hat OpenShift Service on AWS user-defined projects. You can\n",
      "monitor:\n",
      "Metrics provided through service endpoints in user-defined projects.\n",
      "Pods running in user-defined projects.\n",
      "4.1.2. Additional resources\n",
      "Accessing monitoring for user-defined projects\n",
      "Default monitoring components\n",
      "Default monitoring targets\n",
      "4.1.3. Next steps\n",
      "Accessing monitoring for user-defined projects\n",
      "4.2. ACCESSING MONITORING FOR USER-DEFINED PROJECTS\n",
      "When you install a Red Hat OpenShift Service on AWS (ROSA) cluster, monitoring for user-defined\n",
      "projects is enabled by default. With monitoring for user-defined projects enabled, you can monitor your\n",
      "own ROSA projects without the need for an additional monitoring solution.\n",
      "The \n",
      "dedicated-admin\n",
      " user has default permissions to configure and access monitoring for user-\n",
      "defined projects.\n",
      "NOTE\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "38\n",
      "NOTE\n",
      "Custom Prometheus instances and the Prometheus Operator installed through Operator\n",
      "Lifecycle Manager (OLM) can cause issues with user-defined project monitoring if it is\n",
      "enabled. Custom Prometheus instances are not supported.\n",
      "Optionally, you can disable monitoring for user-defined projects during or after a cluster installation.\n",
      "4.2.1. Next steps\n",
      "Configuring the monitoring stack\n",
      "4.3. CONFIGURING THE MONITORING STACK\n",
      "After you configure the monitoring stack, you can review common configuration scenarios and configure\n",
      "monitoring of user-defined projects.\n",
      "4.3.1. Maintenance and support for monitoring\n",
      "The supported way of configuring Red Hat OpenShift Service on AWS Monitoring is by using the options\n",
      "described in this document. \n",
      "Do not use other configurations, as they are unsupported.\n",
      "IMPORTANT\n",
      "Installing another Prometheus instance is not supported by the Red Hat Site Reliability\n",
      "Engineers (SREs).\n",
      "Configuration paradigms can change across Prometheus releases, and such cases can only be handled\n",
      "gracefully if all configuration possibilities are controlled. If you use configurations other than those\n",
      "described in this section, your changes will disappear because the \n",
      "cluster-monitoring-operator\n",
      "reconciles any differences. The Operator resets everything to the defined state by default and by\n",
      "design.\n",
      "4.3.1.1. Support considerations for monitoring user-defined projects\n",
      "The following modifications are explicitly not supported:\n",
      "Installing custom Prometheus instances on Red Hat OpenShift Service on AWS\n",
      "4.3.2. Configuring the monitoring stack\n",
      "In Red Hat OpenShift Service on AWS, you can configure the stack that monitors workloads for user-\n",
      "defined projects by using the \n",
      "user-workload-monitoring-config\n",
      " \n",
      "ConfigMap\n",
      " object. Config maps\n",
      "configure the Cluster Monitoring Operator (CMO), which in turn configures the components of the\n",
      "stack.\n",
      "Prerequisites\n",
      "You have access to the cluster as a user with the \n",
      "dedicated-admin\n",
      " role.\n",
      "You have created the \n",
      "user-workload-monitoring-config\n",
      " \n",
      "ConfigMap\n",
      " object.\n",
      "You have installed the OpenShift CLI (\n",
      "oc\n",
      ").\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "39\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Edit the \n",
      "ConfigMap\n",
      " object.\n",
      "a\n",
      ". \n",
      "Edit the \n",
      "user-workload-monitoring-config\n",
      " \n",
      "ConfigMap\n",
      " object in the \n",
      "openshift-user-\n",
      "workload-monitoring\n",
      " project:\n",
      "b\n",
      ". \n",
      "Add your configuration under \n",
      "data.config.yaml\n",
      " as a key-value pair \n",
      "<component_name>: <component_configuration>\n",
      ":\n",
      "Substitute \n",
      "<component>\n",
      " and \n",
      "<configuration_for_the_component>\n",
      " accordingly.\n",
      "The following example \n",
      "ConfigMap\n",
      " object configures a data retention period and minimum\n",
      "container resource requests for Prometheus. This relates to the Prometheus instance that\n",
      "monitors user-defined projects only:\n",
      "Defines the Prometheus component and the subsequent lines define its configuration.\n",
      "Configures a 24 hour data retention period for the Prometheus instance that monitors\n",
      "user-defined projects.\n",
      "Defines a minimum resource request of 200 millicores for the Prometheus container.\n",
      "Defines a minimum pod resource request of 2 GiB of memory for the Prometheus\n",
      "container.\n",
      "2\n",
      ". \n",
      "Save the file to apply the changes to the \n",
      "ConfigMap\n",
      " object. The pods affected by the new\n",
      "$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-\n",
      "config\n",
      "apiVersion:\n",
      " v1\n",
      "kind:\n",
      " ConfigMap\n",
      "metadata:\n",
      "  name:\n",
      " user-workload-monitoring-config\n",
      "  namespace:\n",
      " openshift-user-workload-monitoring\n",
      "data:\n",
      "  config.yaml: \n",
      "|\n",
      "    <component>:\n",
      "      <configuration_for_the_component>\n",
      "apiVersion:\n",
      " v1\n",
      "kind:\n",
      " ConfigMap\n",
      "metadata:\n",
      "  name:\n",
      " user-workload-monitoring-config\n",
      "  namespace:\n",
      " openshift-user-workload-monitoring\n",
      "data:\n",
      "  config.yaml: \n",
      "|\n",
      "    prometheus:\n",
      " \n",
      "1\n",
      "      retention:\n",
      " \n",
      "24\n",
      "h \n",
      "2\n",
      "      resources:\n",
      "        requests:\n",
      "          cpu:\n",
      " \n",
      "200\n",
      "m \n",
      "3\n",
      "          memory:\n",
      " \n",
      "2\n",
      "Gi \n",
      "4\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "40\n",
      "2\n",
      ". \n",
      "Save the file to apply the changes to the \n",
      "ConfigMap\n",
      " object. The pods affected by the new\n",
      "configuration are restarted automatically.\n",
      "WARNING\n",
      "When changes are saved to a monitoring config map, the pods and other\n",
      "resources in the related project might be redeployed. The running\n",
      "monitoring processes in that project might also be restarted.\n",
      "4.3.3. Configurable monitoring components\n",
      "This table shows the monitoring components you can configure and the keys used to specify the\n",
      "components in the \n",
      "user-workload-monitoring-config\n",
      " \n",
      "ConfigMap\n",
      " objects:\n",
      "Table 4.2. Configurable monitoring components\n",
      "Component\n",
      "user-workload-monitoring-config config map key\n",
      "Prometheus Operator\n",
      "prometheusOperator\n",
      "Prometheus\n",
      "prometheus\n",
      "Thanos Ruler\n",
      "thanosRuler\n",
      "4.3.4. Moving monitoring components to different nodes\n",
      "You can move any of the components that monitor workloads for user-defined projects to specific\n",
      "worker nodes. It is not permitted to move components to control plane or infrastructure nodes.\n",
      "Prerequisites\n",
      "You have access to the cluster as a user with the \n",
      "dedicated-admin\n",
      " role.\n",
      "You have created the \n",
      "user-workload-monitoring-config\n",
      " \n",
      "ConfigMap\n",
      " object.\n",
      "You have installed the OpenShift CLI (\n",
      "oc\n",
      ").\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "To move a component that monitors user-defined projects, edit the \n",
      "ConfigMap\n",
      " object:\n",
      "a\n",
      ". \n",
      "Edit the \n",
      "user-workload-monitoring-config\n",
      " \n",
      "ConfigMap\n",
      " object in the \n",
      "openshift-user-\n",
      "workload-monitoring\n",
      " project:\n",
      "b\n",
      ". \n",
      "Specify the \n",
      "nodeSelector\n",
      " constraint for the component under \n",
      "data.config.yaml\n",
      ":\n",
      "\n",
      "$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-\n",
      "config\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "41\n",
      "Substitute \n",
      "<component>\n",
      " accordingly and substitute \n",
      "<node_key>: <node_value>\n",
      " with the\n",
      "map of key-value pairs that specifies the destination nodes. Often, only a single key-value\n",
      "pair is used.\n",
      "The component can only run on nodes that have each of the specified key-value pairs as\n",
      "labels. The nodes can have additional labels as well.\n",
      "IMPORTANT\n",
      "Many of the monitoring components are deployed by using multiple pods\n",
      "across different nodes in the cluster to maintain high availability. When\n",
      "moving monitoring components to labeled nodes, ensure that enough\n",
      "matching nodes are available to maintain resilience for the component. If\n",
      "only one label is specified, ensure that enough nodes contain that label to\n",
      "distribute all of the pods for the component across separate nodes.\n",
      "Alternatively, you can specify multiple labels each relating to individual\n",
      "nodes.\n",
      "NOTE\n",
      "If monitoring components remain in a \n",
      "Pending\n",
      " state after configuring the \n",
      "nodeSelector\n",
      " constraint, check the pod logs for errors relating to taints and\n",
      "tolerations.\n",
      "For example, to move monitoring components for user-defined projects to specific worker\n",
      "nodes labeled \n",
      "nodename: worker1\n",
      ", \n",
      "nodename: worker2\n",
      ", and \n",
      "nodename: worker2\n",
      ", use:\n",
      "apiVersion:\n",
      " v1\n",
      "kind:\n",
      " ConfigMap\n",
      "metadata:\n",
      "  name:\n",
      " user-workload-monitoring-config\n",
      "  namespace:\n",
      " openshift-user-workload-monitoring\n",
      "data:\n",
      "  config.yaml: \n",
      "|\n",
      "    <component>:\n",
      "      nodeSelector:\n",
      "        <node_key>: <node_value\n",
      ">\n",
      "        <node_key>: <node_value>\n",
      "        <...>\n",
      "apiVersion:\n",
      " v1\n",
      "kind:\n",
      " ConfigMap\n",
      "metadata:\n",
      "  name:\n",
      " user-workload-monitoring-config\n",
      "  namespace:\n",
      " openshift-user-workload-monitoring\n",
      "data:\n",
      "  config.yaml: \n",
      "|\n",
      "    prometheusOperator:\n",
      "      nodeSelector:\n",
      "        nodename:\n",
      " worker1\n",
      "    prometheus:\n",
      "      nodeSelector:\n",
      "        nodename:\n",
      " worker1\n",
      "        nodename:\n",
      " worker2\n",
      "    thanosRuler:\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "42\n",
      "2\n",
      ". \n",
      "Save the file to apply the changes. The components affected by the new configuration are\n",
      "moved to the new nodes automatically.\n",
      "WARNING\n",
      "When changes are saved to a monitoring config map, the pods and other\n",
      "resources in the related project might be redeployed. The running\n",
      "monitoring processes in that project might also be restarted.\n",
      "4.3.5. Assigning tolerations to components that monitor user-defined projects\n",
      "You can assign tolerations to the components that monitor user-defined projects, to enable moving\n",
      "them to tainted worker nodes. Scheduling is not permitted on control plane or infrastructure nodes.\n",
      "Prerequisites\n",
      "You have access to the cluster as a user with the \n",
      "dedicated-admin\n",
      " role.\n",
      "You have created the \n",
      "user-workload-monitoring-config\n",
      " \n",
      "ConfigMap\n",
      " object in the \n",
      "openshift-\n",
      "user-workload-monitoring\n",
      " namespace.\n",
      "The OpenShift CLI (\n",
      "oc\n",
      ") is installed.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Edit the \n",
      "ConfigMap\n",
      " object:\n",
      "a\n",
      ". \n",
      "Edit the \n",
      "user-workload-monitoring-config\n",
      " \n",
      "ConfigMap\n",
      " object in the \n",
      "openshift-user-\n",
      "workload-monitoring\n",
      " project:\n",
      "b\n",
      ". \n",
      "Specify \n",
      "tolerations\n",
      " for the component:\n",
      "      nodeSelector:\n",
      "        nodename:\n",
      " worker1\n",
      "        nodename:\n",
      " worker2\n",
      "\n",
      "$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-\n",
      "config\n",
      "apiVersion:\n",
      " v1\n",
      "kind:\n",
      " ConfigMap\n",
      "metadata:\n",
      "  name:\n",
      " user-workload-monitoring-config\n",
      "  namespace:\n",
      " openshift-user-workload-monitoring\n",
      "data:\n",
      "  config.yaml: \n",
      "|\n",
      "    <component>:\n",
      "      tolerations:\n",
      "        <toleration_specification\n",
      ">\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "43\n",
      "Substitute \n",
      "<component>\n",
      " and \n",
      "<toleration_specification>\n",
      " accordingly.\n",
      "For example, \n",
      "oc adm taint nodes node1 key1=value1:NoSchedule\n",
      " adds a taint to \n",
      "node1\n",
      "with the key \n",
      "key1\n",
      " and the value \n",
      "value1\n",
      ". This prevents monitoring components from\n",
      "deploying pods on \n",
      "node1\n",
      " unless a toleration is configured for that taint. The following\n",
      "example configures the \n",
      "thanosRuler\n",
      " component to tolerate the example taint:\n",
      "2\n",
      ". \n",
      "Save the file to apply the changes. The new component placement configuration is applied\n",
      "automatically.\n",
      "WARNING\n",
      "When changes are saved to a monitoring config map, the pods and other\n",
      "resources in the related project might be redeployed. The running\n",
      "monitoring processes in that project might also be restarted.\n",
      "Additional resources\n",
      "See the \n",
      "OpenShift Container Platform documentation\n",
      " on taints and tolerations\n",
      "See the \n",
      "Kubernetes documentation\n",
      " on taints and tolerations\n",
      "4.3.6. Configuring persistent storage\n",
      "Running cluster monitoring with persistent storage means that your metrics are stored to a persistent\n",
      "volume (PV) and can survive a pod being restarted or recreated. This is ideal if you require your metrics\n",
      "data to be guarded from data loss. For production environments, it is highly recommended to configure\n",
      "persistent storage. Because of the high IO demands, it is advantageous to use local storage.\n",
      "IMPORTANT\n",
      "See \n",
      "Recommended configurable storage technology\n",
      ".\n",
      "4.3.6.1. Persistent storage prerequisites\n",
      "Use the block type of storage.\n",
      "apiVersion:\n",
      " v1\n",
      "kind:\n",
      " ConfigMap\n",
      "metadata:\n",
      "  name:\n",
      " user-workload-monitoring-config\n",
      "  namespace:\n",
      " openshift-user-workload-monitoring\n",
      "data:\n",
      "  config.yaml: \n",
      "|\n",
      "    thanosRuler:\n",
      "      tolerations:\n",
      "      - key:\n",
      " \n",
      "\"key1\"\n",
      "        operator:\n",
      " \n",
      "\"Equal\"\n",
      "        value:\n",
      " \n",
      "\"value1\"\n",
      "        effect:\n",
      " \n",
      "\"NoSchedule\"\n",
      "\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "44\n",
      "4.3.6.2. Configuring a local persistent volume claim\n",
      "For monitoring components to use a persistent volume (PV), you must configure a persistent volume\n",
      "claim (PVC).\n",
      "Prerequisites\n",
      "You have access to the cluster as a user with the \n",
      "dedicated-admin\n",
      " role.\n",
      "You have created the \n",
      "user-workload-monitoring-config\n",
      " \n",
      "ConfigMap\n",
      " object.\n",
      "You have installed the OpenShift CLI (\n",
      "oc\n",
      ").\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "To configure a PVC for a component that monitors user-defined projects, edit the \n",
      "ConfigMap\n",
      "object:\n",
      "a\n",
      ". \n",
      "Edit the \n",
      "user-workload-monitoring-config\n",
      " \n",
      "ConfigMap\n",
      " object in the \n",
      "openshift-user-\n",
      "workload-monitoring\n",
      " project:\n",
      "b\n",
      ". \n",
      "Add your PVC configuration for the component under \n",
      "data.config.yaml\n",
      ":\n",
      "See the \n",
      "Kubernetes documentation on PersistentVolumeClaims\n",
      " for information on how to\n",
      "specify \n",
      "volumeClaimTemplate\n",
      ".\n",
      "The following example configures a PVC that claims local persistent storage for the\n",
      "Prometheus instance that monitors user-defined projects:\n",
      "$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-\n",
      "config\n",
      "apiVersion:\n",
      " v1\n",
      "kind:\n",
      " ConfigMap\n",
      "metadata:\n",
      "  name:\n",
      " user-workload-monitoring-config\n",
      "  namespace:\n",
      " openshift-user-workload-monitoring\n",
      "data:\n",
      "  config.yaml: \n",
      "|\n",
      "    <component>:\n",
      "      volumeClaimTemplate:\n",
      "        spec:\n",
      "          storageClassName:\n",
      " <storage_class\n",
      ">\n",
      "          resources:\n",
      "            requests:\n",
      "              storage:\n",
      " <amount_of_storage\n",
      ">\n",
      "apiVersion:\n",
      " v1\n",
      "kind:\n",
      " ConfigMap\n",
      "metadata:\n",
      "  name:\n",
      " user-workload-monitoring-config\n",
      "  namespace:\n",
      " openshift-user-workload-monitoring\n",
      "data:\n",
      "  config.yaml: \n",
      "|\n",
      "    prometheus:\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "45\n",
      "In the above example, the storage class created by the Local Storage Operator is called \n",
      "local-storage\n",
      ".\n",
      "The following example configures a PVC that claims local persistent storage for Thanos\n",
      "Ruler:\n",
      "2\n",
      ". \n",
      "Save the file to apply the changes. The pods affected by the new configuration are restarted\n",
      "automatically and the new storage configuration is applied.\n",
      "WARNING\n",
      "When changes are saved to a monitoring config map, the pods and other\n",
      "resources in the related project might be redeployed. The running\n",
      "monitoring processes in that project might also be restarted.\n",
      "4.3.6.3. Modifying the retention time for Prometheus metrics data\n",
      "By default, the Red Hat OpenShift Service on AWS monitoring stack configures the retention time for\n",
      "Prometheus data to be 15 days. You can modify the retention time for the Prometheus instance that\n",
      "monitors user-defined projects, to change how soon the data is deleted.\n",
      "Prerequisites\n",
      "You have access to the cluster as a user with the \n",
      "dedicated-admin\n",
      " role.\n",
      "You have created the \n",
      "user-workload-monitoring-config\n",
      " \n",
      "ConfigMap\n",
      " object.\n",
      "You have installed the OpenShift CLI (\n",
      "oc\n",
      ").\n",
      "      volumeClaimTemplate:\n",
      "        spec:\n",
      "          storageClassName:\n",
      " local-storage\n",
      "          resources:\n",
      "            requests:\n",
      "              storage:\n",
      " \n",
      "40\n",
      "Gi\n",
      "apiVersion:\n",
      " v1\n",
      "kind:\n",
      " ConfigMap\n",
      "metadata:\n",
      "  name:\n",
      " user-workload-monitoring-config\n",
      "  namespace:\n",
      " openshift-user-workload-monitoring\n",
      "data:\n",
      "  config.yaml: \n",
      "|\n",
      "    thanosRuler:\n",
      "      volumeClaimTemplate:\n",
      "        spec:\n",
      "          storageClassName:\n",
      " local-storage\n",
      "          resources:\n",
      "            requests:\n",
      "              storage:\n",
      " \n",
      "40\n",
      "Gi\n",
      "\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "46\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "To modify the retention time for the Prometheus instance that monitors user-defined projects,\n",
      "edit the \n",
      "ConfigMap\n",
      " object:\n",
      "a\n",
      ". \n",
      "Edit the \n",
      "user-workload-monitoring-config\n",
      " \n",
      "ConfigMap\n",
      " object in the \n",
      "openshift-user-\n",
      "workload-monitoring\n",
      " project:\n",
      "b\n",
      ". \n",
      "Add your retention time configuration under \n",
      "data.config.yaml\n",
      ":\n",
      "Substitute \n",
      "<time_specification>\n",
      " with a number directly followed by \n",
      "ms\n",
      " (milliseconds), \n",
      "s\n",
      "(seconds), \n",
      "m\n",
      " (minutes), \n",
      "h\n",
      " (hours), \n",
      "d\n",
      " (days), \n",
      "w\n",
      " (weeks), or \n",
      "y\n",
      " (years).\n",
      "The following example sets the retention time to 24 hours for the Prometheus instance that\n",
      "monitors user-defined projects:\n",
      "2\n",
      ". \n",
      "Save the file to apply the changes. The pods affected by the new configuration are restarted\n",
      "automatically.\n",
      "WARNING\n",
      "When changes are saved to a monitoring config map, the pods and other\n",
      "resources in the related project might be redeployed. The running\n",
      "monitoring processes in that project might also be restarted.\n",
      "Additional resources\n",
      "$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-\n",
      "config\n",
      "apiVersion:\n",
      " v1\n",
      "kind:\n",
      " ConfigMap\n",
      "metadata:\n",
      "  name:\n",
      " user-workload-monitoring-config\n",
      "  namespace:\n",
      " openshift-user-workload-monitoring\n",
      "data:\n",
      "  config.yaml: \n",
      "|\n",
      "    prometheus:\n",
      "      retention:\n",
      " <time_specification\n",
      ">\n",
      "apiVersion:\n",
      " v1\n",
      "kind:\n",
      " ConfigMap\n",
      "metadata:\n",
      "  name:\n",
      " user-workload-monitoring-config\n",
      "  namespace:\n",
      " openshift-user-workload-monitoring\n",
      "data:\n",
      "  config.yaml: \n",
      "|\n",
      "    prometheus:\n",
      "      retention:\n",
      " \n",
      "24\n",
      "h\n",
      "\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "47\n",
      "Understanding persistent storage\n",
      "Optimizing storage\n",
      "4.3.7. Controlling the impact of unbound metrics attributes in user-defined projects\n",
      "Developers can create labels to define attributes for metrics in the form of key-value pairs. The number\n",
      "of potential key-value pairs corresponds to the number of possible values for an attribute. An attribute\n",
      "that has an unlimited number of potential values is called an unbound attribute. For example, a \n",
      "customer_id\n",
      " attribute is unbound because it has an infinite number of possible values.\n",
      "Every assigned key-value pair has a unique time series. The use of many unbound attributes in labels\n",
      "can result in an exponential increase in the number of time series created. This can impact Prometheus\n",
      "performance and can consume a lot of disk space.\n",
      "A \n",
      "dedicated-admin\n",
      " can use the following measure to control the impact of unbound metrics attributes\n",
      "in user-defined projects:\n",
      "Limit the number of samples that can be accepted\n",
      " per target scrape in user-defined projects\n",
      "NOTE\n",
      "Limiting scrape samples can help prevent the issues caused by adding many unbound\n",
      "attributes to labels. Developers can also prevent the underlying cause by limiting the\n",
      "number of unbound attributes that they define for metrics. Using attributes that are\n",
      "bound to a limited set of possible values reduces the number of potential key-value pair\n",
      "combinations.\n",
      "4.3.7.1. Setting a scrape sample limit for user-defined projects\n",
      "You can limit the number of samples that can be accepted per target scrape in user-defined projects.\n",
      "WARNING\n",
      "If you set a sample limit, no further sample data is ingested for that target scrape\n",
      "after the limit is reached.\n",
      "Prerequisites\n",
      "You have access to the cluster as a user with the \n",
      "dedicated-admin\n",
      " role.\n",
      "You have created the \n",
      "user-workload-monitoring-config\n",
      " \n",
      "ConfigMap\n",
      " object.\n",
      "You have installed the OpenShift CLI (\n",
      "oc\n",
      ").\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Edit the \n",
      "user-workload-monitoring-config\n",
      " \n",
      "ConfigMap\n",
      " object in the \n",
      "openshift-user-\n",
      "workload-monitoring\n",
      " project:\n",
      "\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "48\n",
      "1\n",
      "2\n",
      ". \n",
      "Add the \n",
      "enforcedSampleLimit\n",
      " configuration to \n",
      "data.config.yaml\n",
      " to limit the number of\n",
      "samples that can be accepted per target scrape in user-defined projects:\n",
      "A value is required if this parameter is specified. This \n",
      "enforcedSampleLimit\n",
      " example limits\n",
      "the number of samples that can be accepted per target scrape in user-defined projects to\n",
      "50,000.\n",
      "3\n",
      ". \n",
      "Save the file to apply the changes. The limit is applied automatically.\n",
      "WARNING\n",
      "When changes are saved to the \n",
      "user-workload-monitoring-config\n",
      " \n",
      "ConfigMap\n",
      " object, the pods and other resources in the \n",
      "openshift-user-\n",
      "workload-monitoring\n",
      " project might be redeployed. The running\n",
      "monitoring processes in that project might also be restarted.\n",
      "Additional resources\n",
      "Determining why Prometheus is consuming a lot of disk space\n",
      " for steps to query which metrics\n",
      "have the highest number of scrape samples\n",
      "4.3.8. Setting log levels for monitoring components\n",
      "You can configure the log level for Prometheus Operator, Prometheus, and Thanos Ruler.\n",
      "The following log levels can be applied to each of those components in the \n",
      "user-workload-monitoring-\n",
      "config\n",
      " \n",
      "ConfigMap\n",
      " object:\n",
      "debug\n",
      ". Log debug, informational, warning, and error messages.\n",
      "info\n",
      ". Log informational, warning, and error messages.\n",
      "warn\n",
      ". Log warning and error messages only.\n",
      "error\n",
      ". Log error messages only.\n",
      "The default log level is \n",
      "info\n",
      ".\n",
      "$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config\n",
      "apiVersion:\n",
      " v1\n",
      "kind:\n",
      " ConfigMap\n",
      "metadata:\n",
      "  name:\n",
      " user-workload-monitoring-config\n",
      "  namespace:\n",
      " openshift-user-workload-monitoring\n",
      "data:\n",
      "  config.yaml: \n",
      "|\n",
      "    prometheus:\n",
      "      enforcedSampleLimit:\n",
      " \n",
      "50000\n",
      " \n",
      "1\n",
      "\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "49\n",
      "1\n",
      "2\n",
      "Prerequisites\n",
      "You have access to the cluster as a user with the \n",
      "dedicated-admin\n",
      " role.\n",
      "You have created the \n",
      "user-workload-monitoring-config\n",
      " \n",
      "ConfigMap\n",
      " object.\n",
      "You have installed the OpenShift CLI (\n",
      "oc\n",
      ").\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Edit the \n",
      "ConfigMap\n",
      " object:\n",
      "a\n",
      ". \n",
      "Edit the \n",
      "user-workload-monitoring-config\n",
      " \n",
      "ConfigMap\n",
      " object in the \n",
      "openshift-user-\n",
      "workload-monitoring\n",
      " project:\n",
      "b\n",
      ". \n",
      "Add \n",
      "logLevel: <log_level>\n",
      " for a component under \n",
      "data.config.yaml\n",
      ":\n",
      "The monitoring component that you are applying a log level to.\n",
      "The log level to apply to the component.\n",
      "2\n",
      ". \n",
      "Save the file to apply the changes. The pods for the component restarts automatically when you\n",
      "apply the log-level change.\n",
      "WARNING\n",
      "When changes are saved to a monitoring config map, the pods and other\n",
      "resources in the related project might be redeployed. The running\n",
      "monitoring processes in that project might also be restarted.\n",
      "3\n",
      ". \n",
      "Confirm that the log level has been applied by reviewing the deployment or pod configuration in\n",
      "the related project. The following example checks the log level in the \n",
      "prometheus-operator\n",
      "deployment in the \n",
      "openshift-user-workload-monitoring\n",
      " project:\n",
      "$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-\n",
      "config\n",
      "apiVersion:\n",
      " v1\n",
      "kind:\n",
      " ConfigMap\n",
      "metadata:\n",
      "  name:\n",
      " user-workload-monitoring-config\n",
      "  namespace:\n",
      " openshift-user-workload-monitoring\n",
      "data:\n",
      "  config.yaml: \n",
      "|\n",
      "    <component>: \n",
      "1\n",
      "      logLevel:\n",
      " <log_level> \n",
      "2\n",
      "\n",
      "$ oc -n openshift-user-workload-monitoring get deploy prometheus-operator -o yaml |  grep\n",
      " \n",
      "\"log-level\"\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "50\n",
      "Example output\n",
      "4\n",
      ". \n",
      "Check that the pods for the component are running. The following example lists the status of\n",
      "pods in the \n",
      "openshift-user-workload-monitoring\n",
      " project:\n",
      "NOTE\n",
      "If an unrecognized \n",
      "loglevel\n",
      " value is included in the \n",
      "ConfigMap\n",
      " object, the pods\n",
      "for the component might not restart successfully.\n",
      "4.3.9. Next steps\n",
      "Managing metrics\n",
      "4.4. ENABLING ALERT ROUTING FOR USER-DEFINED PROJECTS\n",
      "In Red Hat OpenShift Service on AWS, a cluster administrator can enable alert routing for user-defined\n",
      "projects.\n",
      "IMPORTANT\n",
      "Managing alerting rules for user-defined projects is only available in Red Hat OpenShift\n",
      "Service on AWS version 4.11 and up.\n",
      "This process consists of two general steps:\n",
      "Enable alert routing for user-defined projects to use a separate Alertmanager instance.\n",
      "Grant additional users permission to configure alert routing for user-defined projects.\n",
      "After you complete these steps, developers and other users can configure custom alerts and alert\n",
      "routing for their user-defined projects.\n",
      "4.4.1. Understanding alert routing for user-defined projects\n",
      "As a cluster administrator, you can enable alert routing for user-defined projects. With this feature, you\n",
      "can allow users with the \n",
      "alert-routing-edit\n",
      " role to configure alert notification routing and receivers for\n",
      "user-defined projects. These notifications are routed by an Alertmanager instance dedicated to user-\n",
      "defined monitoring.\n",
      "Users can then create and configure user-defined alert routing by creating or editing the \n",
      "AlertmanagerConfig\n",
      " objects for their user-defined projects without the help of an administrator.\n",
      "After a user has defined alert routing for a user-defined project, user-defined alert notifications are\n",
      "routed to the \n",
      "alertmanager-user-workload\n",
      " pods in the \n",
      "openshift-user-workload-monitoring\n",
      "namespace.\n",
      "NOTE\n",
      "        - --log-level=debug\n",
      "$ oc -n openshift-user-workload-monitoring get pods\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "51\n",
      "1\n",
      "2\n",
      "NOTE\n",
      "The following are limitations of alert routing for user-defined projects:\n",
      "For user-defined alerting rules, user-defined routing is scoped to the namespace\n",
      "in which the resource is defined. For example, a routing configuration in\n",
      "namespace \n",
      "ns1\n",
      " only applies to \n",
      "PrometheusRules\n",
      " resources in the same\n",
      "namespace.\n",
      "When a namespace is excluded from user-defined monitoring, \n",
      "AlertmanagerConfig\n",
      " resources in the namespace cease to be part of the\n",
      "Alertmanager configuration.\n",
      "4.4.2. Enabling a separate Alertmanager instance for user-defined alert routing\n",
      "In Red Hat OpenShift Service on AWS, you may want to deploy a dedicated Alertmanager instance for\n",
      "user-defined projects, which provides user-defined alerts separate from default platform alerts. In these\n",
      "cases, you can optionally enable a separate instance of Alertmanager to send alerts for user-defined\n",
      "projects only.\n",
      "Prerequisites\n",
      "You have access to the cluster as a user with the \n",
      "cluster-admin\n",
      " or \n",
      "dedicated-admin\n",
      " role.\n",
      "You have enabled monitoring for user-defined projects in the \n",
      "cluster-monitoring-config\n",
      "config map for the \n",
      "openshift-monitoring\n",
      " namespace.\n",
      "You have installed the OpenShift CLI (\n",
      "oc\n",
      ").\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Edit the \n",
      "user-workload-monitoring-config\n",
      " \n",
      "ConfigMap\n",
      " object:\n",
      "2\n",
      ". \n",
      "Add \n",
      "enabled: true\n",
      " and \n",
      "enableAlertmanagerConfig: true\n",
      " in the \n",
      "alertmanager\n",
      " section under \n",
      "data/config.yaml\n",
      ":\n",
      "Set the \n",
      "enabled\n",
      " value to \n",
      "true\n",
      " to enable a dedicated instance of the Alertmanager for\n",
      "user-defined projects in a cluster. Set the value to \n",
      "false\n",
      " or omit the key entirely to disable\n",
      "the Alertmanager for user-defined projects. If you set this value to \n",
      "false\n",
      " or if the key is\n",
      "omitted, user-defined alerts are routed to the default platform Alertmanager instance.\n",
      "Set the \n",
      "enableAlertmanagerConfig\n",
      " value to \n",
      "true\n",
      " to enable users to define their own alert\n",
      "routing configurations with \n",
      "AlertmanagerConfig\n",
      " objects.\n",
      "$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config\n",
      "apiVersion:\n",
      " v1\n",
      "kind:\n",
      " ConfigMap\n",
      "metadata:\n",
      "  name:\n",
      " user-workload-monitoring-config\n",
      "  namespace:\n",
      " openshift-user-workload-monitoring\n",
      "data:\n",
      "  config.yaml: \n",
      "|\n",
      "    alertmanager:\n",
      "      enabled:\n",
      " \n",
      "true\n",
      " \n",
      "1\n",
      "      enableAlertmanagerConfig:\n",
      " \n",
      "true\n",
      " \n",
      "2\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "52\n",
      "1\n",
      "routing configurations with \n",
      "AlertmanagerConfig\n",
      " objects.\n",
      "3\n",
      ". \n",
      "Save the file to apply the changes. The dedicated instance of Alertmanager for user-defined\n",
      "projects starts automatically.\n",
      "Verification\n",
      "Verify that the \n",
      "user-workload\n",
      " Alertmanager instance has started:\n",
      "Example output\n",
      "4.4.3. Granting users permission to configure alert routing for user-defined projects\n",
      "You can grant users permission to configure alert routing for user-defined projects.\n",
      "Prerequisites\n",
      "You have access to the cluster as a user with the \n",
      "cluster-admin\n",
      " or \n",
      "dedicated-admin\n",
      " role.\n",
      "The user account that you are assigning the role to already exists.\n",
      "You have installed the OpenShift CLI (\n",
      "oc\n",
      ").\n",
      "You have enabled monitoring for user-defined projects.\n",
      "Procedure\n",
      "Assign the \n",
      "alert-routing-edit\n",
      " role to a user in the user-defined project:\n",
      "For \n",
      "<namespace>\n",
      ", substitute the namespace for the user-defined project, such as \n",
      "ns1\n",
      ".\n",
      "For \n",
      "<user>\n",
      ", substitute the username for the account to which you want to assign the role.\n",
      "Additional resources\n",
      "Accessing monitoring for user-defined projects\n",
      "Creating alert routing for user-defined projects\n",
      "4.5. MANAGING METRICS\n",
      "Red Hat OpenShift Service on AWS collects metrics for cluster components, and you can use\n",
      "Prometheus to query and visualize them.\n",
      "4.5.1. Understanding metrics\n",
      "# oc -n openshift-user-workload-monitoring get alertmanager\n",
      "NAME            VERSION   REPLICAS   AGE\n",
      "user-workload   0.24.0    2          100s\n",
      "$ oc -n <namespace> adm policy add-role-to-user alert-routing-edit <user> \n",
      "1\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "53\n",
      "In Red Hat OpenShift Service on AWS, cluster components are monitored by scraping metrics exposed\n",
      "through service endpoints. You can also configure metrics collection for user-defined projects. Metrics\n",
      "enable you to monitor how cluster components and your own workloads are performing.\n",
      "You can define the metrics that you want to provide for your own workloads by using Prometheus client\n",
      "libraries at the application level.\n",
      "In Red Hat OpenShift Service on AWS, metrics are exposed through an HTTP service endpoint under\n",
      "the \n",
      "/metrics\n",
      " canonical name. You can list all available metrics for a service by running a \n",
      "curl\n",
      " query\n",
      "against \n",
      "http://<endpoint>/metrics\n",
      ". For instance, you can expose a route to the \n",
      "prometheus-example-\n",
      "app\n",
      " example application and then run the following to view all of its available metrics:\n",
      "Example output\n",
      "Additional resources\n",
      "See the \n",
      "Prometheus documentation\n",
      " for details on Prometheus client libraries.\n",
      "4.5.2. Setting up metrics collection for user-defined projects\n",
      "You can create a \n",
      "ServiceMonitor\n",
      " resource to scrape metrics from a service endpoint in a user-defined\n",
      "project. This assumes that your application uses a Prometheus client library to expose metrics to the \n",
      "/metrics\n",
      " canonical name.\n",
      "This section describes how to deploy a sample service in a user-defined project and then create a \n",
      "ServiceMonitor\n",
      " resource that defines how that service should be monitored.\n",
      "4.5.2.1. Deploying a sample service\n",
      "To test monitoring of a service in a user-defined project, you can deploy a sample service.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Create a YAML file for the service configuration. In this example, it is called \n",
      "prometheus-\n",
      "example-app.yaml\n",
      ".\n",
      "2\n",
      ". \n",
      "Add the following deployment and service configuration details to the file:\n",
      "$ curl http://<example_app_endpoint>/metrics\n",
      "# HELP http_requests_total Count of all HTTP requests\n",
      "# TYPE http_requests_total counter\n",
      "http_requests_total{code=\"200\",method=\"get\"} 4\n",
      "http_requests_total{code=\"404\",method=\"get\"} 2\n",
      "# HELP version Version information about this binary\n",
      "# TYPE version gauge\n",
      "version{version=\"v0.1.0\"} 1\n",
      "apiVersion:\n",
      " v1\n",
      "kind:\n",
      " Namespace\n",
      "metadata:\n",
      "  name:\n",
      " ns1\n",
      "---\n",
      "apiVersion:\n",
      " apps/v1\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "54\n",
      "This configuration deploys a service named \n",
      "prometheus-example-app\n",
      " in the user-defined \n",
      "ns1\n",
      "project. This service exposes the custom \n",
      "version\n",
      " metric.\n",
      "3\n",
      ". \n",
      "Apply the configuration to the cluster:\n",
      "It takes some time to deploy the service.\n",
      "4\n",
      ". \n",
      "You can check that the pod is running:\n",
      "Example output\n",
      "kind:\n",
      " Deployment\n",
      "metadata:\n",
      "  labels:\n",
      "    app:\n",
      " prometheus-example-app\n",
      "  name:\n",
      " prometheus-example-app\n",
      "  namespace:\n",
      " ns1\n",
      "spec:\n",
      "  replicas:\n",
      " \n",
      "1\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app:\n",
      " prometheus-example-app\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app:\n",
      " prometheus-example-app\n",
      "    spec:\n",
      "      containers:\n",
      "      - image:\n",
      " quay.io/brancz/prometheus-example-app:v0\n",
      ".2\n",
      ".0\n",
      "        imagePullPolicy:\n",
      " IfNotPresent\n",
      "        name:\n",
      " prometheus-example-app\n",
      "---\n",
      "apiVersion:\n",
      " v1\n",
      "kind:\n",
      " Service\n",
      "metadata:\n",
      "  labels:\n",
      "    app:\n",
      " prometheus-example-app\n",
      "  name:\n",
      " prometheus-example-app\n",
      "  namespace:\n",
      " ns1\n",
      "spec:\n",
      "  ports:\n",
      "  - port:\n",
      " \n",
      "8080\n",
      "    protocol:\n",
      " TCP\n",
      "    targetPort:\n",
      " \n",
      "8080\n",
      "    name:\n",
      " web\n",
      "  selector:\n",
      "    app:\n",
      " prometheus-example-app\n",
      "  type:\n",
      " ClusterIP\n",
      "$ oc apply -f prometheus-example-app.yaml\n",
      "$ oc -n ns1 get pod\n",
      "NAME                                      READY     STATUS    RESTARTS   AGE\n",
      "prometheus-example-app-7857545cb7-sbgwq   1/1       Running   0          81m\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "55\n",
      "4.5.2.2. Specifying how a service is monitored\n",
      "To use the metrics exposed by your service, you must configure Red Hat OpenShift Service on AWS\n",
      "monitoring to scrape metrics from the \n",
      "/metrics\n",
      " endpoint. You can do this using a \n",
      "ServiceMonitor\n",
      "custom resource definition (CRD) that specifies how a service should be monitored, or a \n",
      "PodMonitor\n",
      "CRD that specifies how a pod should be monitored. The former requires a \n",
      "Service\n",
      " object, while the\n",
      "latter does not, allowing Prometheus to directly scrape metrics from the metrics endpoint exposed by a\n",
      "pod.\n",
      "NOTE\n",
      "In Red Hat OpenShift Service on AWS, you can use the \n",
      "tlsConfig\n",
      " property for a \n",
      "ServiceMonitor\n",
      " resource to specify the TLS configuration to use when scraping metrics\n",
      "from an endpoint. The \n",
      "tlsConfig\n",
      " property is not yet available for \n",
      "PodMonitor\n",
      " resources.\n",
      "If you need to use a TLS configuration when scraping metrics, you must use the \n",
      "ServiceMonitor\n",
      " resource.\n",
      "This procedure shows you how to create a \n",
      "ServiceMonitor\n",
      " resource for a service in a user-defined\n",
      "project.\n",
      "Prerequisites\n",
      "You have access to the cluster as a user with the \n",
      "dedicated-admin\n",
      " role or the \n",
      "monitoring-edit\n",
      "role.\n",
      "For this example, you have deployed the \n",
      "prometheus-example-app\n",
      " sample service in the \n",
      "ns1\n",
      "project.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Create a YAML file for the \n",
      "ServiceMonitor\n",
      " resource configuration. In this example, the file is\n",
      "called \n",
      "example-app-service-monitor.yaml\n",
      ".\n",
      "2\n",
      ". \n",
      "Add the following \n",
      "ServiceMonitor\n",
      " resource configuration details:\n",
      "This defines a \n",
      "ServiceMonitor\n",
      " resource that scrapes the metrics exposed by the \n",
      "prometheus-\n",
      "example-app\n",
      " sample service, which includes the \n",
      "version\n",
      " metric.\n",
      "3\n",
      ". \n",
      "Apply the configuration to the cluster:\n",
      "apiVersion:\n",
      " monitoring.coreos.com/v1\n",
      "kind:\n",
      " ServiceMonitor\n",
      "metadata:\n",
      "  labels:\n",
      "    k8s-app:\n",
      " prometheus-example-monitor\n",
      "  name:\n",
      " prometheus-example-monitor\n",
      "  namespace:\n",
      " ns1\n",
      "spec:\n",
      "  endpoints:\n",
      "  - interval:\n",
      " \n",
      "30\n",
      "s\n",
      "    port:\n",
      " web\n",
      "    scheme:\n",
      " http\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app:\n",
      " prometheus-example-app\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "56\n",
      "It takes some time to deploy the \n",
      "ServiceMonitor\n",
      " resource.\n",
      "4\n",
      ". \n",
      "You can check that the \n",
      "ServiceMonitor\n",
      " resource is running:\n",
      "Example output\n",
      "Additional resources\n",
      "Accessing monitoring for user-defined projects\n",
      ".\n",
      "4.5.3. Querying metrics\n",
      "The OpenShift monitoring dashboard lets you run Prometheus Query Language (PromQL) queries to\n",
      "examine metrics visualized on a plot. This functionality provides information about the state of a cluster\n",
      "and any user-defined projects that you are monitoring.\n",
      "As a \n",
      "dedicated-admin\n",
      ", you can query one or more namespaces at a time for metrics about user-defined\n",
      "projects.\n",
      "As a developer, you must specify a project name when querying metrics. You must have the required\n",
      "privileges to view metrics for the selected project.\n",
      "4.5.3.1. Querying metrics for all projects as an administrator\n",
      "As a \n",
      "dedicated-admin\n",
      " or as a user with view permissions for all projects, you can access metrics for all\n",
      "default Red Hat OpenShift Service on AWS and user-defined projects in the Metrics UI.\n",
      "NOTE\n",
      "Only dedicated administrators have access to the third-party UIs provided with Red Hat\n",
      "OpenShift Service on AWS Monitoring.\n",
      "Prerequisites\n",
      "You have access to the cluster as a user with the \n",
      "dedicated-admin\n",
      " role or with view permissions\n",
      "for all projects.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "From the \n",
      "Administrator\n",
      " perspective in the OpenShift web console, select \n",
      "Observe\n",
      " \n",
      "→\n",
      " \n",
      "Metrics\n",
      ".\n",
      "2\n",
      ". \n",
      "Select \n",
      "Insert Metric at Cursor\n",
      " to view a list of predefined queries.\n",
      "3\n",
      ". \n",
      "To create a custom query, add your Prometheus Query Language (PromQL) query to the\n",
      "Expression\n",
      " field.\n",
      "$ oc apply -f example-app-service-monitor.yaml\n",
      "$ oc -n ns1 get servicemonitor\n",
      "NAME                         AGE\n",
      "prometheus-example-monitor   81m\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "57\n",
      "4\n",
      ". \n",
      "To add multiple queries, select \n",
      "Add Query\n",
      ".\n",
      "5\n",
      ". \n",
      "To delete a query, select \n",
      " \n",
      "next to the query, then choose \n",
      "Delete query\n",
      ".\n",
      "6\n",
      ". \n",
      "To disable a query from being run, select \n",
      " \n",
      "next to the query and choose \n",
      "Disable query\n",
      ".\n",
      "7\n",
      ". \n",
      "Select \n",
      "Run Queries\n",
      " to run the queries that you have created. The metrics from the queries are\n",
      "visualized on the plot. If a query is invalid, the UI shows an error message.\n",
      "NOTE\n",
      "Queries that operate on large amounts of data might time out or overload the\n",
      "browser when drawing time series graphs. To avoid this, select \n",
      "Hide graph\n",
      " and\n",
      "calibrate your query using only the metrics table. Then, after finding a feasible\n",
      "query, enable the plot to draw the graphs.\n",
      "8\n",
      ". \n",
      "Optional: The page URL now contains the queries you ran. To use this set of queries again in the\n",
      "future, save this URL.\n",
      "Additional resources\n",
      "See the \n",
      "Prometheus query documentation\n",
      " for more information about creating PromQL\n",
      "queries.\n",
      "4.5.3.2. Querying metrics for user-defined projects as a developer\n",
      "You can access metrics for a user-defined project as a developer or as a user with view permissions for\n",
      "the project.\n",
      "In the \n",
      "Developer\n",
      " perspective, the Metrics UI includes some predefined CPU, memory, bandwidth, and\n",
      "network packet queries for the selected project. You can also run custom Prometheus Query Language\n",
      "(PromQL) queries for CPU, memory, bandwidth, network packet and application metrics for the project.\n",
      "NOTE\n",
      "Developers can only use the \n",
      "Developer\n",
      " perspective and not the \n",
      "Administrator\n",
      "perspective. As a developer you can only query metrics for one project at a time.\n",
      "Developers cannot access the third-party UIs provided with Red Hat OpenShift Service\n",
      "on AWS monitoring.\n",
      "Prerequisites\n",
      "You have access to the cluster as a developer or as a user with view permissions for the project\n",
      "that you are viewing metrics for.\n",
      "You have enabled monitoring for user-defined projects.\n",
      "You have deployed a service in a user-defined project.\n",
      "You have created a \n",
      "ServiceMonitor\n",
      " custom resource definition (CRD) for the service to define\n",
      "how the service is monitored.\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "58\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "From the \n",
      "Developer\n",
      " perspective in the Red Hat OpenShift Service on AWS web console, select\n",
      "Observe\n",
      " \n",
      "→\n",
      " \n",
      "Metrics\n",
      ".\n",
      "2\n",
      ". \n",
      "Select the project that you want to view metrics for in the \n",
      "Project:\n",
      " list.\n",
      "3\n",
      ". \n",
      "Choose a query from the \n",
      "Select Query\n",
      " list, or run a custom PromQL query by selecting \n",
      "Show\n",
      "PromQL\n",
      ".\n",
      "NOTE\n",
      "In the \n",
      "Developer\n",
      " perspective, you can only run one query at a time.\n",
      "Additional resources\n",
      "See the \n",
      "Prometheus query documentation\n",
      " for more information about creating PromQL\n",
      "queries.\n",
      "See \n",
      "Querying metrics for user-defined projects as a developer\n",
      " for details on accessing non-\n",
      "cluster metrics as a developer or a privileged user\n",
      "4.5.3.3. Exploring the visualized metrics\n",
      "After running the queries, the metrics are displayed on an interactive plot. The X-axis in the plot\n",
      "represents time and the Y-axis represents metrics values. Each metric is shown as a colored line on the\n",
      "graph. You can manipulate the plot interactively and explore the metrics.\n",
      "Procedure\n",
      "In the \n",
      "Administrator\n",
      " perspective:\n",
      "1\n",
      ". \n",
      "Initially, all metrics from all enabled queries are shown on the plot. You can select which metrics\n",
      "are shown.\n",
      "NOTE\n",
      "By default, the query table shows an expanded view that lists every metric and its\n",
      "current value. You can select \n",
      "˅\n",
      " to minimize the expanded view for a query.\n",
      "To hide all metrics from a query, click \n",
      " \n",
      "for the query and click \n",
      "Hide all series\n",
      ".\n",
      "To hide a specific metric, go to the query table and click the colored square near the metric\n",
      "name.\n",
      "2\n",
      ". \n",
      "To zoom into the plot and change the time range, do one of the following:\n",
      "Visually select the time range by clicking and dragging on the plot horizontally.\n",
      "Use the menu in the left upper corner to select the time range.\n",
      "3\n",
      ". \n",
      "To reset the time range, select \n",
      "Reset Zoom\n",
      ".\n",
      "4\n",
      ". \n",
      "To display outputs for all queries at a specific point in time, hover over the plot at that point.\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "59\n",
      "4\n",
      ". \n",
      "To display outputs for all queries at a specific point in time, hover over the plot at that point.\n",
      "The query outputs appear in a pop-up box.\n",
      "5\n",
      ". \n",
      "To hide the plot, select \n",
      "Hide Graph\n",
      ".\n",
      "In the \n",
      "Developer\n",
      " perspective:\n",
      "1\n",
      ". \n",
      "To zoom into the plot and change the time range, do one of the following:\n",
      "Visually select the time range by clicking and dragging on the plot horizontally.\n",
      "Use the menu in the left upper corner to select the time range.\n",
      "2\n",
      ". \n",
      "To reset the time range, select \n",
      "Reset Zoom\n",
      ".\n",
      "3\n",
      ". \n",
      "To display outputs for all queries at a specific point in time, hover over the plot at that point.\n",
      "The query outputs appear in a pop-up box.\n",
      "Additional resources\n",
      "See the \n",
      "Querying metrics\n",
      " section on using the PromQL interface\n",
      "Troubleshooting monitoring issues\n",
      "4.5.4. Next steps\n",
      "Alerts\n",
      "4.6. ALERTS\n",
      "In Red Hat OpenShift Service on AWS, the Alerting UI enables you to manage alerts, silences, and\n",
      "alerting rules.\n",
      "Alerting rules\n",
      ". Alerting rules contain a set of conditions that outline a particular state within a\n",
      "cluster. Alerts are triggered when those conditions are true. An alerting rule can be assigned a\n",
      "severity that defines how the alerts are routed.\n",
      "Alerts\n",
      ". An alert is fired when the conditions defined in an alerting rule are true. Alerts provide a\n",
      "notification that a set of circumstances are apparent within an Red Hat OpenShift Service on\n",
      "AWS cluster.\n",
      "Silences\n",
      ". A silence can be applied to an alert to prevent notifications from being sent when the\n",
      "conditions for an alert are true. You can mute an alert after the initial notification, while you work\n",
      "on resolving the underlying issue.\n",
      "NOTE\n",
      "The alerts, silences, and alerting rules that are available in the Alerting UI relate to the\n",
      "projects that you have access to. For example, if you are logged in with \n",
      "cluster-admin\n",
      " or \n",
      "dedicated-admin\n",
      " privileges, all alerts, silences, and alerting rules are accessible.\n",
      "4.6.1. Accessing the Alerting UI in the Administrator and Developer perspectives\n",
      "The Alerting UI is accessible through the Administrator perspective and the Developer perspective in\n",
      "the Red Hat OpenShift Service on AWS web console.\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "60\n",
      "In the \n",
      "Administrator\n",
      " perspective, select \n",
      "Observe\n",
      " \n",
      "→\n",
      " \n",
      "Alerting\n",
      ". The three main pages in the\n",
      "Alerting UI in this perspective are the \n",
      "Alerts\n",
      ", \n",
      "Silences\n",
      ", and \n",
      "Alerting Rules\n",
      " pages.\n",
      "In the \n",
      "Developer\n",
      " perspective, select \n",
      "Observe\n",
      " \n",
      "→\n",
      " \n",
      "<project_name>\n",
      " \n",
      "→\n",
      " \n",
      "Alerts\n",
      ". In this perspective,\n",
      "alerts, silences, and alerting rules are all managed from the \n",
      "Alerts\n",
      " page. The results shown in\n",
      "the \n",
      "Alerts\n",
      " page are specific to the selected project.\n",
      "NOTE\n",
      "In the Developer perspective, you can select from core Red Hat OpenShift Service on\n",
      "AWS and user-defined projects that you have access to in the \n",
      "Project:\n",
      " list. However,\n",
      "alerts, silences, and alerting rules relating to core Red Hat OpenShift Service on AWS\n",
      "projects are not displayed if you do not have \n",
      "cluster-admin\n",
      " privileges.\n",
      "4.6.2. Searching and filtering alerts, silences, and alerting rules\n",
      "You can filter the alerts, silences, and alerting rules that are displayed in the Alerting UI. This section\n",
      "provides a description of each of the available filtering options.\n",
      "Understanding alert filters\n",
      "In the \n",
      "Administrator\n",
      " perspective, the \n",
      "Alerts\n",
      " page in the Alerting UI provides details about alerts\n",
      "relating to default Red Hat OpenShift Service on AWS and user-defined projects. The page includes a\n",
      "summary of severity, state, and source for each alert. The time at which an alert went into its current\n",
      "state is also shown.\n",
      "You can filter by alert state, severity, and source. By default, only \n",
      "Platform\n",
      " alerts that are \n",
      "Firing\n",
      " are\n",
      "displayed. The following describes each alert filtering option:\n",
      "Alert State\n",
      " filters:\n",
      "Firing\n",
      ". The alert is firing because the alert condition is true and the optional \n",
      "for\n",
      " duration\n",
      "has passed. The alert will continue to fire as long as the condition remains true.\n",
      "Pending\n",
      ". The alert is active but is waiting for the duration that is specified in the alerting\n",
      "rule before it fires.\n",
      "Silenced\n",
      ". The alert is now silenced for a defined time period. Silences temporarily mute\n",
      "alerts based on a set of label selectors that you define. Notifications will not be sent for\n",
      "alerts that match all the listed values or regular expressions.\n",
      "Severity\n",
      " filters:\n",
      "Critical\n",
      ". The condition that triggered the alert could have a critical impact. The alert\n",
      "requires immediate attention when fired and is typically paged to an individual or to a critical\n",
      "response team.\n",
      "Warning\n",
      ". The alert provides a warning notification about something that might require\n",
      "attention to prevent a problem from occurring. Warnings are typically routed to a ticketing\n",
      "system for non-immediate review.\n",
      "Info\n",
      ". The alert is provided for informational purposes only.\n",
      "None\n",
      ". The alert has no defined severity.\n",
      "You can also create custom severity definitions for alerts relating to user-defined projects.\n",
      "Source\n",
      " filters:\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "61\n",
      "Platform\n",
      ". Platform-level alerts relate only to default Red Hat OpenShift Service on AWS\n",
      "projects. These projects provide core Red Hat OpenShift Service on AWS functionality.\n",
      "User\n",
      ". User alerts relate to user-defined projects. These alerts are user-created and are\n",
      "customizable. User-defined workload monitoring can be enabled post-installation to\n",
      "provide observability into your own workloads.\n",
      "Understanding silence filters\n",
      "In the \n",
      "Administrator\n",
      " perspective, the \n",
      "Silences\n",
      " page in the Alerting UI provides details about silences\n",
      "applied to alerts in default Red Hat OpenShift Service on AWS and user-defined projects. The page\n",
      "includes a summary of the state of each silence and the time at which a silence ends.\n",
      "You can filter by silence state. By default, only \n",
      "Active\n",
      " and \n",
      "Pending\n",
      " silences are displayed. The following\n",
      "describes each silence state filter option:\n",
      "Silence State\n",
      " filters:\n",
      "Active\n",
      ". The silence is active and the alert will be muted until the silence is expired.\n",
      "Pending\n",
      ". The silence has been scheduled and it is not yet active.\n",
      "Expired\n",
      ". The silence has expired and notifications will be sent if the conditions for an alert\n",
      "are true.\n",
      "Understanding alerting rule filters\n",
      "In the \n",
      "Administrator\n",
      " perspective, the \n",
      "Alerting Rules\n",
      " page in the Alerting UI provides details about\n",
      "alerting rules relating to default Red Hat OpenShift Service on AWS and user-defined projects. The\n",
      "page includes a summary of the state, severity, and source for each alerting rule.\n",
      "You can filter alerting rules by alert state, severity, and source. By default, only \n",
      "Platform\n",
      " alerting rules\n",
      "are displayed. The following describes each alerting rule filtering option:\n",
      "Alert State\n",
      " filters:\n",
      "Firing\n",
      ". The alert is firing because the alert condition is true and the optional \n",
      "for\n",
      " duration\n",
      "has passed. The alert will continue to fire as long as the condition remains true.\n",
      "Pending\n",
      ". The alert is active but is waiting for the duration that is specified in the alerting\n",
      "rule before it fires.\n",
      "Silenced\n",
      ". The alert is now silenced for a defined time period. Silences temporarily mute\n",
      "alerts based on a set of label selectors that you define. Notifications will not be sent for\n",
      "alerts that match all the listed values or regular expressions.\n",
      "Not Firing\n",
      ". The alert is not firing.\n",
      "Severity\n",
      " filters:\n",
      "Critical\n",
      ". The conditions defined in the alerting rule could have a critical impact. When true,\n",
      "these conditions require immediate attention. Alerts relating to the rule are typically paged\n",
      "to an individual or to a critical response team.\n",
      "Warning\n",
      ". The conditions defined in the alerting rule might require attention to prevent a\n",
      "problem from occurring. Alerts relating to the rule are typically routed to a ticketing system\n",
      "for non-immediate review.\n",
      "Info\n",
      ". The alerting rule provides informational alerts only.\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "62\n",
      "None\n",
      ". The alerting rule has no defined severity.\n",
      "You can also create custom severity definitions for alerting rules relating to user-defined\n",
      "projects.\n",
      "Source\n",
      " filters:\n",
      "Platform\n",
      ". Platform-level alerting rules relate only to default Red Hat OpenShift Service on\n",
      "AWS projects. These projects provide core Red Hat OpenShift Service on AWS\n",
      "functionality.\n",
      "User\n",
      ". User-defined workload alerting rules relate to user-defined projects. These alerting\n",
      "rules are user-created and are customizable. User-defined workload monitoring can be\n",
      "enabled post-installation to provide observability into your own workloads.\n",
      "Searching and filtering alerts, silences, and alerting rules in the Developer perspective\n",
      "In the \n",
      "Developer\n",
      " perspective, the Alerts page in the Alerting UI provides a combined view of alerts and\n",
      "silences relating to the selected project. A link to the governing alerting rule is provided for each\n",
      "displayed alert.\n",
      "In this view, you can filter by alert state and severity. By default, all alerts in the selected project are\n",
      "displayed if you have permission to access the project. These filters are the same as those described for\n",
      "the \n",
      "Administrator\n",
      " perspective.\n",
      "4.6.3. Getting information about alerts, silences, and alerting rules\n",
      "The Alerting UI provides detailed information about alerts and their governing alerting rules and silences.\n",
      "Prerequisites\n",
      "You have access to the cluster as a developer or as a user with view permissions for the project\n",
      "that you are viewing metrics for.\n",
      "Procedure\n",
      "To obtain information about alerts in the Administrator perspective\n",
      ":\n",
      "1\n",
      ". \n",
      "Open the Red Hat OpenShift Service on AWS web console and navigate to the \n",
      "Observe\n",
      " \n",
      "→\n",
      "Alerting\n",
      " \n",
      "→\n",
      " \n",
      "Alerts\n",
      " page.\n",
      "2\n",
      ". \n",
      "Optional: Search for alerts by name using the \n",
      "Name\n",
      " field in the search list.\n",
      "3\n",
      ". \n",
      "Optional: Filter alerts by state, severity, and source by selecting filters in the \n",
      "Filter\n",
      " list.\n",
      "4\n",
      ". \n",
      "Optional: Sort the alerts by clicking one or more of the \n",
      "Name\n",
      ", \n",
      "Severity\n",
      ", \n",
      "State\n",
      ", and \n",
      "Source\n",
      "column headers.\n",
      "5\n",
      ". \n",
      "Select the name of an alert to navigate to its \n",
      "Alert Details\n",
      " page. The page includes a graph that\n",
      "illustrates alert time series data. It also provides information about the alert, including:\n",
      "A description of the alert\n",
      "Messages associated with the alerts\n",
      "Labels attached to the alert\n",
      "A link to its governing alerting rule\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "63\n",
      "Silences for the alert, if any exist\n",
      "To obtain information about silences in the Administrator perspective\n",
      ":\n",
      "1\n",
      ". \n",
      "Navigate to the \n",
      "Observe\n",
      " \n",
      "→\n",
      " \n",
      "Alerting\n",
      " \n",
      "→\n",
      " \n",
      "Silences\n",
      " page.\n",
      "2\n",
      ". \n",
      "Optional: Filter the silences by name using the \n",
      "Search by name\n",
      " field.\n",
      "3\n",
      ". \n",
      "Optional: Filter silences by state by selecting filters in the \n",
      "Filter\n",
      " list. By default, \n",
      "Active\n",
      " and\n",
      "Pending\n",
      " filters are applied.\n",
      "4\n",
      ". \n",
      "Optional: Sort the silences by clicking one or more of the \n",
      "Name\n",
      ", \n",
      "Firing Alerts\n",
      ", and \n",
      "State\n",
      " column\n",
      "headers.\n",
      "5\n",
      ". \n",
      "Select the name of a silence to navigate to its \n",
      "Silence Details\n",
      " page. The page includes the\n",
      "following details:\n",
      "Alert specification\n",
      "Start time\n",
      "End time\n",
      "Silence state\n",
      "Number and list of firing alerts\n",
      "To obtain information about alerting rules in the Administrator perspective\n",
      ":\n",
      "1\n",
      ". \n",
      "Navigate to the \n",
      "Observe\n",
      " \n",
      "→\n",
      " \n",
      "Alerting\n",
      " \n",
      "→\n",
      " \n",
      "Alerting Rules\n",
      " page.\n",
      "2\n",
      ". \n",
      "Optional: Filter alerting rules by state, severity, and source by selecting filters in the \n",
      "Filter\n",
      " list.\n",
      "3\n",
      ". \n",
      "Optional: Sort the alerting rules by clicking one or more of the \n",
      "Name\n",
      ", \n",
      "Severity\n",
      ", \n",
      "Alert State\n",
      ", and\n",
      "Source\n",
      " column headers.\n",
      "4\n",
      ". \n",
      "Select the name of an alerting rule to navigate to its \n",
      "Alerting Rule Details\n",
      " page. The page\n",
      "provides the following details about the alerting rule:\n",
      "Alerting rule name, severity, and description\n",
      "The expression that defines the condition for firing the alert\n",
      "The time for which the condition should be true for an alert to fire\n",
      "A graph for each alert governed by the alerting rule, showing the value with which the alert is\n",
      "firing\n",
      "A table of all alerts governed by the alerting rule\n",
      "To obtain information about alerts, silences, and alerting rules in the Developer perspective\n",
      ":\n",
      "1\n",
      ". \n",
      "Navigate to the \n",
      "Observe\n",
      " \n",
      "→\n",
      " \n",
      "<project_name>\n",
      " \n",
      "→\n",
      " \n",
      "Alerts\n",
      " page.\n",
      "2\n",
      ". \n",
      "View details for an alert, silence, or an alerting rule:\n",
      "Alert Details\n",
      " can be viewed by selecting \n",
      ">\n",
      " to the left of an alert name and then selecting\n",
      "the alert in the list.\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "64\n",
      "Silence Details\n",
      " can be viewed by selecting a silence in the \n",
      "Silenced By\n",
      " section of the \n",
      "Alert\n",
      "Details\n",
      " page. The \n",
      "Silence Details\n",
      " page includes the following information:\n",
      "Alert specification\n",
      "Start time\n",
      "End time\n",
      "Silence state\n",
      "Number and list of firing alerts\n",
      "Alerting Rule Details\n",
      " can be viewed by selecting \n",
      "View Alerting Rule\n",
      " in the \n",
      " \n",
      "menu on\n",
      "the right of an alert in the \n",
      "Alerts\n",
      " page.\n",
      "NOTE\n",
      "Only alerts, silences, and alerting rules relating to the selected project are displayed in the\n",
      "Developer\n",
      " perspective.\n",
      "4.6.4. Managing silences\n",
      "You can create a silence to stop receiving notifications about an alert when it is firing. It might be useful\n",
      "to silence an alert after being first notified, while you resolve the underlying issue.\n",
      "When creating a silence, you must specify whether it becomes active immediately or at a later time. You\n",
      "must also set a duration period after which the silence expires.\n",
      "You can view, edit, and expire existing silences.\n",
      "4.6.4.1. Silencing alerts\n",
      "You can either silence a specific alert or silence alerts that match a specification that you define.\n",
      "Prerequisites\n",
      "You are a cluster administrator and have access to the cluster as a user with the \n",
      "cluster-admin\n",
      "cluster role.\n",
      "You are a non-administator user and have access to the cluster as a user with the following user\n",
      "roles:\n",
      "The \n",
      "cluster-monitoring-view\n",
      " cluster role, which allows you to access Alertmanager.\n",
      "The \n",
      "monitoring-alertmanager-edit\n",
      " role, which permits you to create and silence alerts in\n",
      "the \n",
      "Administrator\n",
      " perspective in the web console.\n",
      "The \n",
      "monitoring-rules-edit\n",
      " role, which permits you to create and silence alerts in the\n",
      "Developer\n",
      " perspective in the web console.\n",
      "Procedure\n",
      "To silence a specific alert:\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "65\n",
      "In the \n",
      "Administrator\n",
      " perspective:\n",
      "1\n",
      ". \n",
      "Navigate to the \n",
      "Observe\n",
      " \n",
      "→\n",
      " \n",
      "Alerting\n",
      " \n",
      "→\n",
      " \n",
      "Alerts\n",
      " page of the Red Hat OpenShift Service on\n",
      "AWS web console.\n",
      "2\n",
      ". \n",
      "For the alert that you want to silence, select the \n",
      " \n",
      "in the right-hand column and select\n",
      "Silence Alert\n",
      ". The \n",
      "Silence Alert\n",
      " form will appear with a pre-populated specification for the\n",
      "chosen alert.\n",
      "3\n",
      ". \n",
      "Optional: Modify the silence.\n",
      "4\n",
      ". \n",
      "You must add a comment before creating the silence.\n",
      "5\n",
      ". \n",
      "To create the silence, select \n",
      "Silence\n",
      ".\n",
      "In the \n",
      "Developer\n",
      " perspective:\n",
      "1\n",
      ". \n",
      "Navigate to the \n",
      "Observe\n",
      " \n",
      "→\n",
      " \n",
      "<project_name>\n",
      " \n",
      "→\n",
      " \n",
      "Alerts\n",
      " page in the Red Hat OpenShift\n",
      "Service on AWS web console.\n",
      "2\n",
      ". \n",
      "Expand the details for an alert by selecting \n",
      ">\n",
      " to the left of the alert name. Select the name\n",
      "of the alert in the expanded view to open the \n",
      "Alert Details\n",
      " page for the alert.\n",
      "3\n",
      ". \n",
      "Select \n",
      "Silence Alert\n",
      ". The \n",
      "Silence Alert\n",
      " form will appear with a prepopulated specification\n",
      "for the chosen alert.\n",
      "4\n",
      ". \n",
      "Optional: Modify the silence.\n",
      "5\n",
      ". \n",
      "You must add a comment before creating the silence.\n",
      "6\n",
      ". \n",
      "To create the silence, select \n",
      "Silence\n",
      ".\n",
      "To silence a set of alerts by creating an alert specification in the \n",
      "Administrator\n",
      " perspective:\n",
      "1\n",
      ". \n",
      "Navigate to the \n",
      "Observe\n",
      " \n",
      "→\n",
      " \n",
      "Alerting\n",
      " \n",
      "→\n",
      " \n",
      "Silences\n",
      " page in the Red Hat OpenShift Service on\n",
      "AWS web console.\n",
      "2\n",
      ". \n",
      "Select \n",
      "Create Silence\n",
      ".\n",
      "3\n",
      ". \n",
      "Set the schedule, duration, and label details for an alert in the \n",
      "Create Silence\n",
      " form. You must\n",
      "also add a comment for the silence.\n",
      "4\n",
      ". \n",
      "To create silences for alerts that match the label sectors that you entered in the previous step,\n",
      "select \n",
      "Silence\n",
      ".\n",
      "4.6.4.2. Editing silences\n",
      "You can edit a silence, which will expire the existing silence and create a new one with the changed\n",
      "configuration.\n",
      "Procedure\n",
      "To edit a silence in the \n",
      "Administrator\n",
      " perspective:\n",
      "1\n",
      ". \n",
      "Navigate to the \n",
      "Observe\n",
      " \n",
      "→\n",
      " \n",
      "Alerting\n",
      " \n",
      "→\n",
      " \n",
      "Silences\n",
      " page.\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "66\n",
      "2\n",
      ". \n",
      "For the silence you want to modify, select the \n",
      " \n",
      "in the last column and choose \n",
      "Edit silence\n",
      ".\n",
      "Alternatively, you can select \n",
      "Actions\n",
      " \n",
      "→\n",
      " \n",
      "Edit Silence\n",
      " in the \n",
      "Silence Details\n",
      " page for a silence.\n",
      "3\n",
      ". \n",
      "In the \n",
      "Edit Silence\n",
      " page, enter your changes and select \n",
      "Silence\n",
      ". This will expire the existing\n",
      "silence and create one with the chosen configuration.\n",
      "To edit a silence in the \n",
      "Developer\n",
      " perspective:\n",
      "1\n",
      ". \n",
      "Navigate to the \n",
      "Observe\n",
      " \n",
      "→\n",
      " \n",
      "<project_name>\n",
      " \n",
      "→\n",
      " \n",
      "Alerts\n",
      " page.\n",
      "2\n",
      ". \n",
      "Expand the details for an alert by selecting \n",
      ">\n",
      " to the left of the alert name. Select the name of\n",
      "the alert in the expanded view to open the \n",
      "Alert Details\n",
      " page for the alert.\n",
      "3\n",
      ". \n",
      "Select the name of a silence in the \n",
      "Silenced By\n",
      " section in that page to navigate to the \n",
      "Silence\n",
      "Details\n",
      " page for the silence.\n",
      "4\n",
      ". \n",
      "Select the name of a silence to navigate to its \n",
      "Silence Details\n",
      " page.\n",
      "5\n",
      ". \n",
      "Select \n",
      "Actions\n",
      " \n",
      "→\n",
      " \n",
      "Edit Silence\n",
      " in the \n",
      "Silence Details\n",
      " page for a silence.\n",
      "6\n",
      ". \n",
      "In the \n",
      "Edit Silence\n",
      " page, enter your changes and select \n",
      "Silence\n",
      ". This will expire the existing\n",
      "silence and create one with the chosen configuration.\n",
      "4.6.4.3. Expiring silences\n",
      "You can expire a silence. Expiring a silence deactivates it forever.\n",
      "Procedure\n",
      "To expire a silence in the \n",
      "Administrator\n",
      " perspective:\n",
      "1\n",
      ". \n",
      "Navigate to the \n",
      "Observe\n",
      " \n",
      "→\n",
      " \n",
      "Alerting\n",
      " \n",
      "→\n",
      " \n",
      "Silences\n",
      " page.\n",
      "2\n",
      ". \n",
      "For the silence you want to modify, select the \n",
      " \n",
      "in the last column and choose \n",
      "Expire\n",
      "silence\n",
      ".\n",
      "Alternatively, you can select \n",
      "Actions\n",
      " \n",
      "→\n",
      " \n",
      "Expire Silence\n",
      " in the \n",
      "Silence Details\n",
      " page for a silence.\n",
      "To expire a silence in the \n",
      "Developer\n",
      " perspective:\n",
      "1\n",
      ". \n",
      "Navigate to the \n",
      "Observe\n",
      " \n",
      "→\n",
      " \n",
      "<project_name>\n",
      " \n",
      "→\n",
      " \n",
      "Alerts\n",
      " page.\n",
      "2\n",
      ". \n",
      "Expand the details for an alert by selecting \n",
      ">\n",
      " to the left of the alert name. Select the name of\n",
      "the alert in the expanded view to open the \n",
      "Alert Details\n",
      " page for the alert.\n",
      "3\n",
      ". \n",
      "Select the name of a silence in the \n",
      "Silenced By\n",
      " section in that page to navigate to the \n",
      "Silence\n",
      "Details\n",
      " page for the silence.\n",
      "4\n",
      ". \n",
      "Select the name of a silence to navigate to its \n",
      "Silence Details\n",
      " page.\n",
      "5\n",
      ". \n",
      "Select \n",
      "Actions\n",
      " \n",
      "→\n",
      " \n",
      "Expire Silence\n",
      " in the \n",
      "Silence Details\n",
      " page for a silence.\n",
      "4.6.5. Managing alerting rules for user-defined projects\n",
      "Red Hat OpenShift Service on AWS monitoring ships with a set of default alerting rules. As a cluster\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "67\n",
      "Red Hat OpenShift Service on AWS monitoring ships with a set of default alerting rules. As a cluster\n",
      "administrator, you can view the default alerting rules.\n",
      "In Red Hat OpenShift Service on AWS 4, you can create, view, edit, and remove alerting rules in user-\n",
      "defined projects.\n",
      "IMPORTANT\n",
      "Managing alerting rules for user-defined projects is only available in Red Hat OpenShift\n",
      "Service on AWS version 4.11 and up.\n",
      "Alerting rule considerations\n",
      "The default alerting rules are used specifically for the Red Hat OpenShift Service on AWS\n",
      "cluster.\n",
      "Some alerting rules intentionally have identical names. They send alerts about the same event\n",
      "with different thresholds, different severity, or both.\n",
      "Inhibition rules prevent notifications for lower severity alerts that are firing when a higher\n",
      "severity alert is also firing.\n",
      "4.6.5.1. Optimizing alerting for user-defined projects\n",
      "You can optimize alerting for your own projects by considering the following recommendations when\n",
      "creating alerting rules:\n",
      "Minimize the number of alerting rules that you create for your project\n",
      ". Create alerting rules\n",
      "that notify you of conditions that impact you. It is more difficult to notice relevant alerts if you\n",
      "generate many alerts for conditions that do not impact you.\n",
      "Create alerting rules for symptoms instead of causes\n",
      ". Create alerting rules that notify you of\n",
      "conditions regardless of the underlying cause. The cause can then be investigated. You will\n",
      "need many more alerting rules if each relates only to a specific cause. Some causes are then\n",
      "likely to be missed.\n",
      "Plan before you write your alerting rules\n",
      ". Determine what symptoms are important to you and\n",
      "what actions you want to take if they occur. Then build an alerting rule for each symptom.\n",
      "Provide clear alert messaging\n",
      ". State the symptom and recommended actions in the alert\n",
      "message.\n",
      "Include severity levels in your alerting rules\n",
      ". The severity of an alert depends on how you need\n",
      "to react if the reported symptom occurs. For example, a critical alert should be triggered if a\n",
      "symptom requires immediate attention by an individual or a critical response team.\n",
      "Optimize alert routing\n",
      ". Deploy an alerting rule directly on the Prometheus instance in the \n",
      "openshift-user-workload-monitoring\n",
      " project if the rule does not query default Red Hat\n",
      "OpenShift Service on AWS metrics. This reduces latency for alerting rules and minimizes the\n",
      "load on monitoring components.\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "68\n",
      "WARNING\n",
      "Default Red Hat OpenShift Service on AWS metrics for user-defined\n",
      "projects provide information about CPU and memory usage, bandwidth\n",
      "statistics, and packet rate information. Those metrics cannot be included in\n",
      "an alerting rule if you route the rule directly to the Prometheus instance in\n",
      "the \n",
      "openshift-user-workload-monitoring\n",
      " project. Alerting rule\n",
      "optimization should be used only if you have read the documentation and\n",
      "have a comprehensive understanding of the monitoring architecture.\n",
      "Additional resources\n",
      "See the \n",
      "Prometheus alerting documentation\n",
      " for further guidelines on optimizing alerts\n",
      "See \n",
      "Monitoring overview\n",
      " for details about Red Hat OpenShift Service on AWS 4 monitoring\n",
      "architecture\n",
      "4.6.5.2. Creating alerting rules for user-defined projects\n",
      "You can create alerting rules for user-defined projects. Those alerting rules will fire alerts based on the\n",
      "values of chosen metrics.\n",
      "Prerequisites\n",
      "You have enabled monitoring for user-defined projects.\n",
      "You are logged in as a user that has the \n",
      "monitoring-rules-edit\n",
      " role for the project where you\n",
      "want to create an alerting rule.\n",
      "You have installed the OpenShift CLI (\n",
      "oc\n",
      ").\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Create a YAML file for alerting rules. In this example, it is called \n",
      "example-app-alerting-\n",
      "rule.yaml\n",
      ".\n",
      "2\n",
      ". \n",
      "Add an alerting rule configuration to the YAML file. For example:\n",
      "NOTE\n",
      "When you create an alerting rule, a project label is enforced on it if a rule with the\n",
      "same name exists in another project.\n",
      "\n",
      "apiVersion:\n",
      " monitoring.coreos.com/v1\n",
      "kind:\n",
      " PrometheusRule\n",
      "metadata:\n",
      "  name:\n",
      " example-alert\n",
      "  namespace:\n",
      " ns1\n",
      "spec:\n",
      "  groups:\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "69\n",
      "This configuration creates an alerting rule named \n",
      "example-alert\n",
      ". The alerting rule fires an alert\n",
      "when the \n",
      "version\n",
      " metric exposed by the sample service becomes \n",
      "0\n",
      ".\n",
      "IMPORTANT\n",
      "A user-defined alerting rule can include metrics for its own project and cluster\n",
      "metrics. You cannot include metrics for another user-defined project.\n",
      "For example, an alerting rule for the user-defined project \n",
      "ns1\n",
      " can have metrics\n",
      "from \n",
      "ns1\n",
      " and cluster metrics, such as the CPU and memory metrics. However,\n",
      "the rule cannot include metrics from \n",
      "ns2\n",
      ".\n",
      "Additionally, you cannot create alerting rules for the \n",
      "openshift-*\n",
      " core Red Hat\n",
      "OpenShift Service on AWS projects. Red Hat OpenShift Service on AWS\n",
      "monitoring by default provides a set of alerting rules for these projects.\n",
      "3\n",
      ". \n",
      "Apply the configuration file to the cluster:\n",
      "It takes some time to create the alerting rule.\n",
      "4.6.5.3. Reducing latency for alerting rules that do not query platform metrics\n",
      "If an alerting rule for a user-defined project does not query default cluster metrics, you can deploy the\n",
      "rule directly on the Prometheus instance in the \n",
      "openshift-user-workload-monitoring\n",
      " project. This\n",
      "reduces latency for alerting rules by bypassing Thanos Ruler when it is not required. This also helps to\n",
      "minimize the overall load on monitoring components.\n",
      "WARNING\n",
      "Default Red Hat OpenShift Service on AWS metrics for user-defined projects\n",
      "provide information about CPU and memory usage, bandwidth statistics, and\n",
      "packet rate information. Those metrics cannot be included in an alerting rule if you\n",
      "deploy the rule directly to the Prometheus instance in the \n",
      "openshift-user-\n",
      "workload-monitoring\n",
      " project. The procedure outlined in this section should only be\n",
      "used if you have read the documentation and have a comprehensive understanding\n",
      "of the monitoring architecture.\n",
      "Prerequisites\n",
      "You have enabled monitoring for user-defined projects.\n",
      "You are logged in as a user that has the \n",
      "monitoring-rules-edit\n",
      " role for the project where you\n",
      "want to create an alerting rule.\n",
      "  - name:\n",
      " example\n",
      "    rules:\n",
      "    - alert:\n",
      " VersionAlert\n",
      "      expr:\n",
      " version{job=\n",
      "\"prometheus-example-app\"\n",
      "} == \n",
      "0\n",
      "$ oc apply -f example-app-alerting-rule.yaml\n",
      "\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "70\n",
      "You have installed the OpenShift CLI (\n",
      "oc\n",
      ").\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Create a YAML file for alerting rules. In this example, it is called \n",
      "example-app-alerting-\n",
      "rule.yaml\n",
      ".\n",
      "2\n",
      ". \n",
      "Add an alerting rule configuration to the YAML file that includes a label with the key \n",
      "openshift.io/prometheus-rule-evaluation-scope\n",
      " and value \n",
      "leaf-prometheus\n",
      ". For example:\n",
      "If that label is present, the alerting rule is deployed on the Prometheus instance in the \n",
      "openshift-user-\n",
      "workload-monitoring\n",
      " project. If the label is not present, the alerting rule is deployed to Thanos Ruler.\n",
      "1\n",
      ". \n",
      "Apply the configuration file to the cluster:\n",
      "It takes some time to create the alerting rule.\n",
      "See \n",
      "Monitoring overview\n",
      " for details about Red Hat OpenShift Service on AWS 4 monitoring\n",
      "architecture.\n",
      "4.6.5.4. Accessing alerting rules for user-defined projects\n",
      "To list alerting rules for a user-defined project, you must have been assigned the \n",
      "monitoring-rules-\n",
      "view\n",
      " role for the project.\n",
      "Prerequisites\n",
      "You have enabled monitoring for user-defined projects.\n",
      "You are logged in as a user that has the \n",
      "monitoring-rules-view\n",
      " role for your project.\n",
      "You have installed the OpenShift CLI (\n",
      "oc\n",
      ").\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "You can list alerting rules in \n",
      "<project>\n",
      ":\n",
      "apiVersion:\n",
      " monitoring.coreos.com/v1\n",
      "kind:\n",
      " PrometheusRule\n",
      "metadata:\n",
      "  name:\n",
      " example-alert\n",
      "  namespace:\n",
      " ns1\n",
      "  labels:\n",
      "    openshift.io/prometheus-rule-evaluation-scope: leaf-prometheus\n",
      "spec:\n",
      "  groups:\n",
      "  - name:\n",
      " example\n",
      "    rules:\n",
      "    - alert:\n",
      " VersionAlert\n",
      "      expr:\n",
      " version{job=\n",
      "\"prometheus-example-app\"\n",
      "} == \n",
      "0\n",
      "$ oc apply -f example-app-alerting-rule.yaml\n",
      "$ oc -n <project> get prometheusrule\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "71\n",
      "2\n",
      ". \n",
      "To list the configuration of an alerting rule, run the following:\n",
      "4.6.5.5. Listing alerting rules for all projects in a single view\n",
      "As a cluster administrator, you can list alerting rules for core Red Hat OpenShift Service on AWS and\n",
      "user-defined projects together in a single view.\n",
      "Prerequisites\n",
      "You have access to the cluster as a user with the \n",
      "cluster-admin\n",
      " or \n",
      "dedicated-admin\n",
      " role.\n",
      "You have installed the OpenShift CLI (\n",
      "oc\n",
      ").\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "In the \n",
      "Administrator\n",
      " perspective, navigate to \n",
      "Observe\n",
      " \n",
      "→\n",
      " \n",
      "Alerting\n",
      " \n",
      "→\n",
      " \n",
      "Alerting Rules\n",
      ".\n",
      "2\n",
      ". \n",
      "Select the \n",
      "Platform\n",
      " and \n",
      "User\n",
      " sources in the \n",
      "Filter\n",
      " drop-down menu.\n",
      "NOTE\n",
      "The \n",
      "Platform\n",
      " source is selected by default.\n",
      "4.6.5.6. Removing alerting rules for user-defined projects\n",
      "You can remove alerting rules for user-defined projects.\n",
      "Prerequisites\n",
      "You have enabled monitoring for user-defined projects.\n",
      "You are logged in as a user that has the \n",
      "monitoring-rules-edit\n",
      " role for the project where you\n",
      "want to create an alerting rule.\n",
      "You have installed the OpenShift CLI (\n",
      "oc\n",
      ").\n",
      "Procedure\n",
      "To remove rule \n",
      "<foo>\n",
      " in \n",
      "<namespace>\n",
      ", run the following:\n",
      "Additional resources\n",
      "See the \n",
      "Alertmanager documentation\n",
      "Additional resources\n",
      "See \n",
      "Monitoring overview\n",
      " for details about Red Hat OpenShift Service on AWS monitoring\n",
      "architecture.\n",
      "$ oc -n <project> get prometheusrule <rule> -o yaml\n",
      "$ oc -n <namespace> delete prometheusrule <foo>\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "72\n",
      "1\n",
      "2\n",
      "See the \n",
      "Alertmanager documentation\n",
      " for information about alerting rules.\n",
      "See the \n",
      "Prometheus relabeling documentation\n",
      " for information about how relabeling works.\n",
      "See the \n",
      "Prometheus alerting documentation\n",
      " for further guidelines on optimizing alerts.\n",
      "4.6.6. Applying a custom configuration to Alertmanager for user-defined alert\n",
      "routing\n",
      "If you have enabled a separate instance of Alertmanager dedicated to user-defined alert routing, you\n",
      "can overwrite the configuration for this instance of Alertmanager by editing the \n",
      "alertmanager-user-\n",
      "workload\n",
      " secret in the \n",
      "openshift-user-workload-monitoring\n",
      " namespace.\n",
      "Prerequisites\n",
      "You have access to the cluster as a user with the \n",
      "cluster-admin\n",
      " or \n",
      "dedicated-admin\n",
      " role.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Print the currently active Alertmanager configuration into the file \n",
      "alertmanager.yaml\n",
      ":\n",
      "2\n",
      ". \n",
      "Edit the configuration in \n",
      "alertmanager.yaml\n",
      ":\n",
      "Specifies which alerts match the route. This example shows all alerts that have the \n",
      "service=\"prometheus-example-monitor\"\n",
      " label.\n",
      "Specifies the receiver to use for the alerts group.\n",
      "3\n",
      ". \n",
      "Apply the new configuration in the file:\n",
      "Additional resources\n",
      "See \n",
      "the PagerDuty official site\n",
      " for more information on PagerDuty.\n",
      "$ oc -n openshift-user-workload-monitoring get secret alertmanager-user-workload --\n",
      "template='{{ index .data \"alertmanager.yaml\" }}' | base64 --decode > alertmanager.yaml\n",
      "route:\n",
      "  receiver:\n",
      " Default\n",
      "  group_by:\n",
      "  - name:\n",
      " Default\n",
      "  routes:\n",
      "  - matchers:\n",
      "    -\n",
      " \n",
      "\"service = prometheus-example-monitor\"\n",
      " \n",
      "1\n",
      "    receiver:\n",
      " <receiver> \n",
      "2\n",
      "receivers:\n",
      "- name:\n",
      " Default\n",
      "- name:\n",
      " <receiver\n",
      ">\n",
      "#  <receiver_configuration>\n",
      "$ oc -n openshift-user-workload-monitoring create secret generic alertmanager-user-\n",
      "workload --from-file=alertmanager.yaml --dry-run=client -o=yaml |  oc -n openshift-user-\n",
      "workload-monitoring replace secret --filename=-\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "73\n",
      "See \n",
      "the PagerDuty Prometheus Integration Guide\n",
      " to learn how to retrieve the \n",
      "service_key\n",
      ".\n",
      "See \n",
      "Alertmanager configuration\n",
      " for configuring alerting through different alert receivers.\n",
      "4.6.7. Next steps\n",
      "Reviewing monitoring dashboards\n",
      "4.7. REVIEWING MONITORING DASHBOARDS\n",
      "Red Hat OpenShift Service on AWS provides monitoring dashboards that help you understand the state\n",
      "of user-defined projects.\n",
      "In the \n",
      "Developer\n",
      " perspective, you can access dashboards that provide the following statistics for a\n",
      "selected project:\n",
      "CPU usage\n",
      "Memory usage\n",
      "Bandwidth information\n",
      "Packet rate information\n",
      "Figure 4.1. Example dashboard in the Developer perspective\n",
      "NOTE\n",
      "In the \n",
      "Developer\n",
      " perspective, you can view dashboards for only one project at a time.\n",
      "4.7.1. Reviewing monitoring dashboards as a developer\n",
      "In the \n",
      "Developer\n",
      " perspective, you can view dashboards relating to a selected project. You must have\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "74\n",
      "In the \n",
      "Developer\n",
      " perspective, you can view dashboards relating to a selected project. You must have\n",
      "access to monitor a project to view dashboard information for it.\n",
      "Prerequisites\n",
      "You have access to the cluster as a \n",
      "dedicated-admin\n",
      " or as a user with view permissions for the\n",
      "project that you are viewing the dashboard for.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "In the \n",
      "Developer\n",
      " perspective in the Red Hat OpenShift Service on AWS web console, navigate\n",
      "to \n",
      "Observe\n",
      " \n",
      "→\n",
      " \n",
      "Dashboard\n",
      ".\n",
      "2\n",
      ". \n",
      "Choose a project in the \n",
      "Project:\n",
      " list.\n",
      "3\n",
      ". \n",
      "Choose a workload in the \n",
      "All Workloads\n",
      " list.\n",
      "4\n",
      ". \n",
      "Optional: Select a time range for the graphs in the \n",
      "Time Range\n",
      " list.\n",
      "5\n",
      ". \n",
      "Optional: Select a \n",
      "Refresh Interval\n",
      ".\n",
      "6\n",
      ". \n",
      "Hover over each of the graphs within a dashboard to display detailed information about specific\n",
      "items.\n",
      "4.7.2. Next steps\n",
      "Troubleshooting monitoring issues\n",
      "4.8. TROUBLESHOOTING MONITORING ISSUES\n",
      "Find troubleshooting steps for common monitoring issues with user-defined projects.\n",
      "4.8.1. Determining why user-defined project metrics are unavailable\n",
      "If metrics are not displaying when monitoring user-defined projects, follow these steps to troubleshoot\n",
      "the issue.\n",
      "Procedure\n",
      "1\n",
      ". \n",
      "Query the metric name and verify that the project is correct:\n",
      "a\n",
      ". \n",
      "From the \n",
      "Developer\n",
      " perspective in the OpenShift Container Platform web console, select\n",
      "Observe\n",
      " \n",
      "→\n",
      " \n",
      "Metrics\n",
      ".\n",
      "b\n",
      ". \n",
      "Select the project that you want to view metrics for in the \n",
      "Project:\n",
      " list.\n",
      "c\n",
      ". \n",
      "Choose a query from the \n",
      "Select Query\n",
      " list, or run a custom PromQL query by selecting\n",
      "Show PromQL\n",
      ".\n",
      "The \n",
      "Select Query\n",
      " pane shows the metric names.\n",
      "Queries must be done on a per-project basis. The metrics that are shown relate to the\n",
      "project that you have selected.\n",
      "2\n",
      ". \n",
      "Verify that the pod that you want metrics from is actively serving metrics. Run the following \n",
      "oc\n",
      " \n",
      "exec\n",
      " command into a pod to target the \n",
      "podIP\n",
      ", \n",
      "port\n",
      ", and \n",
      "/metrics\n",
      ".\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "75\n",
      "1\n",
      "NOTE\n",
      "You must run the command on a pod that has \n",
      "curl\n",
      " installed.\n",
      "The following example output shows a result with a valid version metric.\n",
      "Example output\n",
      "An invalid output indicates that there is a problem with the corresponding application.\n",
      "3\n",
      ". \n",
      "If you are using a \n",
      "PodMonitor\n",
      " CRD, verify that the \n",
      "PodMonitor\n",
      " CRD is configured to point to\n",
      "the correct pods using label matching. For more information, see the Prometheus Operator\n",
      "documentation.\n",
      "4\n",
      ". \n",
      "If you are using a \n",
      "ServiceMonitor\n",
      " CRD, and if the \n",
      "/metrics\n",
      " endpoint of the pod is showing\n",
      "metric data, follow these steps to verify the configuration:\n",
      "a\n",
      ". \n",
      "Verify that the service is pointed to the correct \n",
      "/metrics\n",
      " endpoint. The service \n",
      "labels\n",
      " in this\n",
      "output must match the services monitor \n",
      "labels\n",
      " and the \n",
      "/metrics\n",
      " endpoint defined by the\n",
      "service in the subsequent steps.\n",
      "Example output\n",
      "Specifies that this is a service API.\n",
      "$ oc exec <sample_pod> -n <sample_namespace> -- curl <target_pod_IP>:<port>/metrics\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "# HELP version Version information about this binary-- --:--:-- --:--:--     0\n",
      "# TYPE version gauge\n",
      "version{version=\"v0.1.0\"} 1\n",
      "100   102  100   102    0     0  51000      0 --:--:-- --:--:-- --:--:-- 51000\n",
      "$ oc get service\n",
      "apiVersion: v1\n",
      "kind: Service \n",
      "1\n",
      "metadata:\n",
      "  labels: \n",
      "2\n",
      "    app: prometheus-example-app\n",
      "  name: prometheus-example-app\n",
      "  namespace: ns1\n",
      "spec:\n",
      "  ports:\n",
      "  - port: 8080\n",
      "    protocol: TCP\n",
      "    targetPort: 8080\n",
      "    name: web\n",
      "  selector:\n",
      "    app: prometheus-example-app\n",
      "  type: ClusterIP\n",
      "Red Hat OpenShift Service on AWS 4 Cluster administration\n",
      "76\n",
      "2\n",
      "Specifies the labels that are being used for this service.\n",
      "b\n",
      ". \n",
      "Query the \n",
      "serviceIP\n",
      ", \n",
      "port\n",
      ", and \n",
      "/metrics\n",
      " endpoints to see if the same metrics from the \n",
      "curl\n",
      "command you ran on the pod previously:\n",
      "i\n",
      ". \n",
      "Run the following command to find the service IP:\n",
      "ii\n",
      ". \n",
      "Query the \n",
      "/metrics\n",
      " endpoint:\n",
      "Valid metrics are returned in the following example.\n",
      "Example output\n",
      "c\n",
      ". \n",
      "Use label matching to verify that the \n",
      "ServiceMonitor\n",
      " object is configured to point to the\n",
      "desired service. To do this, compare the \n",
      "Service\n",
      " object from the \n",
      "oc get service\n",
      " output to\n",
      "the \n",
      "ServiceMonitor\n",
      " object from the \n",
      "oc get servicemonitor\n",
      " output. The labels must match\n",
      "for the metrics to be displayed.\n",
      "For example, from the previous steps, notice how the \n",
      "Service\n",
      " object has the \n",
      "app:\n",
      " \n",
      "prometheus-example-app\n",
      " label and the \n",
      "ServiceMonitor\n",
      " object has the same \n",
      "app:\n",
      " \n",
      "prometheus-example-app\n",
      " match label.\n",
      "5\n",
      ". \n",
      "If everything looks valid and the metrics are still unavailable, please contact the support team\n",
      "for further help.\n",
      "$ oc get service -n <target_namespace>\n",
      "$ oc exec <sample_pod> -n <sample_namespace> -- curl <service_IP>:\n",
      "<port>/metrics\n",
      "% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                               Dload  Upload   Total   Spent    Left  Speed\n",
      "100   102  100   102    0     0  51000      0 --:--:-- --:--:-- --:--:--   99k\n",
      "# HELP version Version information about this binary\n",
      "# TYPE version gauge\n",
      "version{version=\"v0.1.0\"} 1\n",
      "CHAPTER 4. MONITORING USER-DEFINED PROJECTS\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "print(text_cadmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31366dd9-3a41-467f-999b-90453a8d14c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lundb\\anaconda3\\envs\\fm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from typing import Optional, List, Mapping, Any\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/fastchat-t5-3b-v1.0\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"lmsys/fastchat-t5-3b-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The house is wonderful'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\"Translate English to Spanish: The house is wonderful\", return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApiLLM(LLM):\n",
    "    \n",
    "    model: transformers.models.t5.modeling_t5.T5ForConditionalGeneration\n",
    "    tokenizer: transformers.models.t5.tokenization_t5_fast.T5TokenizerFast\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "    \n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Call the LLM.\"\"\"\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        outputs = model.generate(input_ids)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # @property\n",
    "    # def _identifying_params(self) -> Mapping[str, Any]:\n",
    "    #     \"\"\"Get the identifying parameters.\"\"\"\n",
    "    #     return {\"api_url\": self.api_url, \"api_params\": self.api_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 documents were loaded in 10349 chunks\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader('../data/external', glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "documents = loader.load()  # FYI with the current dataset, documents[42] is the FAQ\n",
    "\n",
    "text_splitter = MarkdownTextSplitter(chunk_overlap=0, chunk_size=500)  # Consider setting chunk_size=1000\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"{len(documents)} documents were loaded in {len(texts)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest text chunk is index 334, with lenght 500\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(text.page_content) for text in texts)\n",
    "max_len_idx = [i for i, text in enumerate(texts) if len(text.page_content) == max_len][0]\n",
    "print(f\"The longest text chunk is index {max_len_idx}, with lenght {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: ../data/interim\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "# https://langchain.readthedocs.io/en/latest/modules/indexes/vectorstore_examples/chroma.html#persist-the-database\n",
    "db_dir = \"../data/interim\"\n",
    "docsearch = None\n",
    "if os.path.isdir(os.path.join(db_dir, \"index\")):\n",
    "    # Load the existing vector store\n",
    "    docsearch = Chroma(persist_directory=db_dir, embedding_function=embeddings)\n",
    "else:\n",
    "    # Create a new vector store\n",
    "    docsearch = Chroma.from_documents(texts[:1000], embeddings, persist_directory=db_dir)\n",
    "    docsearch.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ApiLLM(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a talkative AI model who loves to explain how things work. You are smart and constantly learning.\n",
    "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Detailed answer:\"\"\"\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])\n",
    "chain_type_kwargs = {\"prompt\": qa_prompt}\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever(), chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ['Where can I see a roadmap or make feature requests for the service?',\n",
    "           'How is the pricing of Red Hat OpenShift Service on AWS calculated?',\n",
    "           'Is there an upfront commitment?',\n",
    "           'How can I delete ROSA cluster?',\n",
    "           'Can I shut down my VMs temporarily?', # https://docs.openshift.com/rosa/rosa_architecture/rosa_policy_service_definition/rosa-service-definition.html#rosa-sdpolicy-instance-types_rosa-service-definition\n",
    "           'How can I automatically deploy ROSA cluster?',\n",
    "           'How can my ROSA cluster autoscale?',\n",
    "           'How can I install aws load balancer controller',\n",
    "           'How can I install Prometheus Operator with my ROSA cluster?',\n",
    "           'What time is it?',\n",
    "           'How can I federate metrics to a centralized Prometheus Cluster?',\n",
    "           'What is the meaning of life?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lundb\\anaconda3\\envs\\fm\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where can I see a roadmap or make feature requests for the service?\n",
      "Answer: Where can I see a roadmap or make feature requests for the service?\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39m# split the answer into chunks, separating context and answer\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(result[\u001b[39m\"\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m---> 10\u001b[0m answer \u001b[39m=\u001b[39m result[\u001b[39m\"\u001b[39;49m\u001b[39mresult\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39mDetailed answer:\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39;49m]\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAnswer: \u001b[39m\u001b[39m\"\u001b[39m, answer)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for query in queries:\n",
    "    answers.append(qa_chain(query))\n",
    "\n",
    "# Print the answers\n",
    "for result in answers:\n",
    "    print(\"Question:\", result[\"query\"])\n",
    "    # split the answer into chunks, separating context and answer\n",
    "    print(result[\"result\"])\n",
    "    answer = result[\"result\"].split(\"Detailed answer:\")[1]\n",
    "    print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain currently offers [two wrappers around Hugging Face LLMs](https://langchain.readthedocs.io/en/latest/ecosystem/huggingface.html), one for a local pipeline and one to access a hosted model in Hugging Face Hub.\n",
    "\n",
    "Here we will use the local pipeline version, i.e. running the LLM locally."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from typing import Optional, List, Mapping, Any"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom LLM\n",
    "\n",
    "Here we define a custom LLM wrapper around the Hugging Face pipeline for LangChain to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ApiLLM(LLM):\n",
    "    \n",
    "    api_url: str\n",
    "\n",
    "    api_params: dict\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "    \n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Call the LLM.\"\"\"\n",
    "        payload = json.dumps([prompt, self.api_params]) \n",
    "        response = requests.post(self.api_url, json={\"data\":[payload]}).json()\n",
    "        return response[\"data\"][0]\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"api_url\": self.api_url, \"api_params\": self.api_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# API URL\n",
    "# get the endpoint from an environment value\n",
    "# endpoint = os.environ.get(\"LLM_API_URL\", None)\n",
    "# or set it manually\n",
    "endpoint = \"https://text-generation-api-llm.apps.et-cluster.6mwp.p1.openshiftapps.com/api\"\n",
    "\n",
    "# Generation parameters\n",
    "# Reference: https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig\n",
    "params = {\n",
    "    'max_new_tokens': 500,\n",
    "    'do_sample': True,\n",
    "    'temperature': 0.7,\n",
    "    'top_p': 0.73,\n",
    "    'typical_p': 1,\n",
    "    'repetition_penalty': 1.0,\n",
    "    'encoder_repetition_penalty': 1.0,\n",
    "    'top_k': 0,\n",
    "    'min_length': 10,\n",
    "    'no_repeat_ngram_size': 0,\n",
    "    'num_beams': 1,\n",
    "    'penalty_alpha': 0,\n",
    "    'length_penalty': 0,\n",
    "    'early_stopping': False,\n",
    "    'seed': -1,\n",
    "    'add_bos_token': True,\n",
    "    'custom_stopping_strings': [],\n",
    "    'truncation_length': 2048,\n",
    "    'ban_eos_token': False,\n",
    "    'skip_special_tokens': True,\n",
    "}\n",
    "\n",
    "llm = ApiLLM(api_url=endpoint, api_params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\lundb\\anaconda3\\envs\\fm\\lib\\site-packages\\requests\\models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 971\u001b[0m     \u001b[39mreturn\u001b[39;00m complexjson\u001b[39m.\u001b[39mloads(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[39mexcept\u001b[39;00m JSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[39m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[39m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lundb\\anaconda3\\envs\\fm\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lundb\\anaconda3\\envs\\fm\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n",
      "File \u001b[1;32mc:\\Users\\lundb\\anaconda3\\envs\\fm\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(llm(\u001b[39m\"\u001b[39;49m\u001b[39mThis is a test\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\lundb\\anaconda3\\envs\\fm\\lib\\site-packages\\langchain\\llms\\base.py:246\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, prompt: \u001b[39mstr\u001b[39m, stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    245\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check Cache and run the LLM on the given prompt and input.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([prompt], stop\u001b[39m=\u001b[39;49mstop)\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\n",
      "File \u001b[1;32mc:\\Users\\lundb\\anaconda3\\envs\\fm\\lib\\site-packages\\langchain\\llms\\base.py:140\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_end(output, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\lundb\\anaconda3\\envs\\fm\\lib\\site-packages\\langchain\\llms\\base.py:137\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[0;32m    134\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompts, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[0;32m    135\u001b[0m )\n\u001b[0;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(prompts, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[1;32mc:\\Users\\lundb\\anaconda3\\envs\\fm\\lib\\site-packages\\langchain\\llms\\base.py:324\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    322\u001b[0m generations \u001b[39m=\u001b[39m []\n\u001b[0;32m    323\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[1;32m--> 324\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(prompt, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[0;32m    325\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[0;32m    326\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m, in \u001b[0;36mApiLLM._call\u001b[1;34m(self, prompt, stop)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call the LLM.\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m payload \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps([prompt, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_params]) \n\u001b[1;32m---> 14\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mpost(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_url, json\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m:[payload]})\u001b[39m.\u001b[39;49mjson()\n\u001b[0;32m     15\u001b[0m \u001b[39mreturn\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\lundb\\anaconda3\\envs\\fm\\lib\\site-packages\\requests\\models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m     \u001b[39mreturn\u001b[39;00m complexjson\u001b[39m.\u001b[39mloads(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[39mexcept\u001b[39;00m JSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[39m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[39m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m     \u001b[39mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[39m.\u001b[39mmsg, e\u001b[39m.\u001b[39mdoc, e\u001b[39m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "print(llm(\"This is a test\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Document indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 documents were loaded in 10349 chunks\n"
     ]
    }
   ],
   "source": [
    "# Note: using TextLoader here instead of UnstructuredMarkdownLoader. MarkdownTextSplitter will do the job of parsing markdown\n",
    "loader = DirectoryLoader('../data/external', glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "documents = loader.load()  # FYI with the current dataset, documents[42] is the FAQ\n",
    "\n",
    "text_splitter = MarkdownTextSplitter(chunk_overlap=0, chunk_size=500)  # Consider setting chunk_size=1000\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"{len(documents)} documents were loaded in {len(texts)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest text chunk is index 334, with lenght 500\n"
     ]
    }
   ],
   "source": [
    "# Find the index of the longest text in texts\n",
    "max_len = max(len(text.page_content) for text in texts)\n",
    "max_len_idx = [i for i, text in enumerate(texts) if len(text.page_content) == max_len][0]\n",
    "print(f\"The longest text chunk is index {max_len_idx}, with lenght {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='```bash\\noc create imagestream ruby\\noc tag openshift/ruby:2.5-ubi8 ruby:2.5\\necho << EOF | oc apply -f -\\nkind: BuildConfig\\napiVersion: build.openshift.io/v1\\nmetadata:\\n  name: ruby-sample-build\\n  namespace: test-ecr-secret-operator\\nspec:\\n  runPolicy: Serial\\n  source:\\n    git:\\n      uri: \"https://github.com/openshift/ruby-hello-world\"\\n  strategy:\\n    sourceStrategy:\\n      from:\\n        kind: \"ImageStreamTag\"\\n        name: \"ruby:2.5\"\\n      incremental: true\\n  output:\\n    to:\\n      kind: \"DockerImage\"' metadata={'source': 'data\\\\external\\\\rh-mobb\\\\ecr-secret-operator-_index.md'}\n"
     ]
    }
   ],
   "source": [
    "# show the biggest chunk\n",
    "print(texts[max_len_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lundb\\anaconda3\\envs\\fm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using embedded DuckDB with persistence: data will be stored in: ./data/interim\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "# https://langchain.readthedocs.io/en/latest/modules/indexes/vectorstore_examples/chroma.html#persist-the-database\n",
    "db_dir = \"../data/interim\"\n",
    "docsearch = None\n",
    "if os.path.isdir(os.path.join(db_dir, \"index\")):\n",
    "    # Load the existing vector store\n",
    "    docsearch = Chroma(persist_directory=db_dir, embedding_function=embeddings)\n",
    "else:\n",
    "    # Create a new vector store\n",
    "    docsearch = Chroma.from_documents(texts[:1000], embeddings, persist_directory=db_dir)\n",
    "    docsearch.persist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# template = \"\"\"When learning about Red Hat OpenShift Service on AWS (ROSA), and considering the following context:\n",
    "# ========= snippets start here =========\n",
    "# {context}\n",
    "# ========= spippets end here =========\n",
    "# Given this question: {question}\n",
    "# The answer to the question is:\"\"\"\n",
    "#\n",
    "\n",
    "# Use the following pieces of context to answer the question at the end. Answer with as much details and explanations as possible.\n",
    "template = \"\"\"You are a talkative AI model who loves to explain how things work. You are smart and constantly learning.\n",
    "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Detailed answer:\"\"\"\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])\n",
    "chain_type_kwargs = {\"prompt\": qa_prompt}\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever(), chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "\n",
    "References:\n",
    "- https://langchain.readthedocs.io/en/latest/modules/indexes/combine_docs.html for types of chains to combine documents\n",
    "- https://langchain.readthedocs.io/en/latest/modules/indexes/chain_examples/question_answering.html for the QA example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries = ['Where can I see a roadmap or make feature requests for the service?',\n",
    "           'How is the pricing of Red Hat OpenShift Service on AWS calculated?',\n",
    "           'Is there an upfront commitment?',\n",
    "           'How can I delete ROSA cluster?',\n",
    "           'Can I shut down my VMs temporarily?', # https://docs.openshift.com/rosa/rosa_architecture/rosa_policy_service_definition/rosa-service-definition.html#rosa-sdpolicy-instance-types_rosa-service-definition\n",
    "           'How can I automatically deploy ROSA cluster?',\n",
    "           'How can my ROSA cluster autoscale?',\n",
    "           'How can I install aws load balancer controller',\n",
    "           'How can I install Prometheus Operator with my ROSA cluster?',\n",
    "           'What time is it?',\n",
    "           'How can I federate metrics to a centralized Prometheus Cluster?',\n",
    "           'What is the meaning of life?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where can I see a roadmap or make feature requests for the service?\n",
      "Answer:   https://ai.google.com/\n",
      "Question: How is the pricing of Red Hat OpenShift Service on AWS calculated?\n",
      "Answer:   not enough information\n",
      "Question: Is there an upfront commitment?\n",
      "Answer:   No\n",
      "Question: How can I delete ROSA cluster?\n",
      "Answer:   Delete ROSA cluster from the command line.\n",
      "Question: Can I shut down my VMs temporarily?\n",
      "Answer:   No\n",
      "Question: How can I automatically deploy ROSA cluster?\n",
      "Answer:   ROSA is a set of tools for building, running, and monitoring large distributed computing clusters.\n",
      "Question: How can my ROSA cluster autoscale?\n",
      "Answer:   You don't know\n",
      "Question: How can I install aws load balancer controller\n",
      "Answer:   you can download the Aws Load Balancer Controller from the Cloud Controller\n",
      "Question: How can I install Prometheus Operator with my ROSA cluster?\n",
      "Answer:   See the installation guide for ROSA cluster.\n",
      "Question: What time is it?\n",
      "Answer:   It is 1am\n",
      "Question: How can I federate metrics to a centralized Prometheus Cluster?\n",
      "Answer:   You can use the federation interface to connect to the cluster.\n",
      "Question: What is the meaning of life?\n",
      "Answer:   You are a talkative AI model who loves to explain how things work. You are smart and constantly learning.\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for query in queries:\n",
    "    answers.append(qa_chain(query))\n",
    "\n",
    "# Print the answers\n",
    "for result in answers:\n",
    "    print(\"Question:\", result[\"query\"])\n",
    "    # split the answer into chunks, separating context and answer\n",
    "    answer = result[\"result\"].split(\"Detailed answer:\")[1]\n",
    "    print(\"Answer: \", answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "dfef540ac477f3be91e5308acd671ac2a81f2c8cc1125947821b3502f71b441e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
